{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5D7XJoeERrJ"
      },
      "source": [
        "# **AI Agents for Social Sciences: Week 2 Homework Modules**\n",
        "\n",
        "- __Instructor:__ James Evans\n",
        "\n",
        "- __Notebook Author & TAs:__ Shiyang Lai, Gio Choi, Avi Oberoi, Jesse Zhou\n",
        "\n",
        "__Perform 3 out of this week's following 5 modules:__\n",
        "\n",
        "<font color=\"orange\">NOTE: We acknowledge that this is another notebook with a lot of code - but do not fear, it is only here to help you. You don't need to run all of it, and in many cases we pursue multiple ways to perform an activity, such as a word or sentence embedding. Think of all of the code as a resource to help you do the HW and your group projects.</font>\n",
        "\n",
        "<font color=\"orange\">NOTE: As usual, we recommend looking at the HW first before going through and running each of the cells, as some of it may not be relevant to what you seek to accomplish. This notebook is expected to be a resource on the different kinds of static neural based methods for text, which can be quickly adapted for your analysis of choice. You are not expected to master all of these methods, but to be aware of them. The homework will require you to use a variety of them to come up with a meaningful exploration and analysis of your textual dataset.</font>\n",
        "\n",
        "<font color=\"purple\">NOTE: After completing the assignment (tutorial + memo), please take a moment to fill out [this short survey](https://forms.gle/bjN9Z9YWYDeUj5dh8). Your feedback helps us understand your experience with the homework and the coding assistant.</font>\n",
        "\n",
        "## 1. **Neural Embeddings**\n",
        "\n",
        "### **Summary:**\n",
        "In this module, we introduce multiple techniques to construct your own word and document embeddings through different forms of neural networks such as Word2Vec, FastText, GloVe, etc. Furthermore, we will also guide you to debias a word embedding space and use text embeddings for downstream tasks such as deep-learning-based topic analysis.\n",
        "\n",
        "### **Tasks/Questions:**\n",
        "\n",
        "**1)** Train a word2vec model on your dataset. Explore some of the relationships between words which are learned on your dataset.\n",
        "\n",
        "**2)** Perform an advanced word embedding analysis, such as visualising embeddings, creating dynamic embeddings, projecting embeddings on distinct dimensions, debiasing, retrofitting, or aligning.\n",
        "\n",
        "**3)** How do you think such methods can be useful in your social scientific research? Hint: think of what different parts of speech or entities might entail!\n",
        "\n",
        "## 2. **Encoders, Decoders, Seq2Seq and Attention**\n",
        "\n",
        "### **Summary:**\n",
        "Encoder-Decoder models are a powerful way to deal with sequence to sequence problems - this kind of setup is also called seq2seq. When we use this kind of sequence to sequence alignment and also map certain parts of the sequences with different values (a process called Attention), the sequence to sequence prediction task performance increases. In this section and the next we will focus on Machine Translation as the domain to try Encoder and Decoder tasks, without and then with attention.\n",
        "\n",
        "### **Tasks/Questions:**\n",
        "\n",
        "**1)** Use a large pre-trained language model for sequence classification, question answering, language modeling, or any other NLP task. Fine-tune the model on your dataset.\n",
        "\n",
        "**2)** What model did you use? How did your model perform?\n",
        "\n",
        "**3)** Build a SHAP-based explainer of your model for 3 inputs from your data. Describe the feature importance relationships you see for each of your inputs.\n",
        "\n",
        "**4)** Build a SHAP-based explainer of your model over a substantial subsample of your data. Describe the aggregate feature importance values you obtain for each of your labels.\n",
        "\n",
        "## 3. **Transformers**\n",
        "\n",
        "### **Summary:**\n",
        "Transformers are a neural architecture comprised of encoder component and a decoder component, which use self-attention to reach high levels of performance for sequence to sequence tasks (and drops all RNN components!) This is THE most popular archetecture for generative language models as of now. GPT-N is essentially a variant of this model family.\n",
        "\n",
        "### **Tasks/Questions:**\n",
        "\n",
        "**1)** Use a large pre-trained language model different from the one used in the prior module's assignment for sequence classification, question answering, language modeling, or any other NLP task. Fine-tune the model on your dataset.\n",
        "\n",
        "**2)** What model did you use? How did your model perform?\n",
        "\n",
        "**3)** BONUS: use a PyTorch or Keras sequence to sequence model to perform a task other than machine translation.\n",
        "\n",
        "## 4. **BERT Word and Sentence Embeddings**\n",
        "\n",
        "### **Summary:**\n",
        "The BERT (Bidirectional Encoder Representations from Transformers) model is a powerful deep learning model introduced by Google in 2018. It reads text in both directions, helping it understand the full context of words in a sentence. BERT has been used for many language tasks like answering questions, classifying text, and analyzing sentiment. Its success has inspired improved versions and made it a key tool in natural language processing.\n",
        "\n",
        "### **Tasks/Questions:**\n",
        "\n",
        "**1)** Use BERT or a related model to create word and sentence embeddings from your own corpus, and perform different similarity metrics on your embeddings, as well as perplexity measures.\n",
        "\n",
        "**2)** How does fine-tuning your model change the similarity and perplexity metrics?\n",
        "\n",
        "**3)** BONUS: Fine tune a Transformers based model and use bertviz to visualise its attention layers using two sentences. Compare this to two sentences on a non fine tuned model.\n",
        "\n",
        "## 5. **Diffusion Models**\n",
        "\n",
        "### **Summary:**\n",
        "Here, we move to the frontier of contemporary foundational language model research by introducing a relat\n",
        "ively new architectural paradigm that departs from the dominant Transformer framework: diffusion models. Originally developed and popularized for image generation, diffusion models have recently been adapted to text generation tasks. These models generate outputs through an iterative denoising process, starting from random noise and progressively refining it into coherent sequences. Unlike traditional autoregressive language models, diffusion-based approaches can generate tokens in parallel, or selectively incorporate sequential dependencies to balance efficiency and expressiveness. In this brief tutorial, we offer a simple, concrete illustration of how diffusion models can be applied to text generation.\n",
        "\n",
        "### **Tasks/Questions:**\n",
        "\n",
        "**1)** Play with hyper-parameters in `DiffusionConfig` class and report how 1 or 2 of them impacts model learning.\n",
        "\n",
        "**2)** Propose one application of diffusion-based language models.\n",
        "\n",
        "**3)** BONUS: What unique properties of language do diffusion-based embeddings encode that transformer-based embeddings fail to capture?\n",
        "\n",
        "## **Memo Pilot Test**\n",
        "\n",
        "Create a preliminary implementation (pilot) of the research design you outlined in your memo of this week. Use the techniques and frameworks introduced in this assignment to demonstrate how your proposed approach can be operationalized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "6jzPTKuT8Tx4",
        "outputId": "27b51458-8a82-4c7b-c96d-b2d2a940596c"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (383095408.py, line 2)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    Neural Embeddings = False  # @param {type:\"boolean\"}\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# @markdown Mark the Modules you completed\n",
        "Neural Embeddings = False  # @param {type:\"boolean\"}\n",
        "Encoders, Decoders, Seq2Seq and Attention = False  # @param {type:\"boolean\"}\n",
        "Transformers = False  # @param {type:\"boolean\"}\n",
        "BERT Word and Sentence Embeddings and Bayesian Optimization = False  # @param {type:\"boolean\"}\n",
        "Diffusion Models = False  # @param {type:\"boolean\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8Wi9GqlAZJ3"
      },
      "source": [
        "# Module 0: Installation and Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EwYPuxdAsCu"
      },
      "source": [
        "## Imports and Installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "id": "HQArsckGAYf2"
      },
      "outputs": [],
      "source": [
        "# Let's quitely install some necessary dependencies here\n",
        "!pip install -q git+https://github.com/UChicago-Computational-Content-Analysis/lucem_illud.git\n",
        "!pip install -q -U yellowbrick\n",
        "!pip install -q umap-learn\n",
        "!pip install -q transformers\n",
        "!pip install -q shap\n",
        "!pip install -q datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EO45Tv3ZBN8z"
      },
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "File \u001b[0;32m/opt/anaconda3/envs/agents/lib/python3.13/site-packages/lucem_illud/processing.py:10\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 10\u001b[0m     nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n",
            "File \u001b[0;32m/opt/anaconda3/envs/agents/lib/python3.13/site-packages/spacy/__init__.py:52\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03mname (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03mRETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m util\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[1;32m     53\u001b[0m     name,\n\u001b[1;32m     54\u001b[0m     vocab\u001b[38;5;241m=\u001b[39mvocab,\n\u001b[1;32m     55\u001b[0m     disable\u001b[38;5;241m=\u001b[39mdisable,\n\u001b[1;32m     56\u001b[0m     enable\u001b[38;5;241m=\u001b[39menable,\n\u001b[1;32m     57\u001b[0m     exclude\u001b[38;5;241m=\u001b[39mexclude,\n\u001b[1;32m     58\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m     59\u001b[0m )\n",
            "File \u001b[0;32m/opt/anaconda3/envs/agents/lib/python3.13/site-packages/spacy/util.py:530\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m--> 530\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
            "\u001b[0;31mOSError\u001b[0m: [E941] Can't find model 'en'. It looks like you're trying to load a model from a shortcut, which is obsolete as of spaCy v3.0. To load the model, use its full name instead:\n\nnlp = spacy.load(\"en_core_web_sm\")\n\nFor more details on the available models, see the models directory: https://spacy.io/models and if you want to create a blank model, use spacy.blank: nlp = spacy.blank(\"en\")",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpora\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dictionary\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LdaModel\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlucem_illud\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# ML and DL packages\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/agents/lib/python3.13/site-packages/lucem_illud/__init__.py:15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_dirs\u001b[39;00m\u001b[38;5;250m  \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownloaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvisualizers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/agents/lib/python3.13/site-packages/lucem_illud/loaders.py:16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mzipfile\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m normalizeTokens, trainTestSplit, word_tokenize, sent_tokenize\n\u001b[1;32m     18\u001b[0m dataDirectory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mloadTextDirectory\u001b[39m(targetDir, encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
            "File \u001b[0;32m/opt/anaconda3/envs/agents/lib/python3.13/site-packages/lucem_illud/processing.py:12\u001b[0m\n\u001b[1;32m     10\u001b[0m     nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m---> 12\u001b[0m     nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m     16\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspacy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/opt/anaconda3/envs/agents/lib/python3.13/site-packages/spacy/__init__.py:52\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m     29\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     36\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[1;32m     37\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m util\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[1;32m     53\u001b[0m         name,\n\u001b[1;32m     54\u001b[0m         vocab\u001b[38;5;241m=\u001b[39mvocab,\n\u001b[1;32m     55\u001b[0m         disable\u001b[38;5;241m=\u001b[39mdisable,\n\u001b[1;32m     56\u001b[0m         enable\u001b[38;5;241m=\u001b[39menable,\n\u001b[1;32m     57\u001b[0m         exclude\u001b[38;5;241m=\u001b[39mexclude,\n\u001b[1;32m     58\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m     59\u001b[0m     )\n",
            "File \u001b[0;32m/opt/anaconda3/envs/agents/lib/python3.13/site-packages/spacy/util.py:531\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[0;32m--> 531\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
            "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# text related packages\n",
        "import gensim\n",
        "import spacy\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import LdaModel\n",
        "import lucem_illud\n",
        "\n",
        "# ML and DL packages\n",
        "import sklearn\n",
        "import keras\n",
        "\n",
        "# generally not good practice, as we want to see warnings - but in this case, there are no critical warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtxlLzZS8MiA"
      },
      "source": [
        "# Module 1: Neural Embeddings\n",
        "\n",
        "We'll now start using neural networks - not explicitly, by building models using Keras and PyTorch, but by using different tricks in training models, we can get a network to end up learning different useful notions of semantics - we then use this model's final embedding (or other aspect) to extract useful features for our text data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhI8b1bl-4NE"
      },
      "source": [
        "## Shallow Neural Embeddings\n",
        "\n",
        "***Word2vec***, much like methods like LSA which we saw earlier, is based on theories of distributional semantics - words that appear around each other are more likely to mean similar things than words that do not appear around each other. Keeping this in mind, our job is to create a high dimensional space where these semantic relations are preserved. The innovation in word2vec is the realization that we can use unlabelled, running text in sentences as inputs for a supervised learning algorithm--as a self-supervision task. It is supervised because we use the words in a sentence to serve as positive and negative examples. Let’s break this down:\n",
        "\n",
        "... \"use the kitchen knife to chop the vegetables\"…\n",
        "\n",
        "**C1   C2   C3   T   C4   C5   C6   C7**\n",
        "\n",
        "Here, the target word is *knife*, and the context words are the ones in its immediate (6-word) window.\n",
        "The first word2vec method we'll see is called skipgram, where the task is to assign a probability for how likely it is that the context window appears around the target word. In the training process, positive examples are samples of words and their context words, and negative examples are created by sampling from pairs of words that do not appear nearby one another.\n",
        "\n",
        "This method of implementing word2vec is called *skipgram with negative sampling*. So while the algorithm tries to better learn which context words are likely to appear around a target word, it ends up pushing the embedded representations for every word so that they are located optimally (e.g., with minimal semantic distortion). In this process of adjusting embedding values, the algorithm brings semantically similar words close together in the resulting high dimensional space, and dissimilar words far away.\n",
        "\n",
        "Another word2vec training method, *Continuous Bag of Words (CBOW)*, works in a similar fashion, and tries to predict the target word, given context. This is converse of skipgram, which tries to predict the context, given the target word. Skip-gram represents rare words and phrases well, often requiring more data for stable representations, while CBOW is several times faster to train than the skip-gram, but with slightly better accuracy for the frequent words in its prediction task. The popular gensim implementation of word2vec has both the methods included.\n",
        "\n",
        "The code below has two examples, one for loading a pre-trained word2vec model, and one for training our own model, with an explanation of the hyperparameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQa7R9ba_ai_"
      },
      "source": [
        "### Training Word Embeddings\n",
        "\n",
        "In the first example, we train a word2vec model on the hobbies dataset with a dimension size of 100 and window of 10, meaning the context window is 10 words. The training process goes through 10 iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lT0PrEUDiHF"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim.parsing.preprocessing import preprocess_documents\n",
        "from gensim.corpora import Dictionary\n",
        "from yellowbrick.datasets import load_hobbies\n",
        "\n",
        "corpus = load_hobbies()\n",
        "len(corpus.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Dl-1quuTByUC"
      },
      "outputs": [],
      "source": [
        "print(\"Example 1\")\n",
        "print(\"Text   :\", corpus.data[0])\n",
        "print(\"Label  :\", corpus.target[0])\n",
        "print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
        "print(\"Example 2\")\n",
        "print(\"Text   :\", corpus.data[200])\n",
        "print(\"Label  :\", corpus.target[200])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXJ7hTPl_kKH"
      },
      "source": [
        "Here, pre-processed texts is a file we created for our topic models, and includes stemmed words. This is a useful exercise to see how pre-processing effects word2vec models. Recall from earlier sections (and homeworks) that the deeper the model the less pre-processing we need to perform because the model can be trained to perform optimal processing.\n",
        "\n",
        "<font color=\"red\">NOTE: if using gensim v3.8.3, use \"size\" for dimension size, and if using v4 +, use \"vector_size\".</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8MBpY7H_ibA"
      },
      "outputs": [],
      "source": [
        "preprocessed_texts = preprocess_documents(corpus.data)\n",
        "dictionary = Dictionary(preprocessed_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjdhifS9Eqes"
      },
      "outputs": [],
      "source": [
        "w2vmodel_cleaned = Word2Vec(preprocessed_texts,\n",
        "                            vector_size=100,\n",
        "                            window=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6CKI4xjFGS6"
      },
      "source": [
        "Let's test the model by looking up some words that we know are in the corpus and find similar words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "2Py45HzqFDaB"
      },
      "outputs": [],
      "source": [
        "def similar_words_df(model, word, topn=10):\n",
        "    return pd.DataFrame(\n",
        "        model.wv.most_similar(word, topn=topn),\n",
        "        columns=[\"word\", \"cosine_similarity\"]\n",
        "    )\n",
        "\n",
        "display(similar_words_df(w2vmodel_cleaned, \"book\"))\n",
        "display(similar_words_df(w2vmodel_cleaned, \"talk\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cWauixBFgbs"
      },
      "source": [
        "<font color=\"orange\">NOTE: it may soon become clear in the next section, but it isn't the best idea to use a corpus with stop words removed. Following from above, we lose a lot of potential relationships we can learn automatically!</font>\n",
        "\n",
        "We don't always need to train our own models. One of the advantages of word embedding (and many deep learning) models is using pre-trained models, where the training has already been completed on a large and/or representative corpus or document collection. In the following lines of code we load either a [Google News model](https://drive.google.com/file/d/1TW3h1SIx__4Y6zEzLnrPry2Z-6BZgtyh/view?usp=sharing) (3+ GB), or a [model trained on NY Times data](https://drive.google.com/file/d/1xrs1UhTH5LPTBFLcflX-nQ86zwCAoLt5/view?usp=sharing) (30 MB). If you have Colab Pro, you can try the heavier model. I have used the mounting capacity on google drive here. You can also just upload the file if you prefer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhTL1ffLFbh0"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GY-f6OtlMoZU"
      },
      "outputs": [],
      "source": [
        "model_address = \"/content/nytimes_cbow.reduced.txt\"\n",
        "\n",
        "wv = gensim.models.KeyedVectors.load_word2vec_format(model_address)\n",
        "nytimes_w2v_model = Word2Vec(vector_size=wv.vector_size, min_count=1)\n",
        "nytimes_w2v_model.build_vocab([wv.index_to_key])\n",
        "\n",
        "# Assign the vectors to the model's wv\n",
        "nytimes_w2v_model.wv.vectors = wv.vectors\n",
        "nytimes_w2v_model.wv.index_to_key = wv.index_to_key\n",
        "nytimes_w2v_model.wv.key_to_index = wv.key_to_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3t1-l86OMfr"
      },
      "outputs": [],
      "source": [
        "display(similar_words_df(nytimes_w2v_model, \"book\"))\n",
        "display(similar_words_df(nytimes_w2v_model, \"talk\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5HiWXH3Of8g"
      },
      "source": [
        "Note how the most similar words make a lot more sense with the pre-trained, larger, ny times word2vec model. Also note what lemmatizing words can dramatically change the results of word2vec! Why? Because we are removing specificity in context that our model can otherwise learn. The smaller the corpus, the more useful to remove content (e.g., stem); the larger the corpus, the more useful to retain it.\n",
        "\n",
        "Just so we have a comparison, we also create a non-cleaned corpus of the hobbies dataset on which to build an embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIbb3Q7tOcsQ"
      },
      "outputs": [],
      "source": [
        "from gensim.parsing.preprocessing import preprocess_string, strip_tags, strip_punctuation\n",
        "\n",
        "CUSTOM_FILTERS = [lambda x: x.lower(), strip_tags, strip_punctuation]\n",
        "\n",
        "tokenized_texts = []\n",
        "for doc in corpus.data:\n",
        "  tokenized_texts.append(preprocess_string(doc, CUSTOM_FILTERS))\n",
        "\n",
        "print(tokenized_texts[0][0:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_A8EJFKPNeL"
      },
      "source": [
        "<font color=\"orange\">NOTE: if using gensim v3.8.3, use \"size\" for dimension size, and if using v4 +, use \"vector_size\"</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G71lVu8BO1W1"
      },
      "outputs": [],
      "source": [
        "w2vmodel = Word2Vec(tokenized_texts,\n",
        "        vector_size=100,\n",
        "        window=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXqfllNUPoHv"
      },
      "source": [
        "### Word and Context Embeddings\n",
        "\n",
        "The gensim model allows us to access both the target embedding for the word itself, and the context embedding for words. These two embedding are multiplied to recreate the original word space, revealing the way in which neural embeddings perform (and are optimized through) matrix factorization of the original word-by-context matrix. The target embedding is what is most commonly used, though in some cases the context is also used along with the target. We show how to use both.\n",
        "\n",
        "The context vector is not showcased in the documentation - we unearthed it thanks to this helpful [StackOverflow link](https://stackoverflow.com/questions/39406092/how-to-get-both-the-word-embeddings-vector-and-context-vector-of-a-given-word-by)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DLU_1NmPfcQ"
      },
      "outputs": [],
      "source": [
        "# target embedding\n",
        "book_embedding_ny = nytimes_w2v_model.wv[\"book\"]\n",
        "book_embedding = w2vmodel.wv[\"book\"]\n",
        "\n",
        "# context embedding\n",
        "book_idx = w2vmodel.wv.key_to_index[\"book\"]\n",
        "book_context_embedding = w2vmodel.syn1neg[book_idx]\n",
        "\n",
        "book_idx = nytimes_w2v_model.wv.key_to_index[\"book\"]\n",
        "book_context_embedding_ny = nytimes_w2v_model.syn1neg[book_idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB87KwtnQODh"
      },
      "source": [
        "### Exploring Semantics with Embeddings\n",
        "\n",
        "So what do these vectors encapsulate? We've seen before that it is some notion of meaning, as captured by distributional semantics, with some aspect of syntactic information. How do we then use these word embeddings? Because they capture semantic information in a space that allows for semantic decomposition, a common task is to perform semantic arithmetic and analogy tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVlwdjyEQqvN"
      },
      "source": [
        "Let's try find the word that least matches the others within a word set (cosine similarity)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0yyYQiiQJWm"
      },
      "outputs": [],
      "source": [
        "nytimes_w2v_model.wv.doesnt_match(['books', 'books', 'novel', 'memoir', 'truck'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wg9jR1k3SmCB"
      },
      "source": [
        "We can also perform arithmetic on vectors using the most_similar function we saw earlier. One of the more popular, earlier examples demonstrating the analogous power of word2vec models was ``vector['king'] - vector['man'] + vector['woman'] = vector['queen']``, where the idea is that we are traversing a \"gender\" dimension and looking at either end of it.\n",
        "\n",
        "Let's see how that works on our NY Times model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hF86kNLBQvt7"
      },
      "outputs": [],
      "source": [
        "display(pd.DataFrame(nytimes_w2v_model.wv.most_similar(positive=['king', 'woman'], negative = ['man']), columns=[\"word\", \"cosine_similarity\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6F_227ZhTUU-"
      },
      "source": [
        "If we shift `['king']` in the direction of woman, we get to `['royal']`...and almost to `['queen']` (try adding woman twice!) Interestingly, `['prince']` is a similarly effeminate royal. We can begin to see the utility of these models, even if they do capture all semantic associations as we might imagine. Larger language models capture even more sophisticated relationships.\n",
        "\n",
        "In the rest of this section we will see other ways in which we can use and explore word embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJYj5I26Tb25"
      },
      "source": [
        "### Visualising Word Embeddings\n",
        "\n",
        "Most methods involving visualising word embeddings involve projecting words into a lower dimensional space and seeing which words appear nearby one another.\n",
        "\n",
        "See this [example by TTIC](https://home.ttic.edu/~kgimpel/wordembviz/wordembviz.html) which visualises different sets of word embeddings, using t-SNE.\n",
        "\n",
        "The code below gives you an idea of how you might do it yourself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omgZEWbgS4tM"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "numWords = 50\n",
        "targetWords = nytimes_w2v_model.wv.index_to_key[200: 200 + numWords]\n",
        "\n",
        "wordsSubMatrix = []\n",
        "for word in targetWords:\n",
        "    wordsSubMatrix.append(nytimes_w2v_model.wv[word])\n",
        "wordsSubMatrix = np.array(wordsSubMatrix)\n",
        "wordsSubMatrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "122fILyd2Yjj"
      },
      "source": [
        "Then we can use PCA to reduce the dimesions (e.g., to 50), and T-SNE to project them down to the two we will visualize. We note that this is nondeterministic process, and so you can repeat and achieve alternative projectsions/visualizations of the words. We also recommend exploring UMAP, which allows projections to any arbitrary number of dimensions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxuTVaEw2ZD9"
      },
      "outputs": [],
      "source": [
        "import sklearn.decomposition\n",
        "import sklearn.manifold\n",
        "\n",
        "pcaWords = sklearn.decomposition.PCA(n_components = 50).fit(wordsSubMatrix)\n",
        "reducedPCA_data = pcaWords.transform(wordsSubMatrix)\n",
        "#T-SNE is theoretically better, but you should experiment\n",
        "tsneWords = sklearn.manifold.TSNE(n_components = 2).fit_transform(reducedPCA_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xD_c8MMk2ekk"
      },
      "source": [
        "We now can plot the points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Gl7gc4s2e-l"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize = (10,6))\n",
        "ax = fig.add_subplot(111)\n",
        "ax.set_frame_on(False)\n",
        "plt.scatter(tsneWords[:, 0], tsneWords[:, 1], alpha = 0)#Making the points invisible\n",
        "for i, word in enumerate(targetWords):\n",
        "    ax.annotate(word, (tsneWords[:, 0][i],tsneWords[:, 1][i]), size =  20 * (numWords - i) / numWords)\n",
        "plt.xticks(())\n",
        "plt.yticks(())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBB9_rTY2hyB"
      },
      "source": [
        "Note how the TTIC projection was a lot more satisfactory because it projected related but differentiated words. Our example plots seemingly random words. We encourage that you try this example again with a curated list of words and see how the visualisation improves!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOaHSDEe2idq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpyiCDHKSGYY"
      },
      "source": [
        "### Projecting Embeddings on Dimensions of Social Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_S2FBu-qSGYY"
      },
      "source": [
        "We can see how word2vec does remarkably well in capturing certain aspects of semantics! Word embeddings have a lot of utility outside of just looking at relations between words, though. Because words derive their meanings from various social characteristics and dimensions, it is possible for us to project word vectors onto word-defined dimensions to see where they lie. We can create these dimensions by choosing words which that might anchor the dimension and then project other words onto this dimension. For example, if we were trying to find a gender dimension, we might make a vector between the words he, him, man and she, her, woman, and so on. Then, with a cosine projection, we can get a value of where our word of choice might lie on this dimension, revealing to us how “gendered” the word is. The following snippets of code implements this for various such dimensions. Using such a technique for social scientific analysis as described by Kozlowski et al 2019; it can be very easily extended for many business and analytical solutions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yr85WkR2SGYZ"
      },
      "source": [
        "First we can visualize with dimension reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "anV88JgpSGYZ"
      },
      "outputs": [],
      "source": [
        "#words to create dimensions\n",
        "tnytTargetWords = ['man','him','he', 'woman', 'her', 'she', 'black','blacks','African', 'white', 'whites', 'Caucasian', 'rich', 'richer', 'richest', 'expensive', 'wealthy', 'poor', 'poorer', 'poorest', 'cheap', 'inexpensive']\n",
        "#words we will be mapping\n",
        "tnytTargetWords += [\"doctor\",\"lawyer\",\"plumber\",\"scientist\",\"hairdresser\", \"nanny\",\"carpenter\",\"entrepreneur\",\"musician\",\"writer\", \"banker\",\"poet\",\"nurse\", \"steak\", \"bacon\", \"croissant\", \"cheesecake\", \"salad\", \"cheeseburger\", \"vegetables\", \"beer\", \"wine\", \"pastry\", \"basketball\", \"baseball\", \"boxing\", \"softball\", \"volleyball\", \"tennis\", \"golf\", \"hockey\", \"soccer\"]\n",
        "\n",
        "\n",
        "wordsSubMatrix = []\n",
        "for word in tnytTargetWords:\n",
        "    wordsSubMatrix.append(nytimes_w2v_model.wv[word])\n",
        "wordsSubMatrix = np.array(wordsSubMatrix)\n",
        "#wordsSubMatrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "m2D_qnW7SGYZ"
      },
      "outputs": [],
      "source": [
        "pcaWordsNYT = sklearn.decomposition.PCA(n_components = 50).fit(wordsSubMatrix)\n",
        "reducedPCA_dataNYT = pcaWordsNYT.transform(wordsSubMatrix)\n",
        "#T-SNE is theoretically better, but you should experiment\n",
        "tsneWordsNYT = sklearn.manifold.TSNE(n_components = 2).fit_transform(reducedPCA_dataNYT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9JLMNGvSGYf"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize = (20,12))\n",
        "ax = fig.add_subplot(111)\n",
        "ax.set_frame_on(False)\n",
        "plt.scatter(tsneWordsNYT[:, 0], tsneWordsNYT[:, 1], alpha = 0) #Making the points invisible\n",
        "\n",
        "for i, word in enumerate(tnytTargetWords):\n",
        "    ax.annotate(word, (tsneWordsNYT[:, 0][i],tsneWordsNYT[:, 1][i]), size =  50 * (len(tnytTargetWords) - i) / len(tnytTargetWords))\n",
        "\n",
        "plt.xticks(())\n",
        "plt.yticks(())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keteMCbaSGYg"
      },
      "source": [
        "Define some convenient functions for getting dimensions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9B-vMN93SGYh"
      },
      "outputs": [],
      "source": [
        "def normalize(vector):\n",
        "    normalized_vector = vector / np.linalg.norm(vector)\n",
        "    return normalized_vector\n",
        "\n",
        "def dimension(model, positives, negatives):\n",
        "    diff = sum([normalize(model.wv[x]) for x in positives]) - sum([normalize(model.wv[y]) for y in negatives])\n",
        "    return diff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THSdZIuOSGYh"
      },
      "source": [
        "Let's calculate three dimensions: gender, race, and class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "GOKUKVmGSGYh"
      },
      "outputs": [],
      "source": [
        "Gender = dimension(nytimes_w2v_model, ['man','him','he'], ['woman', 'her', 'she'])\n",
        "Race = dimension(nytimes_w2v_model, ['black','blacks','African'], ['white', 'whites', 'Caucasian'])\n",
        "Class = dimension(nytimes_w2v_model, ['rich', 'richer', 'richest', 'expensive', 'wealthy'], ['poor', 'poorer', 'poorest', 'cheap', 'inexpensive'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLeoeBkBSGYh"
      },
      "source": [
        "Here we have some words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3wQdvi4BSGYi"
      },
      "outputs": [],
      "source": [
        "Occupations = [\"doctor\",\"lawyer\",\"plumber\",\"scientist\",\"hairdresser\", \"nanny\",\"carpenter\",\"entrepreneur\",\"musician\",\"writer\", \"banker\",\"poet\",\"nurse\"]\n",
        "\n",
        "Foods = [\"steak\", \"bacon\", \"croissant\", \"cheesecake\", \"salad\", \"cheeseburger\", \"vegetables\", \"beer\", \"wine\", \"pastry\"]\n",
        "\n",
        "Sports  = [\"basketball\", \"baseball\", \"boxing\", \"softball\", \"volleyball\", \"tennis\", \"golf\", \"hockey\", \"soccer\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22raR1Y9SGYi"
      },
      "source": [
        "Define a function to project words in a word list to each of the three dimensions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "oOE2Xf6NSGYj"
      },
      "outputs": [],
      "source": [
        "def makeDF(model, word_list):\n",
        "    g = []\n",
        "    r = []\n",
        "    c = []\n",
        "    for word in word_list:\n",
        "        g.append(sklearn.metrics.pairwise.cosine_similarity(nytimes_w2v_model.wv[word].reshape(1,-1), Gender.reshape(1,-1))[0][0])\n",
        "        r.append(sklearn.metrics.pairwise.cosine_similarity(nytimes_w2v_model.wv[word].reshape(1,-1), Race.reshape(1,-1))[0][0])\n",
        "        c.append(sklearn.metrics.pairwise.cosine_similarity(nytimes_w2v_model.wv[word].reshape(1,-1), Class.reshape(1,-1))[0][0])\n",
        "    df = pd.DataFrame({'gender': g, 'race': r, 'class': c}, index = word_list)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocAm5HjjSGYj"
      },
      "source": [
        "Get the projections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "UY7x91KZSGYj"
      },
      "outputs": [],
      "source": [
        "OCCdf = makeDF(nytimes_w2v_model, Occupations)\n",
        "Fooddf = makeDF(nytimes_w2v_model, Foods)\n",
        "Sportsdf = makeDF(nytimes_w2v_model, Sports)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNZEgWtSSGYj"
      },
      "source": [
        "Define some useful functions for plotting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4TAYDcQgSGYk"
      },
      "outputs": [],
      "source": [
        "def Coloring(Series):\n",
        "    x = Series.values\n",
        "    y = x-x.min()\n",
        "    z = y/y.max()\n",
        "    c = list(plt.cm.rainbow(z))\n",
        "    return c\n",
        "\n",
        "def PlotDimension(ax,df, dim):\n",
        "    ax.set_frame_on(False)\n",
        "    ax.set_title(dim, fontsize = 20)\n",
        "    colors = Coloring(df[dim])\n",
        "    for i, word in enumerate(df.index):\n",
        "        ax.annotate(word, (0, df[dim][i]), color = colors[i], alpha = 0.6, fontsize = 12)\n",
        "    MaxY = df[dim].max()\n",
        "    MinY = df[dim].min()\n",
        "    plt.ylim(MinY,MaxY)\n",
        "    plt.yticks(())\n",
        "    plt.xticks(())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrI1stCQSGYk"
      },
      "source": [
        "Plot the occupational words in each of the three dimensions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1BOXgIBSGYk",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize = (12,4))\n",
        "ax1 = fig.add_subplot(131)\n",
        "PlotDimension(ax1, OCCdf, 'gender')\n",
        "ax2 = fig.add_subplot(132)\n",
        "PlotDimension(ax2, OCCdf, 'race')\n",
        "ax3 = fig.add_subplot(133)\n",
        "PlotDimension(ax3, OCCdf, 'class')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00nQWm0QSGYk"
      },
      "source": [
        "Foods:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5v8bTfUNSGYk"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize = (12,4))\n",
        "ax1 = fig.add_subplot(131)\n",
        "PlotDimension(ax1, Fooddf, 'gender')\n",
        "ax2 = fig.add_subplot(132)\n",
        "PlotDimension(ax2, Fooddf, 'race')\n",
        "ax3 = fig.add_subplot(133)\n",
        "PlotDimension(ax3, Fooddf, 'class')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyYvqFqESGYl"
      },
      "source": [
        "Sports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdL2ZBKVSGYl"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize = (12,4))\n",
        "ax1 = fig.add_subplot(131)\n",
        "PlotDimension(ax1, Sportsdf, 'gender')\n",
        "ax2 = fig.add_subplot(132)\n",
        "PlotDimension(ax2, Sportsdf, 'race')\n",
        "ax3 = fig.add_subplot(133)\n",
        "PlotDimension(ax3, Sportsdf, 'class')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHQ--EsWoxGM"
      },
      "source": [
        "### Debiasing Word Embeddings\n",
        "\n",
        "As we are creating these dimensions, it may become clear that such an approach may be adapted to measure socio-cultural biases in embeddings. Language models are trained on human generated corpora and text, so they end up reflecting the social and cultural biases present in text (Caliskan et al. 2017). Keeping in mind that the semantics we generate from natural language corpora reflect real world biases which may be harmful (for e.g, in the non modified,  biased models, man:computer programer:: woman:homemaker), there has been significant work in attempting to correct these biases (Bolukbasi et al 2016). These methods work similarly to the projection based methods we explored, and the following code includes code to debias embedding models so that generated associations (e.g., as in a job ad) do not propagate the bias.\n",
        "\n",
        "In the following section, we use the code from this repository associated with the famous \"Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings\" paper: https://github.com/tolga-b/debiaswe\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOh0mplXRt9W"
      },
      "source": [
        "#### Hands-on Tutorial: Quantifying and Reducing Gender Stereotypes in Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkZYIOzrRt9Z"
      },
      "source": [
        "Ensuring fairness in algorithmically-driven decision-making is important to avoid inadvertent cases of bias and perpetuation of harmful stereotypes. However, modern natural language processing techniques, which learn model parameters based on data, might rely on implicit biases presented in the data to make undesirable stereotypical associations. Such a danger faces us with word embeddings, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. Recent results ([1](https://arxiv.org/abs/1607.06520), [2](https://arxiv.org/abs/1608.07187)) show that word embeddings trained on a Google News articles exhibit extensive female/male gender stereotypes. This raises concerns because their widespread use may tend to amplify these biases.\n",
        "\n",
        "In the following, we provide step-by-step instructions to demonstrate and quanitfy the biases in word embedding.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ja4DZ671Rt9a"
      },
      "outputs": [],
      "source": [
        "# Setup:\n",
        "# Clone the code repository from https://github.com/tolga-b/debiaswe.git\n",
        "!mkdir debiaswe_tutorial\n",
        "!cd debiaswe_tutorial\n",
        "!git clone https://github.com/tolga-b/debiaswe.git\n",
        "\n",
        "# To reduce the time of downloading data, we provide as subset of GoogleNews-vectors in the following location:\n",
        "# https://drive.google.com/file/d/1NH6jcrg8SXbnhpIXRIXF_-KUE7wGxGaG/view?usp=sharing\n",
        "\n",
        "# For full embeddings:\n",
        "# Download embeddings at https://github.com/tolga-b/debiaswe and put them on the following directory\n",
        "# embeddings/GoogleNews-vectors-negative300-hard-debiased.bin\n",
        "# embeddings/GoogleNews-vectors-negative300.bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7SCHdTbgRt9b"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function, division\n",
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import debiaswe.debiaswe as dwe\n",
        "import debiaswe.debiaswe.we as we\n",
        "from debiaswe.debiaswe.we import WordEmbedding\n",
        "from debiaswe.debiaswe.data import load_professions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOInZ2CrRt9c"
      },
      "source": [
        "#### Part 1: Gender Bias in Word Embedding\n",
        "\n",
        "\n",
        "##### Step 1: Load data\n",
        "We first load the word embedding trained on a corpus of Google News texts consisting of 3 million English words and terms. The embedding maps each word into a 300-dimension vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5xBNhxSRt9c"
      },
      "outputs": [],
      "source": [
        "# load google news word2vec\n",
        "E = WordEmbedding('/content/drive/MyDrive/w2v_gnews_small.txt')      # change this path to the location you uploaded the file\n",
        "\n",
        "# load professions\n",
        "professions = load_professions()\n",
        "profession_words = [p[0] for p in professions]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRoEk2-9Rt9d"
      },
      "source": [
        "#### Step 2: Define gender direction\n",
        "\n",
        "We define gender direction by the direction of she - he because they are frequent and do not have alternative word senses (e.g., man can also refer to mankind)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "XHvK-jCkRt9d"
      },
      "outputs": [],
      "source": [
        "# gender direction\n",
        "v_gender = E.diff('she', 'he')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8DMitGKRt9f"
      },
      "source": [
        "#### Step 3: Generating analogies of \"Man: x :: Woman : y\"\n",
        "\n",
        "We show that the word embedding model generates gender-streotypical analogy pairs.\n",
        "To generate the analogy pairs, we use an analogy score defined in the paper mentioned above. This score finds word pairs well aligned with gender direction as well as within a short distance from each other to preserve topic consistency.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "_VWGTYcWRt9f",
        "outputId": "500b9bf0-c32f-4339-8279-44f53f687fe2"
      },
      "outputs": [],
      "source": [
        "# analogies gender\n",
        "a_gender = E.best_analogies_dist_thresh(v_gender)\n",
        "\n",
        "for (a,b,c) in a_gender:\n",
        "    print(a+\"-\"+b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlFbHws2Rt9g"
      },
      "source": [
        "#### Step 4: Analyzing gender bias in word vectors asscoiated with professions\n",
        "\n",
        "Next, we show that many occupations are unintendedly associated with either male of female by projecting their word vectors onto the gender dimension.\n",
        "\n",
        "The script will output the profession words sorted with respect to the projection score in the direction of gender."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RM5LmK0mRt9g"
      },
      "outputs": [],
      "source": [
        "# profession analysis gender\n",
        "sp = sorted([(E.v(w).dot(v_gender), w) for w in profession_words if w in E.index])\n",
        "\n",
        "sp[0:20], sp[-20:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBnODI2qRt9h"
      },
      "source": [
        "#### Part 2 Racial Bias\n",
        "\n",
        "##### Step 5: Define racial direction\n",
        "We define racial direction based on the common names in different Demographic groups."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "gPyYIvchRt9h"
      },
      "outputs": [],
      "source": [
        "names = [\"Emily\", \"Aisha\", \"Anne\", \"Keisha\", \"Jill\", \"Tamika\", \"Allison\", \"Lakisha\", \"Laurie\", \"Tanisha\", \"Sarah\",\n",
        "         \"Latoya\", \"Meredith\", \"Kenya\", \"Carrie\", \"Latonya\", \"Kristen\", \"Ebony\", \"Todd\", \"Rasheed\", \"Neil\", \"Tremayne\",\n",
        "         \"Geoffrey\", \"Kareem\", \"Brett\", \"Darnell\", \"Brendan\", \"Tyrone\", \"Greg\", \"Hakim\", \"Matthew\", \"Jamal\", \"Jay\",\n",
        "         \"Leroy\", \"Brad\", \"Jermaine\"]\n",
        "names_group1 = [names[2 * i] for i in range(len(names) // 2)]\n",
        "names_group2 = [names[2 * i + 1] for i in range(len(names) // 2)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Mr2ch1aGRt9h"
      },
      "outputs": [],
      "source": [
        "# racial direction\n",
        "vs = [sum(E.v(w) for w in names if w in E.index) for names in (names_group2, names_group1)]\n",
        "vs = [v / np.linalg.norm(v) for v in vs]\n",
        "\n",
        "v_racial = vs[1] - vs[0]\n",
        "v_racial = v_racial / np.linalg.norm(v_racial)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hFhnlvlRt9i"
      },
      "source": [
        "#### Step 6: Generating racial biased analogies\n",
        "\n",
        "Similar to Step 3, we generate analogies that align with the racial dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSf2dYUgRt9i"
      },
      "outputs": [],
      "source": [
        "# racial analogies\n",
        "a_racial = E.best_analogies_dist_thresh(v_racial)\n",
        "\n",
        "for (a,b,c) in a_racial:\n",
        "    print(a+\"-\"+b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDWSUCHXRt9i"
      },
      "source": [
        "#### Step 7: Analyzing racial bias in word vectors asscoiated with professions\n",
        "\n",
        "Similar to Step 4, we project occpurations onto the racial dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2rEcdmjRt9j"
      },
      "outputs": [],
      "source": [
        "# profession analysis racial\n",
        "sp = sorted([(E.v(w).dot(v_racial), w) for w in profession_words])\n",
        "\n",
        "sp[0:20], sp[-20:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "2QYw_KsjRt9j"
      },
      "source": [
        "#### Practice\n",
        "\n",
        "Repeat Step 2-4 with debiased word embedding.\n",
        "\n",
        "You can use debiaswe debias function to do the debiasing with word sets of your choosing\n",
        "\n",
        "You can leave equalize_pairs and gender_specific_words blank when coming up with your own groups. We give an example for the case of gender below for you to warm up.\n",
        "\n",
        "<font color=\"red\">Resolving the `ModuleNotFoundError: No module named 'debiaswe.debiasewe'` Issue</font>\n",
        "\n",
        "If you encounter the error `ModuleNotFoundError: No module named 'debiaswe.debiasewe'`, follow these steps to resolve it:\n",
        "\n",
        "1. Navigate to the `debiaswe` directory, then to the nested `debiaswe` folder, and open the `debiaswe.py` file.\n",
        "2. Locate **line 2** in the file.\n",
        "3. Replace the following line:\n",
        "\n",
        "   ```python\n",
        "   import we\n",
        "   ```\n",
        "   with:\n",
        "   ```python\n",
        "   from . import we\n",
        "   ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "EgAr7Nj_Rt9k"
      },
      "outputs": [],
      "source": [
        "from debiaswe.debiaswe.debias import debias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kj1XUYGcRt9k"
      },
      "outputs": [],
      "source": [
        "# Lets load some gender related word lists to help us with debiasing\n",
        "with open('debiaswe/data/definitional_pairs.json', \"r\") as f:\n",
        "    defs = json.load(f)\n",
        "print(\"definitional\", defs)\n",
        "\n",
        "with open('debiaswe/data/equalize_pairs.json', \"r\") as f:\n",
        "    equalize_pairs = json.load(f)\n",
        "\n",
        "with open('debiaswe/data/gender_specific_seed.json', \"r\") as f:\n",
        "    gender_specific_words = json.load(f)\n",
        "print(\"gender specific\", len(gender_specific_words), gender_specific_words[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9C3POgKRt9l"
      },
      "outputs": [],
      "source": [
        "debias(E, gender_specific_words, defs, equalize_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rISqlrSkRt9l"
      },
      "outputs": [],
      "source": [
        "# profession analysis gender\n",
        "sp_debiased = sorted([(E.v(w).dot(v_gender), w) for w in profession_words])\n",
        "\n",
        "sp_debiased[0:20], sp_debiased[-20:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gb-fVp2tRt9m"
      },
      "outputs": [],
      "source": [
        "# analogies gender\n",
        "a_gender_debiased = E.best_analogies_dist_thresh(v_gender)\n",
        "\n",
        "for (a,b,c) in a_gender_debiased:\n",
        "    print(a+\"-\"+b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZJ77gpjdq4y"
      },
      "source": [
        "### Retrofitting Word Vectors\n",
        "\n",
        "We can debias word embeddings (or try to, at least) based on how we can move word vectors around in this high dimensional space. Word2vec and related methods are static in that they are trained only once and word vectors don’t change based on context. Nevertheless, research by (Faruqui et al. 2015) has shown that we can improve the performance of word embeddings on a series of tasks by retrofitting them, which involves using an external lexicon (collection of words) to move certain words closer together or farther apart. While the authors use this to improve performance on a series of semantic tasks, we can be creative in how we want to retrofit our embeddings. Below are instructions for performing this kind of task.\n",
        "\n",
        "This is taken from the repository from the paper cited above:\n",
        "Retrofitting word vectors: https://github.com/mfaruqui/retrofitting\n",
        "\n",
        "For the [retrofit.py](https://github.com/mfaruqui/retrofitting/blob/master/retrofit.py) file, use this link. The code is also copy pasted in the code cell below this.\n",
        "\n",
        "#### Data you need\n",
        "\n",
        "Word vector file\n",
        "Lexicon file (provided here)\n",
        "Each vector file should have one word vector per line as follows (space delimited):-\n",
        "\n",
        "the -1.0 2.4 -0.3 ...\n",
        "\n",
        "#### Running the program\n",
        "\n",
        "python retrofit.py -i word_vec_file -l lexicon_file -n num_iter -o out_vec_file\n",
        "\n",
        "python retrofit.py -i sample_vec.txt -l lexicons/ppdb-xl.txt -n 10 -o out_vec.txt\n",
        "\n",
        "where, 'n' is an integer which specifies the number of iterations for which the optimization is to be performed. Usually n = 10 gives reasonable results.\n",
        "\n",
        "#### Output File: out_vec.txt\n",
        "\n",
        "which are your new retrofitted and (hopefully) improved word vectors, enjoy !\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qn85VN3Ddq4z"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import gzip\n",
        "import math\n",
        "import numpy\n",
        "import re\n",
        "import sys\n",
        "\n",
        "from copy import deepcopy\n",
        "\n",
        "isNumber = re.compile(r'\\d+.*')\n",
        "def norm_word(word):\n",
        "  if isNumber.search(word.lower()):\n",
        "    return '---num---'\n",
        "  elif re.sub(r'\\W+', '', word) == '':\n",
        "    return '---punc---'\n",
        "  else:\n",
        "    return word.lower()\n",
        "\n",
        "''' Read all the word vectors and normalize them '''\n",
        "def read_word_vecs(filename):\n",
        "  wordVectors = {}\n",
        "  if filename.endswith('.gz'): fileObject = gzip.open(filename, 'r')\n",
        "  else: fileObject = open(filename, 'r')\n",
        "\n",
        "  for line in fileObject:\n",
        "    line = line.strip().lower()\n",
        "    word = line.split()[0]\n",
        "    wordVectors[word] = numpy.zeros(len(line.split())-1, dtype=float)\n",
        "    for index, vecVal in enumerate(line.split()[1:]):\n",
        "      wordVectors[word][index] = float(vecVal)\n",
        "    ''' normalize weight vector '''\n",
        "    wordVectors[word] /= math.sqrt((wordVectors[word]**2).sum() + 1e-6)\n",
        "\n",
        "  sys.stderr.write(\"Vectors read from: \"+filename+\" \\n\")\n",
        "  return wordVectors\n",
        "\n",
        "''' Write word vectors to file '''\n",
        "def print_word_vecs(wordVectors, outFileName):\n",
        "  sys.stderr.write('\\nWriting down the vectors in '+outFileName+'\\n')\n",
        "  outFile = open(outFileName, 'w')\n",
        "  for word, values in wordVectors.iteritems():\n",
        "    outFile.write(word+' ')\n",
        "    for val in wordVectors[word]:\n",
        "      outFile.write('%.4f' %(val)+' ')\n",
        "    outFile.write('\\n')\n",
        "  outFile.close()\n",
        "\n",
        "''' Read the PPDB word relations as a dictionary '''\n",
        "def read_lexicon(filename):\n",
        "  lexicon = {}\n",
        "  for line in open(filename, 'r'):\n",
        "    words = line.lower().strip().split()\n",
        "    lexicon[norm_word(words[0])] = [norm_word(word) for word in words[1:]]\n",
        "  return lexicon\n",
        "\n",
        "''' Retrofit word vectors to a lexicon '''\n",
        "def retrofit(wordVecs, lexicon, numIters):\n",
        "  newWordVecs = deepcopy(wordVecs)\n",
        "  wvVocab = set(newWordVecs.keys())\n",
        "  loopVocab = wvVocab.intersection(set(lexicon.keys()))\n",
        "  for it in range(numIters):\n",
        "    # loop through every node also in ontology (else just use data estimate)\n",
        "    for word in loopVocab:\n",
        "      wordNeighbours = set(lexicon[word]).intersection(wvVocab)\n",
        "      numNeighbours = len(wordNeighbours)\n",
        "      #no neighbours, pass - use data estimate\n",
        "      if numNeighbours == 0:\n",
        "        continue\n",
        "      # the weight of the data estimate if the number of neighbours\n",
        "      newVec = numNeighbours * wordVecs[word]\n",
        "      # loop over neighbours and add to new vector (currently with weight 1)\n",
        "      for ppWord in wordNeighbours:\n",
        "        newVec += newWordVecs[ppWord]\n",
        "      newWordVecs[word] = newVec/(2*numNeighbours)\n",
        "  return newWordVecs\n",
        "\n",
        "if __name__=='__main__':\n",
        "\n",
        "  parser = argparse.ArgumentParser()\n",
        "  parser.add_argument(\"-i\", \"--input\", type=str, default=None, help=\"Input word vecs\")\n",
        "  parser.add_argument(\"-l\", \"--lexicon\", type=str, default=None, help=\"Lexicon file name\")\n",
        "  parser.add_argument(\"-o\", \"--output\", type=str, help=\"Output word vecs\")\n",
        "  parser.add_argument(\"-n\", \"--numiter\", type=int, default=10, help=\"Num iterations\")\n",
        "  args = parser.parse_args()\n",
        "\n",
        "  wordVecs = read_word_vecs(args.input)\n",
        "  lexicon = read_lexicon(args.lexicon)\n",
        "  numIter = int(args.numiter)\n",
        "  outFileName = args.output\n",
        "\n",
        "  ''' Enrich the word vectors using ppdb and print the enriched vectors '''\n",
        "  print_word_vecs(retrofit(wordVecs, lexicon, numIter), outFileName)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mPIfaUbpAIh"
      },
      "source": [
        "### Aligning Word Embeddings\n",
        "\n",
        "We've explored alignment in some detail in tutorial 2.2, on Data Integration and Alignment. Specifically, we looked at alignment across time and across languages -  in this section, we will revisit these examples.\n",
        "\n",
        "\n",
        "NOTE: the following section has previously seen code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyxK538kFO6k"
      },
      "source": [
        "#### Time based data\n",
        "\n",
        "In order to explore this, let's get some data that follows a time trend. We'll look at conference proceedings from the American Society for Clinical Oncologists. You can download the data [here](https://drive.google.com/file/d/1R9EiThdJQ3vY84xcoPJGUBeqvemu_o8L/view?usp=sharing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhGzbmRKgxjv"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "ascoDF = pd.read_csv(\"/content/ASCO_abstracts.csv\", index_col=0, on_bad_lines='skip', delimiter=',')\n",
        "# to speed up...\n",
        "ascoDF = ascoDF.sample(frac=0.05, random_state=60615)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnQwwPcfLK-N"
      },
      "outputs": [],
      "source": [
        "# This may take quite a while to finish....\n",
        "# Tokenize sentences with a progress bar\n",
        "ascoDF['tokenized_sents'] = ascoDF['Body'].progress_apply(\n",
        "    lambda x: [lucem_illud.word_tokenize(s) for s in lucem_illud.sent_tokenize(x)]\n",
        ")\n",
        "\n",
        "# Normalize tokens with a progress bar\n",
        "ascoDF['normalized_sents'] = ascoDF['tokenized_sents'].progress_apply(\n",
        "    lambda x: [lucem_illud.normalizeTokens(s) for s in x]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AH-koGZuFO6j"
      },
      "outputs": [],
      "source": [
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8O7D6y_HXDP_"
      },
      "source": [
        "#### Cross Temporal Alignment\n",
        "\n",
        "Below is code that aligns the dimensions of multiple embeddings arrayed over time or some other dimension and allow identification of semantic change as the word vectors change their loadings for focal words. This code comes from the approach piloted at Stanford by William Hamilton, Daniel Jurafsky and Jure Lescovec [here](https://arxiv.org/pdf/1605.09096.pdf).\n",
        "\n",
        "In this case we train the models ourselves instead of using a pre-trained model, so this might take some time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8rKAcNkFO6k"
      },
      "outputs": [],
      "source": [
        "def calc_vectors_norm(model):\n",
        "    \"\"\"\n",
        "    Returns a manually normalized version of the Gensim model's vectors\n",
        "    as a (num_words x embedding_dim) numpy array.\n",
        "    \"\"\"\n",
        "    vectors = model.wv.vectors\n",
        "    # If your model hasn't yet built vectors, you might need to handle that separately\n",
        "    norms = np.sqrt((vectors ** 2).sum(axis=1, keepdims=True))\n",
        "    return (vectors / norms).astype(np.float32)\n",
        "\n",
        "def intersection_align_gensim(m1, m2, words=None):\n",
        "    \"\"\"\n",
        "    Intersect two Gensim 4+ Word2Vec models m1 and m2 so they share the same vocabulary.\n",
        "    If `words` is given, intersect that list/set as well.\n",
        "    The models' vectors, key_to_index, and index_to_key are updated in place.\n",
        "\n",
        "    Returns (m1, m2).\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the vocab for each model\n",
        "    vocab_m1 = set(m1.wv.key_to_index.keys())\n",
        "    vocab_m2 = set(m2.wv.key_to_index.keys())\n",
        "\n",
        "    # Find the common vocabulary\n",
        "    common_vocab = vocab_m1.intersection(vocab_m2)\n",
        "    if words:\n",
        "        common_vocab = common_vocab.intersection(set(words))\n",
        "\n",
        "    # If the vocab is already identical, just return\n",
        "    if (vocab_m1 == common_vocab) and (vocab_m2 == common_vocab):\n",
        "        return (m1, m2)\n",
        "\n",
        "    # Otherwise, sort by descending frequency\n",
        "    # Gensim 4+ stores counts in model.wv.get_vecattr(word, \"count\") (if available)\n",
        "    def total_count(word):\n",
        "        return (m1.wv.get_vecattr(word, \"count\") or 0) + \\\n",
        "               (m2.wv.get_vecattr(word, \"count\") or 0)\n",
        "\n",
        "    common_vocab = list(common_vocab)\n",
        "    common_vocab.sort(key=total_count, reverse=True)\n",
        "\n",
        "    # Rebuild each model so it has only the common vocabulary\n",
        "    for model in [m1, m2]:\n",
        "        old_key_to_index = model.wv.key_to_index\n",
        "        old_vectors = model.wv.vectors\n",
        "        old_norms = calc_vectors_norm(model)  # We'll use this to preserve alignment\n",
        "\n",
        "        # Identify the rows (indices) that correspond to the common vocab words\n",
        "        indices = [old_key_to_index[w] for w in common_vocab]\n",
        "\n",
        "        # Rebuild the main vectors\n",
        "        new_vectors = old_norms[indices]\n",
        "        model.wv.vectors = new_vectors\n",
        "        model.wv.vectors_norm = None  # Force recalculation if needed\n",
        "\n",
        "        # Rebuild key_to_index and index_to_key\n",
        "        model.wv.key_to_index = {}\n",
        "        model.wv.index_to_key = []\n",
        "        for new_idx, word in enumerate(common_vocab):\n",
        "            model.wv.key_to_index[word] = new_idx\n",
        "            model.wv.index_to_key.append(word)\n",
        "\n",
        "        # If you want to keep track of counts, you can also do something like:\n",
        "        # for word in common_vocab:\n",
        "        #     old_count = model.wv.get_vecattr(word, \"count\")\n",
        "        #     if old_count is not None:\n",
        "        #         model.wv.set_vecattr(word, \"count\", old_count)\n",
        "\n",
        "    return (m1, m2)\n",
        "\n",
        "\n",
        "def smart_procrustes_align_gensim(base_embed, other_embed, words=None):\n",
        "    \"\"\"\n",
        "    Procrustes-align two Gensim 4+ Word2Vec models (for comparing same words across models).\n",
        "\n",
        "    1. Intersect their vocabularies (and optionally intersect with `words`).\n",
        "    2. Align the `other_embed` model to the `base_embed` model via Procrustes.\n",
        "    3. Modify `other_embed` in place and return it.\n",
        "    \"\"\"\n",
        "\n",
        "    # Make (shallow) copies so we don't edit the originals\n",
        "    base_embed = copy.copy(base_embed)\n",
        "    other_embed = copy.copy(other_embed)\n",
        "\n",
        "    # 1. Intersect the vocabularies\n",
        "    in_base_embed, in_other_embed = intersection_align_gensim(base_embed, other_embed, words=words)\n",
        "\n",
        "    # 2. Get the normalized vectors from both\n",
        "    base_vecs = calc_vectors_norm(in_base_embed)\n",
        "    other_vecs = calc_vectors_norm(in_other_embed)\n",
        "\n",
        "    # Dot product and SVD\n",
        "    m = np.dot(other_vecs.T, base_vecs)\n",
        "    u, _, v = np.linalg.svd(m)\n",
        "    ortho = np.dot(u, v)  # orthogonal transformation\n",
        "\n",
        "    # Apply the transformation to other_embed's vectors\n",
        "    new_other_vecs = np.dot(calc_vectors_norm(other_embed), ortho)\n",
        "    other_embed.wv.vectors = new_other_vecs\n",
        "    other_embed.wv.vectors_norm = None  # reset, so Gensim can recalc norms if needed\n",
        "\n",
        "    return other_embed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVgJogQQFO6l"
      },
      "outputs": [],
      "source": [
        "def compareModels(df, category, sort=True):\n",
        "    \"\"\"\n",
        "    Example function that trains a separate Word2Vec model for each distinct\n",
        "    value of `df[category]`, then aligns them pairwise via Procrustes.\n",
        "    \"\"\"\n",
        "\n",
        "    embeddings_raw = {}\n",
        "    cats = sorted(set(df[category]))\n",
        "\n",
        "    # Train a model per category (simplified example)\n",
        "    for cat in cats:\n",
        "        print(f\"Embedding {cat}\", end='\\r')\n",
        "        subsetDF = df[df[category] == cat]\n",
        "        # Adjust Word2Vec parameters to your preference\n",
        "        model = Word2Vec(\n",
        "            sentences=subsetDF['normalized_sents'].sum(),\n",
        "            vector_size=100,  # or any dimension you like\n",
        "            min_count=1,\n",
        "            workers=4\n",
        "        )\n",
        "        embeddings_raw[cat] = model\n",
        "\n",
        "    # Align each model to the first model in cats, for instance\n",
        "    embeddings_aligned = {}\n",
        "    for outer_cat in cats:\n",
        "        # Start with the base\n",
        "        base_model = embeddings_raw[outer_cat]\n",
        "        embeddings_aligned[outer_cat] = [base_model]\n",
        "        for inner_cat in cats:\n",
        "            if inner_cat == outer_cat:\n",
        "                continue\n",
        "            aligned = smart_procrustes_align_gensim(\n",
        "                embeddings_aligned[outer_cat][-1],\n",
        "                embeddings_raw[inner_cat]\n",
        "            )\n",
        "            embeddings_aligned[outer_cat].append(aligned)\n",
        "\n",
        "    return embeddings_raw, embeddings_aligned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uv6Bl_KBFO6m"
      },
      "outputs": [],
      "source": [
        "rawEmbeddings, comparedEmbeddings = compareModels(ascoDF, 'Year')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h_AQHSlFO6m"
      },
      "source": [
        "This is the key to our aligning - the smart procrustes align method here is the one doing the aligning, and we are adding the alligned embeddings to the dictionary.\n",
        "\n",
        "We need to compare them across all permutions so we will define another function to help, we will be using 1 - cosine similarity as that gives a more intitive range of 0-2 with low values meaning little change and high meaning lots of change."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2nwc6rpFO6m"
      },
      "outputs": [],
      "source": [
        "def getDivergenceDF(word, embeddingsDict):\n",
        "    \"\"\"\n",
        "    Computes absolute (1 - cosine_similarity) for `word` between a 'base' model\n",
        "    (index 0 in each category list) and each subsequent model (aligned versions).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    word : str\n",
        "        Target word.\n",
        "    embeddingsDict : dict\n",
        "        {\n",
        "            category: [model_base, model_aligned1, model_aligned2, ...],\n",
        "            ...\n",
        "        }\n",
        "        Each value is a list of Word2Vec models.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Rows are categories, columns are each aligned model's divergence from the base.\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "    cats = sorted(embeddingsDict.keys())\n",
        "    dists = {}\n",
        "\n",
        "    for cat in cats:\n",
        "        # Safety check: does this category have at least one model?\n",
        "        if len(embeddingsDict[cat]) == 0:\n",
        "            dists[cat] = []\n",
        "            continue\n",
        "\n",
        "        # The first model is our \"base\" model\n",
        "        base_model = embeddingsDict[cat][0]\n",
        "\n",
        "        # Retrieve base vector for the target word\n",
        "        # Make sure the word actually exists in the vocabulary\n",
        "        if word not in base_model.wv:\n",
        "            # If missing, store something (e.g., np.nan) or skip\n",
        "            dists[cat] = []\n",
        "            continue\n",
        "\n",
        "        base_vec = base_model.wv[word].reshape(1, -1)\n",
        "\n",
        "        # Compare against all subsequent models in this category\n",
        "        cat_dists = []\n",
        "        for aligned_model in embeddingsDict[cat][1:]:\n",
        "            if word not in aligned_model.wv:\n",
        "                # If the word doesn't exist in that model, handle it\n",
        "                cat_dists.append(np.nan)\n",
        "                continue\n",
        "\n",
        "            aligned_vec = aligned_model.wv[word].reshape(1, -1)\n",
        "            cos_sim = cosine_similarity(base_vec, aligned_vec)[0, 0]\n",
        "            divergence = np.abs(1 - cos_sim)\n",
        "            cat_dists.append(divergence)\n",
        "\n",
        "        dists[cat] = cat_dists\n",
        "\n",
        "    # Each category's row in the DataFrame shows divergences for that category\n",
        "    # across the aligned models (in order).\n",
        "    return pd.DataFrame.from_dict(dists, orient='index')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3izzZMkFO6m"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPuazOr1FO6n"
      },
      "outputs": [],
      "source": [
        "import seaborn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcM_q8YkzW1c"
      },
      "source": [
        "We now check certain words and see their movement in time along these abstracts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBvAHcO4FO6n"
      },
      "outputs": [],
      "source": [
        "targetWord = 'breast'\n",
        "\n",
        "pltDF = getDivergenceDF(targetWord, comparedEmbeddings)\n",
        "fig, ax = plt.subplots(figsize = (10, 7))\n",
        "seaborn.heatmap(pltDF, ax = ax, annot = False) #set annot True for a lot more information\n",
        "ax.set_xlabel(\"Starting year\")\n",
        "ax.set_ylabel(\"Final year\")\n",
        "ax.set_ylabel(\"Final year\")\n",
        "ax.set_title(\"Yearly linguistic change for: '{}'\".format(targetWord))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fw5fTLxvFO6n"
      },
      "outputs": [],
      "source": [
        "targetWord = 'combination'\n",
        "\n",
        "pltDF = getDivergenceDF(targetWord, comparedEmbeddings)\n",
        "fig, ax = plt.subplots(figsize = (10, 7))\n",
        "seaborn.heatmap(pltDF, ax = ax, annot = False) #set annot True for a lot more information\n",
        "ax.set_xlabel(\"Starting year\")\n",
        "ax.set_ylabel(\"Final year\")\n",
        "ax.set_ylabel(\"Final year\")\n",
        "ax.set_title(\"Yearly linguistic change for: '{}'\".format(targetWord))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2OynHQwFO6o"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def findDiverence(word, embeddingsDict):\n",
        "    \"\"\"\n",
        "    Computes the sum of (1 - cosine_similarity) between the 'base' model\n",
        "    in the first category and all aligned models (if any).\n",
        "    \"\"\"\n",
        "    cats = sorted(embeddingsDict.keys())\n",
        "    if not cats:\n",
        "        return 0.0\n",
        "\n",
        "    # 'Base' model is the first model in embeddingsDict[cats[0]]\n",
        "    base_models = embeddingsDict[cats[0]]\n",
        "    if not base_models:\n",
        "        return 0.0\n",
        "\n",
        "    base_model = base_models[0]\n",
        "\n",
        "    dists = []\n",
        "    # Compare 'base_model' vs. each aligned model in [1:]\n",
        "    for aligned_model in base_models[1:]:\n",
        "        if (word in base_model.wv) and (word in aligned_model.wv):\n",
        "            base_vec = base_model.wv[word].reshape(1, -1)\n",
        "            aligned_vec = aligned_model.wv[word].reshape(1, -1)\n",
        "            cos_sim = cosine_similarity(base_vec, aligned_vec)[0, 0]\n",
        "            dists.append(1 - cos_sim)\n",
        "        else:\n",
        "            # Optionally handle missing words differently (e.g., skip or np.nan)\n",
        "            pass\n",
        "\n",
        "    return sum(dists)\n",
        "\n",
        "def findMostDivergent(embeddingsDict):\n",
        "    \"\"\"\n",
        "    Finds the word(s) with the largest sum of (1 - cosine_similarity)\n",
        "    across all categories’ base/aligned models.\n",
        "    \"\"\"\n",
        "    # Collect all unique words from all models\n",
        "    all_words = []\n",
        "    for cat_models in embeddingsDict.values():\n",
        "        for model in cat_models:\n",
        "            # In Gensim 4+, model.wv.key_to_index is a dict of {word: index}\n",
        "            all_words.extend(list(model.wv.key_to_index.keys()))\n",
        "    all_words = set(all_words)\n",
        "\n",
        "    print(f\"Found {len(all_words)} words to compare\")\n",
        "\n",
        "    # Compute and store (word, divergence_value) for each unique word\n",
        "    word_divs = []\n",
        "    for w in all_words:\n",
        "        div_value = findDiverence(w, embeddingsDict)\n",
        "        word_divs.append((w, div_value))\n",
        "\n",
        "    # Sort by divergence descending\n",
        "    return sorted(word_divs, key=lambda x: x[1], reverse=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJT9ut0vFO6o"
      },
      "outputs": [],
      "source": [
        "wordDivergences = findMostDivergent(comparedEmbeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrBAiQ8lFO6o"
      },
      "outputs": [],
      "source": [
        "wordDivergences[:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91tuJ15JFO6o"
      },
      "outputs": [],
      "source": [
        "wordDivergences[-20:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_54K9DrkFO6p"
      },
      "outputs": [],
      "source": [
        "targetWord = wordDivergences[2][0]\n",
        "\n",
        "pltDF = getDivergenceDF(targetWord, comparedEmbeddings)\n",
        "fig, ax = plt.subplots(figsize = (10, 7))\n",
        "seaborn.heatmap(pltDF, ax = ax, annot = False) #set annot True for a lot more information\n",
        "ax.set_xlabel(\"Starting year\")\n",
        "ax.set_ylabel(\"Final year\")\n",
        "ax.set_ylabel(\"Final year\")\n",
        "ax.set_title(\"Yearly linguistic change for: '{}'\".format(targetWord))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2m4qOzzDFO6p"
      },
      "outputs": [],
      "source": [
        "targetWord = wordDivergences[-1][0]\n",
        "\n",
        "pltDF = getDivergenceDF(targetWord, comparedEmbeddings)\n",
        "fig, ax = plt.subplots(figsize = (10, 7))\n",
        "seaborn.heatmap(pltDF, ax = ax, annot = False) #set annot True for a lot more information\n",
        "ax.set_xlabel(\"Starting year\")\n",
        "ax.set_ylabel(\"Final year\")\n",
        "ax.set_ylabel(\"Final year\")\n",
        "ax.set_title(\"Yearly linguistic change for: '{}'\".format(targetWord))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jiy11EjTFO6p"
      },
      "source": [
        "We've seen here from a small, research based corpus how it is still able to capture some notion of semantic drift. The word cell has moved among the most and the word view has moved the least. We recommend trying similar analyses on different datasets to see how words change their meaning over time based on the company they keep.\n",
        "\n",
        "#### Cross language alignment\n",
        "\n",
        "While in the previous example we saw how we can use a time-stamped textual corpus to create embeddings and then compare between them, in the following example we use pre-trained embeddings for different languages and align them using some powerful packages.\n",
        "\n",
        "Here, we use Fast Text word embeddings downloaded from their [documentation website](https://fasttext.cc/), for Spanish and French embeddings. Fast Text does not take the word as a special unit but rather the character window, allowing it to achieve more subtle contextual information. You must download these embeddings and upload them to be able to complete this part of the exercise. Note that these files are large! If it doesn't work, restart your session, clear your uploads and try this section again.\n",
        "\n",
        "We then perform an SVD and orthogonal transformation on the data to rotate and reflect it so that it best aligns.\n",
        "The code is adapted from:\n",
        "https://github.com/babylonhealth/fastText_multilingual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKSVD6dnFO6p"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "# !rm -r fastText"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkN8fcy9FO6p"
      },
      "outputs": [],
      "source": [
        "!pip install fasttext\n",
        "import fasttext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYGuEeBSFO6q"
      },
      "outputs": [],
      "source": [
        "# fr_dictionary = fasttext.load_model(vector_file='/content/wiki.fr.vec')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-tGWNCIFO6q"
      },
      "outputs": [],
      "source": [
        "# es_dictionary = fasttext.load_model(vector_file='/content/wiki.es.vec')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQJigDA2FO6q"
      },
      "outputs": [],
      "source": [
        "# fr_vector = fr_dictionary[\"chat\"]\n",
        "# es_vector = es_dictionary[\"gata\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ori0by-FO6q"
      },
      "outputs": [],
      "source": [
        "# print(fasttext.cosine_similarity(fr_vector, es_vector))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwu0CrAMFO6q"
      },
      "outputs": [],
      "source": [
        "# from https://stackoverflow.com/questions/21030391/how-to-normalize-array-numpy\n",
        "def normalized(a, axis=-1, order=2):\n",
        "    \"\"\"Utility function to normalize the rows of a numpy array.\"\"\"\n",
        "    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n",
        "    l2[l2==0] = 1\n",
        "    return a / np.expand_dims(l2, axis)\n",
        "\n",
        "def make_training_matrices(source_dictionary, target_dictionary, bilingual_dictionary):\n",
        "    \"\"\"\n",
        "    Source and target dictionaries are the FastVector objects of\n",
        "    source/target languages. bilingual_dictionary is a list of\n",
        "    translation pair tuples [(source_word, target_word), ...].\n",
        "    \"\"\"\n",
        "    source_matrix = []\n",
        "    target_matrix = []\n",
        "\n",
        "    for (source, target) in bilingual_dictionary:\n",
        "        if source in source_dictionary and target in target_dictionary:\n",
        "            source_matrix.append(source_dictionary[source])\n",
        "            target_matrix.append(target_dictionary[target])\n",
        "\n",
        "    # return training matrices\n",
        "    return np.array(source_matrix), np.array(target_matrix)\n",
        "\n",
        "def learn_transformation(source_matrix, target_matrix, normalize_vectors=True):\n",
        "    \"\"\"\n",
        "    Source and target matrices are numpy arrays, shape\n",
        "    (dictionary_length, embedding_dimension). These contain paired\n",
        "    word vectors from the bilingual dictionary.\n",
        "    \"\"\"\n",
        "    # optionally normalize the training vectors\n",
        "    if normalize_vectors:\n",
        "        source_matrix = normalized(source_matrix)\n",
        "        target_matrix = normalized(target_matrix)\n",
        "\n",
        "    # perform the SVD\n",
        "    product = np.matmul(source_matrix.transpose(), target_matrix)\n",
        "    U, s, V = np.linalg.svd(product)\n",
        "\n",
        "    # return orthogonal transformation which aligns source language to the target\n",
        "    return np.matmul(U, V)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1z--PkRFO6q"
      },
      "source": [
        "To align the two word embeddings, we must try and find some common words with similar meanings. Lets try this by simply going through some common dictionaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVc_f348FO6q"
      },
      "outputs": [],
      "source": [
        "# es_words = set(es_dictionary.word2id.keys())\n",
        "# fr_words = set(fr_dictionary.word2id.keys())\n",
        "# overlap = list(es_words & fr_words)\n",
        "# bilingual_dictionary = [(entry, entry) for entry in overlap]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGpJG0IeFO6r"
      },
      "outputs": [],
      "source": [
        "# len(bilingual_dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsUxstyJFO6r"
      },
      "outputs": [],
      "source": [
        "# form the training matrices\n",
        "# source_matrix, target_matrix = make_training_matrices(fr_dictionary, es_dictionary, bilingual_dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EWw8c0AFO6r"
      },
      "outputs": [],
      "source": [
        "# learn and apply the transformation\n",
        "# transform = learn_transformation(source_matrix, target_matrix)\n",
        "# fr_dictionary.apply_transform(transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmH-71zNFO6r"
      },
      "outputs": [],
      "source": [
        "# fr_vector = fr_dictionary[\"chat\"]\n",
        "# es_vector = es_dictionary[\"gata\"]\n",
        "# print(FastVector.cosine_similarity(fr_vector, es_vector))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYuK8AUtFO6r"
      },
      "source": [
        "Here, we see that by simply picking up words spelled the same, we have done a decent job in aligning the two spaces - the Spanish word for cat and French word for cat are closer than they were in the previously unaligned space. Try using an actual Spanish - French dictionary next time and see how well the embeddings end up aligning!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PpFNCZDXLMU"
      },
      "source": [
        "#### Deep Canonical Alignment\n",
        "\n",
        "Aligning word embeddings is a popular task: here is some more material on aligning embeddings in different contexts, as well as a more efficient deep canonical alignment.\n",
        "\n",
        "[Deep Canonical Alignment](http://proceedings.mlr.press/v28/andrew13.html)\n",
        "\n",
        "https://github.com/Michaelvll/DeepCCA\n",
        "\n",
        "https://github.com/mfaruqui/crosslingual-cca\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vonWUutOsPfP"
      },
      "outputs": [],
      "source": [
        "# code for deep CCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COS_n2RFCJNk"
      },
      "source": [
        "### Dynamic Word Embeddings\n",
        "\n",
        "While some temporal analysis with word embedding models create seperate embedding spaces for different time periods and align them, it is also possible to train all these embeddings in the same space, by assuming a _drift_ between each time period, simily to a dynamic topic model. Recently, it was shown in Bamler and Mandt (2017)(‘dynamic skip-gram’ model) and Yao et al. (2018) (‘dynamic Word2Vec’ model) that it is possible to learn the word embeddings across several time periods jointly, enforcing alignment across all of them simultaneously, and positioning all the models in the same vector space in one step.  This develops the idea of model alignment even further and eliminates the need to first learn separate embeddings for each time period, and then align subsequent model pairs. Of course, it also hides idiosyncratic differences between time periods.\n",
        "\n",
        "- [Dynamic Word Embeddings for Evolving Semantic Discovery](https://arxiv.org/abs/1703.00607), Yao et al, [GitHub implementation](https://github.com/yifan0sun/DynamicWord2Vec)\n",
        "\n",
        "- [Dynamic Word Embeddings](https://arxiv.org/pdf/1702.08359.pdf), Balmer and Mandt, [GitHub Implementation](https://github.com/accessai/dynamic_word_embeddings)\n",
        "\n",
        "- [Dynamic Bernoulli Embeddings for Language Evolution](https://arxiv.org/abs/1703.08052), Rudolph and Blei, [GitHub Implementation](https://github.com/EvanZhuang/dynamic-clustering-of-dynamic-embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-Ye7o0oQz-H"
      },
      "source": [
        "The code below implements a version of this.\n",
        "\n",
        "The example input files can be downloaded at: [Dropbox link](\n",
        "https://www.dropbox.com/s/nifi5nj1oj0fu2i/data.zip?dl=0)\n",
        "\n",
        "The input is the word_pair_PMIs, which is stored in csvs (ignore the mat files); you need to put them all in a path and load everything from there (change the 'training head' variable)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9a_Ln4VDOz08"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Thu Nov 10 13:10:42 2016\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# main script for time CD\n",
        "# trainfile has lines of the form\n",
        "# tok1,tok2,pmi\n",
        "\n",
        "import numpy as np\n",
        "import util_timeCD as util\n",
        "import pickle as pickle\n",
        "\n",
        "# PARAMETERS\n",
        "\n",
        "nw = 20936 # number of words in vocab (11068100/20936 for ngram/nyt)\n",
        "T = range(1990,2016) # total number of time points (20/range(27) for ngram/nyt)\n",
        "cuda = True\n",
        "\n",
        "trainhead = 'data/wordPairPMI_' # location of training data\n",
        "savehead = 'results/'\n",
        "\n",
        "def print_params(r,lam,tau,gam,emph,ITERS):\n",
        "\n",
        "    print('rank = {}'.format(r))\n",
        "    print('frob  regularizer = {}'.format(lam))\n",
        "    print('time  regularizer = {}'.format(tau))\n",
        "    print('symmetry regularizer = {}'.format(gam))\n",
        "    print('emphasize param   = {}'.format(emph))\n",
        "    print('total iterations = {}'.format(ITERS))\n",
        "\n",
        "if __name__=='__main__':\n",
        "    import sys\n",
        "    ITERS = 5 # total passes over the data\n",
        "    lam = 10 #frob regularizer\n",
        "    gam = 100 # forcing regularizer\n",
        "    tau = 50  # smoothing regularizer\n",
        "    r   = 50  # rank\n",
        "    b = nw # batch size\n",
        "    emph = 1 # emphasize the nonzero\n",
        "\n",
        "    foo = sys.argv\n",
        "    for i in range(1,len(foo)):\n",
        "        if foo[i]=='-r':    r = int(float(foo[i+1]))\n",
        "        if foo[i]=='-iters': ITERS = int(float(foo[i+1]))\n",
        "        if foo[i]=='-lam':    lam = float(foo[i+1])\n",
        "        if foo[i]=='-tau':    tau = float(foo[i+1])\n",
        "        if foo[i]=='-gam':    gam = float(foo[i+1])\n",
        "        if foo[i]=='-b':    b = int(float(foo[i+1]))\n",
        "        if foo[i]=='-emph': emph = float(foo[i+1])\n",
        "        if foo[i]=='-check': erchk=foo[i+1]\n",
        "\n",
        "\n",
        "    savefile = savehead+'L'+str(lam)+'T'+str(tau)+'G'+str(gam)+'A'+str(emph)\n",
        "\n",
        "    print('starting training with following parameters')\n",
        "    print_params(r,lam,tau,gam,emph,ITERS)\n",
        "    print('there are a total of {} words, and {} time points'.format(nw,T))\n",
        "\n",
        "    print('X*X*X*X*X*X*X*X*X')\n",
        "    print('initializing')\n",
        "\n",
        "    #Ulist,Vlist = util.initvars(nw,T,r, trainhead)\n",
        "    Ulist,Vlist = util.import_static_init(T)\n",
        "    print(Ulist)\n",
        "    print(Vlist)\n",
        "    print('getting batch indices')\n",
        "    if b < nw:\n",
        "        b_ind = util.getbatches(nw,b)\n",
        "    else:\n",
        "        b_ind = [range(nw)]\n",
        "\n",
        "    import time\n",
        "    start_time = time.time()\n",
        "    # sequential updates\n",
        "    for iteration in range(ITERS):\n",
        "        print_params(r,lam,tau,gam,emph,ITERS)\n",
        "        try:\n",
        "            Ulist = pickle.load(open( \"%sngU_iter%d.p\" % (savefile,iteration), \"rb\" ) )\n",
        "            Vlist = pickle.load(open( \"%sngV_iter%d.p\" % (savefile, iteration), \"rb\" ) )\n",
        "            print('iteration %d loaded succesfully' % iteration)\n",
        "            continue\n",
        "        except(IOError):\n",
        "            pass\n",
        "        loss = 0\n",
        "        # shuffle times\n",
        "        if iteration == 0: times = T\n",
        "        else: times = np.random.permutation(T)\n",
        "\n",
        "        for t in range(len(times)):   # select a time\n",
        "            print('iteration %d, time %d' % (iteration, t))\n",
        "            f = trainhead + str(t) + '.csv'\n",
        "            print(f)\n",
        "\n",
        "            \"\"\"\n",
        "            try:\n",
        "                Ulist = pickle.load( open( \"%sngU_iter%d_time%d_tmp.p\" % (savefile,iteration,t), \"rb\" ) )\n",
        "                Vlist = pickle.load( open( \"%sngV_iter%d_time%d_tmp.p\" % (savefile, iteration,t), \"rb\" ) )\n",
        "                times = pickle.load( open( \"%sngtimes_iter%d_time%d_tmp.p\" % (savefile, iteration,t), \"rb\" ) )\n",
        "                print 'iteration %d time %d loaded succesfully' % (iteration, t)\n",
        "                continue\n",
        "            except(IOError):\n",
        "                pass\n",
        "            \"\"\"\n",
        "\n",
        "            pmi = util.getmat(f,nw,False)\n",
        "            for j in xrange(len(b_ind)): # select a mini batch\n",
        "                print('%d out of %d' % (j,len(b_ind)))\n",
        "                ind = b_ind[j]\n",
        "                ## UPDATE V\n",
        "                # get data\n",
        "                pmi_seg = pmi[:,ind].todense()\n",
        "\n",
        "                if t==0:\n",
        "                    vp = np.zeros((len(ind),r))\n",
        "                    up = np.zeros((len(ind),r))\n",
        "                    iflag = True\n",
        "                else:\n",
        "                    vp = Vlist[t-1][ind,:]\n",
        "                    up = Ulist[t-1][ind,:]\n",
        "                    iflag = False\n",
        "\n",
        "                if t==len(T)-1:\n",
        "                    vn = np.zeros((len(ind),r))\n",
        "                    un = np.zeros((len(ind),r))\n",
        "                    iflag = True\n",
        "                else:\n",
        "                    vn = Vlist[t+1][ind,:]\n",
        "                    un = Ulist[t+1][ind,:]\n",
        "                    iflag = False\n",
        "                Vlist[t][ind,:] = util.update(Ulist[t],emph*pmi_seg,vp,vn,lam,tau,gam,ind,iflag)\n",
        "                Ulist[t][ind,:] = util.update(Vlist[t],emph*pmi_seg,up,un,lam,tau,gam,ind,iflag)\n",
        "\n",
        "\n",
        "            #pickle.dump(Ulist, open( \"%sngU_iter%d_time%d_tmp.p\" % (savefile,iteration,t), \"wb\" ) , pickle.HIGHEST_PROTOCOL)\n",
        "            #pickle.dump(Vlist, open( \"%sngV_iter%d_time%d_tmp.p\" % (savefile, iteration,t), \"wb\" ) , pickle.HIGHEST_PROTOCOL)\n",
        "            #pickle.dump(times, open( \"%sngtimes_iter%d_time%d_tmp.p\" % (savefile, iteration,t), \"wb\" ) , pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "\n",
        "            ####  INNER BATCH LOOP END\n",
        "\n",
        "        # save\n",
        "        print('time elapsed = ', time.time()-start_time)\n",
        "\n",
        "\n",
        "        pickle.dump(Ulist, open( \"%sngU_iter%d.p\" % (savefile,iteration), \"wb\" ) , pickle.HIGHEST_PROTOCOL)\n",
        "        pickle.dump(Vlist, open( \"%sngV_iter%d.p\" % (savefile, iteration), \"wb\" ) , pickle.HIGHEST_PROTOCOL)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhhcvMnAO28m"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Thu Nov 10 10:11:23 2016\n",
        "\n",
        "@author: raon\n",
        "\"\"\"\n",
        "\n",
        "#utility functions for running the CD method\n",
        "#loss: min 1/2 \\sum_t | Yt - UtVt' |^2 + lam/2 \\sum_t(|Ut|^2 + |Vt|^2) +\n",
        "#                                        tau/2 \\sum_t>1(|Vt - Vt-1|^2 + |Ut - Ut-1|^2)\n",
        "#                                        gam/2 \\sum_t (|Ut - Vt|^2)\n",
        "\n",
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "import copy\n",
        "def update(U,Y,Vm1,Vp1,lam,tau,gam,ind,iflag):\n",
        "\n",
        "    UtU = np.dot(U.T,U) # rxr\n",
        "    r = UtU.shape[0]\n",
        "    if iflag:   M   = UtU + (lam + 2*tau + gam)*np.eye(r)\n",
        "    else:       M   = UtU + (lam + tau + gam)*np.eye(r)\n",
        "\n",
        "    Uty = np.dot(U.T,Y) # rxb\n",
        "    Ub  = U[ind,:].T   # rxb\n",
        "    A   = Uty + gam*Ub + tau*(Vm1.T+Vp1.T)  # rxb\n",
        "    Vhat = np.linalg.lstsq(M,A) #rxb\n",
        "    return Vhat[0].T #bxr\n",
        "\n",
        "\n",
        "#for the above function, the equations are to update V. So:\n",
        "#Y is n X b (b = batch size)\n",
        "#r = rank\n",
        "#U is n X r\n",
        "#Vm1 and Vp1 are bXr. so they are b rows of V, transposed\n",
        "\n",
        "def import_static_init(T):\n",
        "    emb = sio.loadmat('data/emb_static.mat')['emb']\n",
        "    U = [copy.deepcopy(emb) for t in T]\n",
        "    V = [copy.deepcopy(emb) for t in T]\n",
        "    return U,V\n",
        "def initvars(vocab_size,T,rank):\n",
        "    # dictionary will store the variables U and V. tuple (t,i) indexes time t and word index i\n",
        "\n",
        "    U,V = [],[]\n",
        "    U.append(np.random.randn(vocab_size,rank)/np.sqrt(rank))\n",
        "    V.append(np.random.randn(vocab_size,rank)/np.sqrt(rank))\n",
        "    for t in xrange(1,T):\n",
        "        U.append(U[0].copy())\n",
        "        V.append(V[0].copy())\n",
        "        print t\n",
        "    return U,V\n",
        "\n",
        "import pandas as pd\n",
        "import scipy.sparse as ss\n",
        "def getmat(f,v,rowflag):\n",
        "    data = pd.read_csv(f)\n",
        "    data = data.as_matrix()\n",
        "\n",
        "    X = ss.coo_matrix((data[:,2],(data[:,0],data[:,1])),shape=(v,v))\n",
        "\n",
        "\n",
        "    if rowflag:\n",
        "        X = ss.csr_matrix(X)\n",
        "        #X = X[inds,:]\n",
        "    else:\n",
        "        X = ss.csc_matrix(X)\n",
        "        #X = X[:,inds]\n",
        "\n",
        "    return X#.todense()\n",
        "\n",
        "def getbatches(vocab,b):\n",
        "    batchinds = []\n",
        "    current = 0\n",
        "    while current<vocab:\n",
        "        inds = range(current,min(current+b,vocab))\n",
        "        current = min(current+b,vocab)\n",
        "        batchinds.append(inds)\n",
        "    return batchinds\n",
        "\n",
        "#   THE FOLLOWING FUNCTION TAKES A WORD ID AND RETURNS CLOSEST WORDS BY COSINE DISTANCE\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "def getclosest(wid,U):\n",
        "    C = []\n",
        "    for t in range(len(U)):\n",
        "        temp = U[t]\n",
        "        K = cosine_similarity(temp[wid,:],temp)\n",
        "        mxinds = np.argsort(-K)\n",
        "        mxinds = mxinds[0:10]\n",
        "        C.append(mxinds)\n",
        "    return C\n",
        "\n",
        "# THE FOLLOWING FUNCTIONS COMPUTES THE REGULARIZER SCORES GIVEN U AND V ENTRIES\n",
        "def compute_symscore(U,V):\n",
        "    return np.linalg.norm(U-V)**2\n",
        "\n",
        "def compute_smoothscore(U,Um1,Up1):\n",
        "    X = np.linalg.norm(U-Up1)**2 + np.linalg.norm(U-Um1)**2\n",
        "    return X\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e_V1tGDpJNX"
      },
      "source": [
        "## Adding more context - FastText and GloVE\n",
        "\n",
        "Since the original word2vec paper, there have followed a slew of word embedding related methods which innovate and build on them in many ways. One popular extension is FastText (Bojanowski et al. 2017), which uses sub-words to generate its vectors. Using subwords means that it is powerful in dealing with unknown words and sparse languages that otherwise have a rich morphological structure. These sub-words are incorporated into the previously skipgram and CBOW methods. For example, if the word is “which”, it is represented as the word itself along with a bag of constituent n-grams. If n=3, the representation looks like <wh, whi, hic, ich, ch>, and we learn a representation for each of these constituents, with the word “which” taking on the average value of these constituents.\n",
        "\n",
        "FastText can be used either via Gensim or the official package, and primarily has two functions - word representations, and text classification (Joulin et al. 2017). See below for code using the FastText package for playing with word representations, taken from their word representations tutorial (https://fasttext.cc/docs/en/unsupervised-tutorial.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hXjY0FPpNVq"
      },
      "outputs": [],
      "source": [
        "! pip install fasttext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqNJ5jWgpSCB"
      },
      "source": [
        "### Representations with FastText\n",
        "\n",
        "We will follow the instructions in the FastText tutorial to prepare our data, (Wikipedia). You can either follow the instructions on the page for setting the data, or download/copy it to drive it from this [google drive link](https://drive.google.com/file/d/12T3nNzf0a7tdhm1lVyfz9Ix9XVITFMCP/view?usp=sharing).\n",
        "\n",
        "In this example, we will be training a model - it is also possible to download and use the many pre-trained models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_1B3ckRpUP1"
      },
      "outputs": [],
      "source": [
        "import fasttext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t37MtAB4JYUb"
      },
      "outputs": [],
      "source": [
        "file_address = \"/content/fil9\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7YqmoQEI360"
      },
      "outputs": [],
      "source": [
        "model = fasttext.train_unsupervised(file_address)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVdsL1O-t171"
      },
      "source": [
        "While FastText is running, the progress and estimated time to completion is shown on your screen. Once the training finishes, the model variable contains information on the trained model, which you can use for querying:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2NARfwxI6FL"
      },
      "outputs": [],
      "source": [
        "model.words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7y1Qr95t7if"
      },
      "source": [
        "It returns all words in the vocabulary, sorted by decreasing frequency. We can get the word vector by:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lxZJCAst4n1"
      },
      "outputs": [],
      "source": [
        "model.get_word_vector(\"the\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXjgMWFCt4lE"
      },
      "outputs": [],
      "source": [
        "model.save_model(\"fil9.bin\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CznMfOVbt4iG"
      },
      "outputs": [],
      "source": [
        "# when we want to use the model again\n",
        "# model = fasttext.load_model(\"result/fil9.bin\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3H_c9lVq0WQ"
      },
      "source": [
        "Let's now print some vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsAsfHyNt4e4"
      },
      "outputs": [],
      "source": [
        "[model.get_word_vector(x) for x in [\"asparagus\", \"pidgey\", \"yellow\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAXTmBV1q_Xv"
      },
      "source": [
        "A nice feature is that you can also query for words that did not appear in your data! Indeed words are represented by the sum of their substrings. As long as the unknown word is made of known substrings, there is a representation of it!\n",
        "\n",
        "As an example let's try with a misspelled word:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmWu9IZsrCl-"
      },
      "outputs": [],
      "source": [
        "model.get_word_vector(\"enviroment\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fP0QFihrsrGS"
      },
      "source": [
        "You still get a word vector for it! But how good it is? Let's find out in the next sections!\n",
        "\n",
        "#### Nearest neighbor queries\n",
        "A simple way to check the quality of a word vector is to look at its nearest neighbors. This gives an intuition of the type of semantic information the vectors are able to capture.\n",
        "\n",
        "This can be achieved with the nearest neighbor (nn) functionality. For example, we can query the 10 nearest neighbors of a word by running the following command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipxJNvy3sngq"
      },
      "outputs": [],
      "source": [
        "model.get_nearest_neighbors('asparagus')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xELqqvJvsxnC"
      },
      "source": [
        "Nice! It seems that vegetable vectors are similar. Note that the nearest neighbor is the word asparagus itself, this means that this word appeared in the dataset. What about Pokemons?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0X6mV5psw53"
      },
      "outputs": [],
      "source": [
        "model.get_nearest_neighbors('pidgey')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4_uK0W1tquU"
      },
      "source": [
        "Different evolution of the same Pokemon have close-by vectors! But what about our misspelled word; is its vector close to anything reasonable? Let's find out:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2uhn-2Uttk0"
      },
      "outputs": [],
      "source": [
        "model.get_nearest_neighbors('enviroment')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuoVYFKBtvU6"
      },
      "source": [
        "Thanks to the information contained in the word, the vector of our misspelled word matches reasonable words! It is not perfect but the main information has been captured.\n",
        "\n",
        "#### Measure of similarity\n",
        "\n",
        "In order to find nearest neighbors, we need to compute a similarity score between words. Our words are represented by continuous word vectors and we can thus apply simple similarities to them. In particular we use the cosine of the angles between two vectors. This similarity is computed for all words in the vocabulary, and the 10 most similar words are shown. Of course, if the word appears in the vocabulary, it will appear on top, with a similarity of 1.\n",
        "\n",
        "Word analogies\n",
        "In a similar spirit, one can play around with word analogies. For example, we can see if our model can guess what is to France as Berlin is to Germany.\n",
        "\n",
        "This can be done with the analogies functionality. It takes a word triplet (like Germany Berlin France) and outputs the analogy:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7f-t9qbt3eD"
      },
      "outputs": [],
      "source": [
        "model.get_analogies(\"berlin\", \"germany\", \"france\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqf2grslt7on"
      },
      "source": [
        "The answer provided by our model is Paris, which is correct. Let's have a look at a less obvious example:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oskukUFvt8Vx"
      },
      "outputs": [],
      "source": [
        "model.get_analogies(\"psx\", \"sony\", \"nintendo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WkoBS0LuAK9"
      },
      "source": [
        "Our model considers that the nintendo analogy of a psx is the gamecube, which seems reasonable. Of course the quality of the analogies depend on the dataset used to train the model; one can only hope to cover domains well-represented in the dataset.\n",
        "\n",
        "#### Importance of character n-grams\n",
        "Using subword-level information is particularly interesting to build vectors for unknown words. For example, the word gearshift does not exist on Wikipedia but we can still query its closest existing words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhHVyXJkt8S7"
      },
      "outputs": [],
      "source": [
        "model.get_nearest_neighbors('gearshift')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0S7iaX3uGMh"
      },
      "source": [
        "Most of the retrieved words share substantial substrings but a few are actually quite different, like cogwheel. You can try other words like sunbathe or grandnieces.\n",
        "\n",
        "Now that we have seen the interest of subword information for unknown words, let's check how it compares to a model that does not use subword information. To train a model without subwords, just run the following command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27llBMXxuMAf"
      },
      "outputs": [],
      "source": [
        "# this line takes a while\n",
        "model_without_subwords = fasttext.train_unsupervised('fil9', maxn=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUrOrMO8uOdi"
      },
      "source": [
        "To illustrate the difference, let us take an uncommon word in Wikipedia, like accomodation which is a misspelling of accommodation. Here is the nearest neighbors obtained without subwords:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwCUYmLGuPI4"
      },
      "outputs": [],
      "source": [
        "model_without_subwords.get_nearest_neighbors('accomodation')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHxCQ-lDuV8r"
      },
      "source": [
        "The result does not make much sense, most of these words are unrelated. On the other hand, using subword information gives the following list of nearest neighbors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYKitqVNuWoK"
      },
      "outputs": [],
      "source": [
        "model.get_nearest_neighbors('accomodation')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ECoxlc0uY0Y"
      },
      "source": [
        "The nearest neighbors captures different variations around the word accommodation. We also get semantically related words such as amenities or catering.\n",
        "\n",
        "#### Conclusion\n",
        "\n",
        "In this section, we show how to obtain word vectors from Wikipedia. This can be done for any language and we provide [pre-trained models](https://fasttext.cc/docs/en/pretrained-vectors.html) with the default setting for 294 of them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30R9lGTapUt3"
      },
      "source": [
        "### FastText Aligned Embeddings and Vectors\n",
        "\n",
        "The FastText website has an incredible resource, outside of the rich documentation and examples - pre-trained word vectors for 157 different languages! This is great because it allows for comparisons between languages, and an opportunity for us to search for structural similarities across languages. One of the ways we can do this is by aligning embeddings, similar to what we did for the diachronic temporal embeddings. We already saw code for aligning FastText embeddings using an unsupervised method (dictionary), and in the code below we will exploring embeddings that are already aligned. You can find code to perform the alignment in an [unsupervised](https://github.com/facebookresearch/fastText/blob/master/alignment/unsup_align.py) way, as well as [supervised](https://github.com/facebookresearch/fastText/blob/master/alignment/align.py). (Joulin et al, 2018)\n",
        "\n",
        "Remember to first download the two files!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsNBxEe5zYCz"
      },
      "source": [
        "#### Loading word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "X9PdNfSkzYC1"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nUGL8BlxzYC1"
      },
      "outputs": [],
      "source": [
        "def load_vec(emb_path, nmax=50000):\n",
        "    vectors = []\n",
        "    word2id = {}\n",
        "    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
        "        next(f)\n",
        "        for i, line in enumerate(f):\n",
        "            word, vect = line.rstrip().split(' ', 1)\n",
        "            vect = np.fromstring(vect, sep=' ')\n",
        "            assert word not in word2id, 'word found twice'\n",
        "            vectors.append(vect)\n",
        "            word2id[word] = len(word2id)\n",
        "            if len(word2id) == nmax:\n",
        "                break\n",
        "    id2word = {v: k for k, v in word2id.items()}\n",
        "    embeddings = np.vstack(vectors)\n",
        "    return embeddings, id2word, word2id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "pafAnr3wzYC2"
      },
      "outputs": [],
      "source": [
        "src_path = '/content/vectors-en.txt'\n",
        "tgt_path = '/content/vectors-es.txt'\n",
        "nmax = 50000  # maximum number of word embeddings to load\n",
        "\n",
        "src_embeddings, src_id2word, src_word2id = load_vec(src_path, nmax)\n",
        "tgt_embeddings, tgt_id2word, tgt_word2id = load_vec(tgt_path, nmax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oI0xy9E8zYC3"
      },
      "source": [
        "#### Get nearest neighbors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "2NKhbl4NzYC3"
      },
      "outputs": [],
      "source": [
        "def get_nn(word, src_emb, src_id2word, tgt_emb, tgt_id2word, K=5):\n",
        "    print(\"Nearest neighbors of \\\"%s\\\":\" % word)\n",
        "    word2id = {v: k for k, v in src_id2word.items()}\n",
        "    word_emb = src_emb[word2id[word]]\n",
        "    scores = (tgt_emb / np.linalg.norm(tgt_emb, 2, 1)[:, None]).dot(word_emb / np.linalg.norm(word_emb))\n",
        "    k_best = scores.argsort()[-K:][::-1]\n",
        "    for i, idx in enumerate(k_best):\n",
        "        print('%.4f - %s' % (scores[idx], tgt_id2word[idx]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A81G_GDTzYC4"
      },
      "outputs": [],
      "source": [
        "# printing nearest neighbors in the source space\n",
        "src_word = 'cat'\n",
        "get_nn(src_word, src_embeddings, src_id2word, src_embeddings, src_id2word, K=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JT6dOnVTzYC4"
      },
      "outputs": [],
      "source": [
        "# printing nearest neighbors in the target space\n",
        "src_word = 'cat'\n",
        "get_nn(src_word, src_embeddings, src_id2word, tgt_embeddings, tgt_id2word, K=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYUqaNE8zYC5"
      },
      "source": [
        "#### Visualize multilingual embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NmwjcH2zYC5"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2, whiten=True)  # TSNE(n_components=2, n_iter=3000, verbose=2)\n",
        "pca.fit(np.vstack([src_embeddings, tgt_embeddings]))\n",
        "print('Variance explained: %.2f' % pca.explained_variance_ratio_.sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "s1DvaSUEzYC6"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_similar_word(src_words, src_word2id, src_emb, tgt_words, tgt_word2id, tgt_emb, pca):\n",
        "\n",
        "    Y = []\n",
        "    word_labels = []\n",
        "    for sw in src_words:\n",
        "        Y.append(src_emb[src_word2id[sw]])\n",
        "        word_labels.append(sw)\n",
        "    for tw in tgt_words:\n",
        "        Y.append(tgt_emb[tgt_word2id[tw]])\n",
        "        word_labels.append(tw)\n",
        "\n",
        "    # find tsne coords for 2 dimensions\n",
        "    Y = pca.transform(Y)\n",
        "    x_coords = Y[:, 0]\n",
        "    y_coords = Y[:, 1]\n",
        "\n",
        "    # display scatter plot\n",
        "    plt.figure(figsize=(10, 8), dpi=80)\n",
        "    plt.scatter(x_coords, y_coords, marker='x')\n",
        "\n",
        "    for k, (label, x, y) in enumerate(zip(word_labels, x_coords, y_coords)):\n",
        "        color = 'blue' if k < len(src_words) else 'red'  # src words in blue / tgt words in red\n",
        "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points', fontsize=19,\n",
        "                     color=color, weight='bold')\n",
        "\n",
        "    plt.xlim(x_coords.min() - 0.2, x_coords.max() + 0.2)\n",
        "    plt.ylim(y_coords.min() - 0.2, y_coords.max() + 0.2)\n",
        "    plt.title('Visualization of the multilingual word embedding space')\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKAtRx5ozYC6"
      },
      "outputs": [],
      "source": [
        "# get 5 random input words\n",
        "src_words = ['university', 'love', 'history', 'tennis', 'research', 'conference']\n",
        "tgt_words = ['universidad', 'amor', 'historia', u'tenis',  u'investigación', 'conferencia']\n",
        "\n",
        "# assert words in dictionaries\n",
        "for sw in src_words:\n",
        "    assert sw in src_word2id, '\"%s\" not in source dictionary' % sw\n",
        "for tw in tgt_words:\n",
        "    assert tw in tgt_word2id, '\"%s\" not in target dictionary' % sw\n",
        "\n",
        "plot_similar_word(src_words, src_word2id, src_embeddings, tgt_words, tgt_word2id, tgt_embeddings, pca)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4p1DgypCXjO"
      },
      "source": [
        "### GloVE\n",
        "\n",
        "While word2vec and FastText focus on the local context of words, there are also methods which also attend to global corpus statistics. The most popular embedding model from this family is GloVe (Pennington et al., 2014), which gets its name from its approach to the embeddings - Global Vectors. It also includes information of ratios of probabilities from the word-word co-occurence matrix, which make it powerful for measuring semantic similarity, while still being able to capture the linear sub-structure (which we construct our dimensions with, and do our algebraic operations on). Below is code for loading GloVe vectors using gensim, and straight into a dictionary.\n",
        "\n",
        "You can download pre-trained GloVe vectors here: https://nlp.stanford.edu/projects/glove/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5t2r-z2jCagu"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiLzIrQSk3GX"
      },
      "outputs": [],
      "source": [
        "glove_file = datapath('/content/glove.6B.100d.txt')\n",
        "word2vec_glove_file = get_tmpfile(\"glove.6B.100d.word2vec.txt\")\n",
        "glove2word2vec(glove_file, word2vec_glove_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQpq8gM_k5uY"
      },
      "outputs": [],
      "source": [
        "model = KeyedVectors.load_word2vec_format(word2vec_glove_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTA0G2xgk72o"
      },
      "outputs": [],
      "source": [
        "model.most_similar('obama')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-WpO3RIk9aK"
      },
      "outputs": [],
      "source": [
        "model.most_similar('banana')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_75zfoHk-ab"
      },
      "outputs": [],
      "source": [
        "model.most_similar(negative='banana')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmlqKQbkk-A1"
      },
      "outputs": [],
      "source": [
        "result = model.most_similar(positive=['woman', 'king'], negative=['man'])\n",
        "print(\"{}: {:.4f}\".format(*result[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gpjVrpTlB0W"
      },
      "outputs": [],
      "source": [
        "def analogy(x1, x2, y1):\n",
        "    result = model.most_similar(positive=[y1, x2], negative=[x1])\n",
        "    return result[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGt5kNHglBvs"
      },
      "outputs": [],
      "source": [
        "analogy('japan', 'japanese', 'australia')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KW8BxHswlF7U"
      },
      "outputs": [],
      "source": [
        "analogy('australia', 'beer', 'france')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6BIqpq4lFzI"
      },
      "outputs": [],
      "source": [
        "analogy('obama', 'clinton', 'reagan')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7shr8-1lFtn"
      },
      "outputs": [],
      "source": [
        "analogy('tall', 'tallest', 'long')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uK7taFUtlJt-"
      },
      "outputs": [],
      "source": [
        "analogy('good', 'fantastic', 'bad')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avhxq-5BlJrJ"
      },
      "outputs": [],
      "source": [
        "print(model.doesnt_match(\"breakfast cereal dinner lunch\".split()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agpXoQy3lL9L"
      },
      "outputs": [],
      "source": [
        "def display_pca_scatterplot(model, words=None, sample=0):\n",
        "    # Select words\n",
        "    if words is None:\n",
        "        if sample > 0:\n",
        "            words = np.random.choice(list(model.key_to_index.keys()), sample)\n",
        "        else:\n",
        "            words = list(model.key_to_index.keys())\n",
        "\n",
        "    # Get word vectors\n",
        "    word_vectors = np.array([model[word] for word in words])\n",
        "\n",
        "    # Perform PCA\n",
        "    twodim = PCA().fit_transform(word_vectors)[:, :2]\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.scatter(twodim[:, 0], twodim[:, 1], edgecolors='k', c='r')\n",
        "    for word, (x, y) in zip(words, twodim):\n",
        "        plt.text(x + 0.05, y + 0.05, word)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVYUe6F-lN9r"
      },
      "outputs": [],
      "source": [
        "display_pca_scatterplot(model,\n",
        "                        ['coffee', 'tea', 'beer', 'wine', 'brandy', 'rum', 'champagne', 'water',\n",
        "                         'spaghetti', 'borscht', 'hamburger', 'pizza', 'falafel', 'sushi', 'meatballs',\n",
        "                         'dog', 'horse', 'cat', 'monkey', 'parrot', 'koala', 'lizard',\n",
        "                         'frog', 'toad', 'monkey', 'ape', 'kangaroo', 'wombat', 'wolf',\n",
        "                         'france', 'germany', 'hungary', 'luxembourg', 'australia', 'fiji', 'china',\n",
        "                         'homework', 'assignment', 'problem', 'exam', 'test', 'class',\n",
        "                         'school', 'college', 'university', 'institute'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_olJWqJlOCq"
      },
      "outputs": [],
      "source": [
        "display_pca_scatterplot(model, sample=300)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o72iuoypppbq"
      },
      "source": [
        "### svd2vec\n",
        "\n",
        "GloVe attempts to use the best of both local and global information (although we note that its performance is comparable to word2vec, which only accounts for local information). Count-based information is built on older methods such as PMI, which stands for Pointwise Mutual Information (Fano 1961) - these measures are built by trying to identify the chances of two events occuring at the same time, and can be refashioned to think about target and context words. There has been further work showing that dense embeddings such as word2vec have a strong relationship with matrix factorisation like processes that use PMI, where word2vec can be seen as implicitly optimizing a shifted version of a PMI matrix (Levy and Goldberg 2015). This work has been taken forward in Levy et al 2015, where they use PMI and SVD to generate vectors comparable to those from word2vec. Below is code for using such methods with the package svd2vec."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gf_MDQsKp8Vh"
      },
      "outputs": [],
      "source": [
        "! pip install svd2vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arnse03a_N4K"
      },
      "source": [
        "We've seen a few corpora so far - the small hobbies corpus, and the wikipedia corpus. We will use the Wikipedia corpus for training this model, as it is more suited to training a language model. While we could directly feed the raw file to train it for FastText, svd2vec requires it to be similar to the gensim format, which is a list of lists representing the corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pR7EzV2NyFN3"
      },
      "outputs": [],
      "source": [
        "from svd2vec import svd2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQXNWMstJPO_"
      },
      "outputs": [],
      "source": [
        "wiki_texts = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVoqA_9iJqO1"
      },
      "outputs": [],
      "source": [
        "for line in open(\"fil9\"):\n",
        "  # this is an example of generating a corpus - you can use your own!\n",
        "  wiki_texts.append(line)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "euohwKC3uB5P"
      },
      "outputs": [],
      "source": [
        "# code to clean files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-Qf2o8080yM"
      },
      "outputs": [],
      "source": [
        "# creating the words representation (can take a while)\n",
        "svd = svd2vec(wiki_texts, window=5, min_count=100, verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_KWCr1WACTb"
      },
      "outputs": [],
      "source": [
        "svd.similarity(\"bad\", \"good\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5dpclQ7ACL2"
      },
      "outputs": [],
      "source": [
        "svd.similarity(\"monday\", \"friday\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AuKGCgVACAD"
      },
      "outputs": [],
      "source": [
        "svd.distance(\"apollo\", \"moon\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoOPEefPAh0U"
      },
      "outputs": [],
      "source": [
        "svd.most_similar(positive=[\"january\"], topn=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nw0PTIP5Ahws"
      },
      "outputs": [],
      "source": [
        "svd.analogy(\"paris\", \"france\", \"berlin\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpbRADc0Ali2"
      },
      "outputs": [],
      "source": [
        "svd.analogy(\"road\", \"cars\", \"rail\", topn=5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0vlhqhpB4x9"
      },
      "source": [
        "This sort of matrix decomposition can be as powerful as word2vec - please follow this [notebook tutorial](https://valentinp72.github.io/svd2vec/gensim_comparison.html) for a demonstration comparing it to the gensim word2vec algorithm on the same corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrVTYvyDuBvZ"
      },
      "source": [
        "## Deep Learning and Computational Linguistics\n",
        "\n",
        "Now that we have our doc object, what can we do with it?\n",
        "We can see that the doc object now contains the entire corpus. This is important because we will be using this doc object to create our corpus for the machine learning algorithms. When creating a corpus for gensim/scikit-learn, we sometimes forget the incredible power which spaCy packs in its pipeline, so we will briefly demonstrate the same in this section with a smaller example sentence. Keep in mind that whatever we can do with a sentence, we can also do with the entire corpus.\n",
        "\n",
        "spaCy does all of this using a powerful but light weight deep learning langauge model under the hood!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqRHPz99uBvZ"
      },
      "outputs": [],
      "source": [
        "sent = nlp(u\"Tom went to IKEA to get some of those delicious Swedish meatballs.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQxXNSm5uhze"
      },
      "outputs": [],
      "source": [
        "from spacy import displacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJe9LZn1uBva"
      },
      "source": [
        "Simple enough sentence, right? When we pass any kind of text through the spaCy pipeline, it becomes annotated. We will quickly have a look at the 3 most important of capabilities which spaCy provides - POS-tagging, NER-tagging, and dependency parsing.\n",
        "\n",
        "#### POS-Tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgaykdNluBva"
      },
      "outputs": [],
      "source": [
        "for token in sent:\n",
        "    print(token.text, token.pos_, token.tag_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqlT2cQYuBva"
      },
      "source": [
        "#### NER-Tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ScHyIOauBva"
      },
      "outputs": [],
      "source": [
        "for token in sent:\n",
        "    print(token.text, token.ent_type_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdBFpFQNuBvb"
      },
      "outputs": [],
      "source": [
        "for ent in sent.ents:\n",
        "    print(ent.text, ent.label_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TZKSvaxuBvb"
      },
      "outputs": [],
      "source": [
        "displacy.render(sent, style='ent', jupyter=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFv25UVguBvb"
      },
      "source": [
        "#### Dependency Parsing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJeP1NSzuBvb"
      },
      "outputs": [],
      "source": [
        "for chunk in sent.noun_chunks:\n",
        "    print(chunk.text, chunk.root.text, chunk.root.dep_,\n",
        "          chunk.root.head.text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4H1yKfYNuBvb"
      },
      "outputs": [],
      "source": [
        "for token in sent:\n",
        "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
        "          [child for child in token.children])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mai0BMEyuBvc"
      },
      "outputs": [],
      "source": [
        "displacy.render(sent, style='dep', jupyter=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3R8KJZ8luJSB"
      },
      "source": [
        "## NOTE: Where are the deep neural nets?\n",
        "\n",
        "So far, we've seen concepts heavily related to and linked to deep learning, but not actually used deep learning for text. In the following sections, we will begin to see deep models used in more obvious ways.\n",
        "\n",
        "We have also already seen an example of deep learning for text, but in the last week - when we used a bi directional LSTM to classify text documents. A reminder - LSTMs are a recurrent neural network which are well suited to language based tasks.\n",
        "\n",
        "While we will continue our mode of using pre-trained models and popular packages to extract sentence and paragraph vectors, some of these methods include an LSTM (and more) under the hood!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvHBM-wPv8cW"
      },
      "outputs": [],
      "source": [
        "# empty cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvI9gVuSp-xG"
      },
      "source": [
        "## Sentence, paragraph, and document embeddings\n",
        "\n",
        "Word embeddings caught on so popularly not just because of their ability to semantically and/or syntactically represent individual words, but because of their utility as building blocks for more complex natural language and text related tasks. Apart from the kinds analysis we've seen, word embeddings are used in text classification and clustering tasks, information retrieval tasks, and as the internal representations for more complex deep neural models (which we will see in the coming sections and chapter).\n",
        "\n",
        "Most of the time, word embeddings are used in these tasks to create some form of sentence, paragraph or document embeddings. These representations are then used in downstream task. The process of putting these embeddings together can be thought of as a vector space composition task - how do we put together these vectors to represent a sentence or larger collection of words? Earlier approaches doing this involved attempts at building linguistically informed vector space addition and multiplication models (Mitchell and Lapata (2008) , Baroni and Zamparelli (2010), (Mitchell and Lapata 2010)), but composition now often takes a deep learning approach, and there are a variety of ways to create these representations, each with their advantages and weaknesses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JETgkWq2tiub"
      },
      "source": [
        "### Non neural representations\n",
        "\n",
        "\n",
        "In this section, we start with some very simple ways to create sentence and document representations, before moving on to neural network based methods. We saw some of these methods for creating representations earlier in the notebook, where we used simple counting methods (bag of words), weighted metrics (TF-IDF), and topic models. Some of these ways of representing longer texts may be very sparse, so we can also perform a dimensionality reduction technique on them such as PCA.\n",
        "\n",
        "Because we already saw the implementation using TF-IDF, SVD/PCA, Random Projection and HDP, the code below demonstrates using the ldamodel we created for the hobbies data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrSzl7xuv5QZ"
      },
      "outputs": [],
      "source": [
        "from gensim.parsing.preprocessing import preprocess_documents\n",
        "\n",
        "preprocessed_texts = preprocess_documents(corpus.data)\n",
        "dictionary = Dictionary(preprocessed_texts)\n",
        "corpus = [dictionary.doc2bow(text) for text in preprocessed_texts]\n",
        "\n",
        "model = LdaModel(corpus, id2word=dictionary, num_topics=5, minimum_probability=1e-8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2I11D2oetjGV"
      },
      "outputs": [],
      "source": [
        "rep = model[corpus]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwWj5BhqvxB_"
      },
      "outputs": [],
      "source": [
        "rep[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nt1xIzJhJVck"
      },
      "source": [
        "Here we see a case of documents being represented as their distribution of topics, based on the topic model we created. You might remember that our topic model wasn't the most informative - you will have to adapt between different neural methods to see what best suits your data and question. It can often be useful to baseline your neural method approach requiring distinct assumptions and assurances."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHbrZN6Mtket"
      },
      "source": [
        "### Averaging word vectors\n",
        "\n",
        "Before the advent of deep learning for text, most text-based systems would use some form of the representations we just observed. But with word2vec (an admittedly shallow neural network), words capture complex semantic content, which means that documents and paragraphs could begin to use this rich information. There are a couple of different approaches to creating document and paragraph embeddings which treat word2vec vectors like concepts. The first two we will see use word embeddings directly to create the representation for longer combinations of words. A naive (but still useful and sometimes more robust!) way to create sentence embeddings is to simply average all the word embeddings in a sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUFGB258touh"
      },
      "outputs": [],
      "source": [
        "def create_vector(text, model, model_type=None):\n",
        "    if model_type == \"word2vec\":\n",
        "        vectors = []\n",
        "        for word in text:\n",
        "            try:\n",
        "                vectors.append(model.wv[word])\n",
        "            except KeyError:\n",
        "                pass\n",
        "        if len(vectors) > 0:\n",
        "            return np.mean(vectors, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VP3nMNsEciiP"
      },
      "outputs": [],
      "source": [
        "hobbie_vector = create_vector(tokenized_texts[0], w2vmodel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01DqUE-Mc_NX"
      },
      "outputs": [],
      "source": [
        "hobbie_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9a4DlDftpOR"
      },
      "source": [
        "### More than just averaging\n",
        "\n",
        "This doesn’t always work out the best though -  as sentences get longer, we run the risk of getting a massive semantic blob as opposed to something which captures fine grained semantics. There are ways around this, and the work by Arora et al (2017) , titled ‘A simple but tough-to-beat baseline for sentence embeddings’ is a good example of using word embeddings while still creating meaningful sentence embeddings. They compute the weighted average of the word vectors in the sentence and then remove the projections of the average vectors on their first principal component (“common component removal”). The weighted average here is similar to TF-IDF, modified for better empirical performance. What is remarkable is despite only performing simple operations on word embeddings, these sentence embeddings outperform more complex architectures such as RNNs specifically built for sentence embeddings. The following code uses a package called Fast Sentence Embeddings (similar to gensim) which creates sentence representations using this technique.\n",
        "\n",
        "[Fast Sentence Embedding](https://github.com/oborchers/Fast_Sentence_Embeddings)\n",
        "\n",
        "[Sentence2vec package (alternative)](https://github.com/peter3125/sentence2vec)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhHoyHnrt3YY"
      },
      "outputs": [],
      "source": [
        "!pip install -U git+https://github.com/oborchers/Fast_Sentence_Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjo3AbwPm3MV"
      },
      "source": [
        "NOTE: this package only works with gensim version 3.8.3, which includes the Keyedvector class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZ2H23Lzmfrw"
      },
      "outputs": [],
      "source": [
        "import gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNgMFzl6mVEc"
      },
      "outputs": [],
      "source": [
        "from fse.models import Average\n",
        "from fse import IndexedList"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFRzPZa7nyNU"
      },
      "outputs": [],
      "source": [
        "model = Average(w2vmodel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXgJiRKYnueU"
      },
      "outputs": [],
      "source": [
        "model.train(IndexedList(tokenized_texts))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYjkg3SRn6YE"
      },
      "outputs": [],
      "source": [
        "corpus.target[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nldV2wbGn9B6"
      },
      "outputs": [],
      "source": [
        "corpus.target[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUTOyd_sn84c"
      },
      "outputs": [],
      "source": [
        "corpus.target[400]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fUyV9iGdiQp"
      },
      "outputs": [],
      "source": [
        "\n",
        "model.sv.similarity(0,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CvXTfVEn04U"
      },
      "outputs": [],
      "source": [
        "model.sv.similarity(0,400)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_0rBa4CoIkb"
      },
      "source": [
        "It's a small corpus with similar language so we don't see a remarkable difference, but we do see that documents with similar labels have closer similarities. We recommend exploring the package and their alternative sentence embedding methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkIp-BX1t32s"
      },
      "source": [
        "### RNN powered Inference for Sentence Vectors\n",
        "\n",
        "Another popular setting to learn such embeddings is using captioned images to learn a “grounded” representation of sentence meaning. The work by (Kiela et al. 2018) is from the same team that came up with NLI based sentence embeddings, and uses captioned images from the popular COCO dataset (which we encountered in Homework 3). In the code below, we use their code and package, InferSent to create and evaluate sentence embeddings. This embedding space is referred to as a universal sentence encoder, and these families of methods are also called sentence encoders because of their use of encoder-decoder architectures.\n",
        "\n",
        "Code for sentence embeddings and inference\n",
        " https://github.com/facebookresearch/InferSent\n",
        "\n",
        "\n",
        "There are 2 versions of InferSent. Version 1 uses GLovE while version 2 uses FastText vectors. You can choose to work with either model. Thus, we download the InferSent Model and the pre-trained Word Vectors. For this, please first save the models.py file from here and store it in your working directory.\n",
        "\n",
        "We also need to save the trained model and pre-trained GLoVe word vectors. According to the code below, our working directory should have an ‘encoders’ folder and a folder called ‘GLoVe’. The encoder folder will have our model while the GloVe folder should have the word vectors:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBO_zjn8vguX"
      },
      "outputs": [],
      "source": [
        "! mkdir encoder\n",
        "! curl -Lo encoder/infersent2.pkl https://dl.fbaipublicfiles.com/infersent/infersent2.pkl\n",
        "\n",
        "! mkdir GloVe\n",
        "! curl -Lo GloVe/glove.840B.300d.zip http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
        "! unzip GloVe/glove.840B.300d.zip -d GloVe/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJOo1NMbwk40"
      },
      "outputs": [],
      "source": [
        "def cosine(u, v):\n",
        "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfZnUjFivi1j"
      },
      "outputs": [],
      "source": [
        "from models import InferSent\n",
        "import torch\n",
        "\n",
        "V = 2\n",
        "MODEL_PATH = 'encoder/infersent%s.pkl' % V\n",
        "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
        "                'pool_type': 'max', 'dpout_model': 0.0, 'version': V}\n",
        "model = InferSent(params_model)\n",
        "model.load_state_dict(torch.load(MODEL_PATH))\n",
        "\n",
        "W2V_PATH = '/content/GloVe/glove.840B.300d.txt'\n",
        "model.set_w2v_path(W2V_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyd2Bsbn04Qn"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dffHcBFH06PD"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GduVOvuiwS0h"
      },
      "outputs": [],
      "source": [
        "model.build_vocab(corpus.data, tokenize=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUAeNPiuwbyi"
      },
      "outputs": [],
      "source": [
        "query = \"I love to play video games\"\n",
        "query_vec = model.encode(query)[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmVPhIMfwe69"
      },
      "outputs": [],
      "source": [
        "query_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4wWqv_VwioX"
      },
      "outputs": [],
      "source": [
        "similarity = []\n",
        "for sent in corpus.data[0:10]:\n",
        "  sim = cosine(query_vec, model.encode([sent])[0])\n",
        "  print(\"Sentence = \", sent, \"; similarity = \", sim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1HNZ0PW1u_q"
      },
      "source": [
        "You can play around with the queries and corpus to check out more similarity metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eB0vd2LauBoN"
      },
      "source": [
        "### Universal Sentence Encoder\n",
        "\n",
        "Google has also come up with a [universal sentence encoder](https://arxiv.org/abs/1803.11175) (Cer et al. 2018) model based on similar supervised classification settings, and has shared the model via tensorflow hub making it easy to use.\n",
        "\n",
        "We can download this universal sentence encoder using tensorflow hub. You can also check this useful [blog post](https://towardsdatascience.com/use-cases-of-googles-universal-sentence-encoder-in-production-dd5aaab4fc15#:~:text=The%20Universal%20Sentence%20Encoder%20encodes,and%20other%20natural%20language%20tasks.&text=It%20comes%20with%20two%20variations,Deep%20Averaging%20Network%20(DAN).) for production level uses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mqgk5ujw30F"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XL26LY-ouHEk"
      },
      "outputs": [],
      "source": [
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
        "model = hub.load(module_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzNZu2qo0iLr"
      },
      "outputs": [],
      "source": [
        "sentence_embeddings = model(tokenized_texts)\n",
        "query = \"I had pizza and pasta\"\n",
        "query_vec = model([query])[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3VDe_7O0phl"
      },
      "outputs": [],
      "source": [
        "for sent in sentences:\n",
        "  sim = cosine(query_vec, model([sent])[0])\n",
        "  print(\"Sentence = \", sent, \"; similarity = \", sim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLhTSxdxSGYI"
      },
      "source": [
        "### Doc2Vec\n",
        "\n",
        "So far we’ve seen unsupervised aggregation methods, and self-supervised methods that use a specific task to help learn a sentence embedding space. The last popular class of embeddings we will look at are unsupervised methods which can also be used to learn larger text embeddings such as paragraphs and documents. For larger documents, the same tasks and settings are not quite as relevant. One of the earliest (and to this day, very popular) method for creating document embeddings is often referred to as doc2vec (Le and Mikolov, 2014, Lau and Baldwin 2016). It is inspired by word2vec, also has two training processes, Distributed Memory and Distributed Bag of Words. DM attempts to use an encoder-decoder model as well as a “memory vector” for the document, and each word in the document is given a vector representation, as well as the document itself. These vectors are then used to predict a single word based on its context. The D-BOW method uses a single document to predict whether a series of sampled words are from the document in question. The popular gensim implementation includes both methods and works well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "Nf6cV8LZSGYI"
      },
      "source": [
        "Instead of just looking at just how words embed within the space, we can look at how the different documents relate to each other within the space. First lets load our data--abstracts of most U.S. physics papers from the 1950s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lI96tD7JSGYI"
      },
      "outputs": [],
      "source": [
        "apsDF = pd.read_csv('../data/APSabstracts1950s.csv', index_col = 0)\n",
        "apsDF[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxMffwWBSGYJ"
      },
      "source": [
        "We will load these as documents into Word2Vec, but first we need to normalize and pick some tags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Cz4X2oqLSGYJ"
      },
      "outputs": [],
      "source": [
        "keywords = ['photomagnetoelectric', 'quantum', 'boltzmann', 'proton', 'positron', 'feynman', 'classical', 'relativity']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "MKdqYCH7SGYJ"
      },
      "outputs": [],
      "source": [
        "apsDF['tokenized_words'] = apsDF['abstract'].apply(lambda x: lucem_illud_2020.word_tokenize(x))\n",
        "apsDF['normalized_words'] = apsDF['tokenized_words'].apply(lambda x: lucem_illud_2020.normalizeTokens(x, lemma=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9VAHqCYHSGYJ"
      },
      "outputs": [],
      "source": [
        "taggedDocs = []\n",
        "for index, row in apsDF.iterrows():\n",
        "    #Just doing a simple keyword assignment\n",
        "    docKeywords = [s for s in keywords if s in row['normalized_words']]\n",
        "    docKeywords.append(row['copyrightYear'])\n",
        "    docKeywords.append(row['doi']) #This lets us extract individual documnets since doi's are unique\n",
        "    taggedDocs.append(gensim.models.doc2vec.LabeledSentence(words = row['normalized_words'], tags = docKeywords))\n",
        "apsDF['TaggedAbstracts'] = taggedDocs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqiIXzd5SGYJ"
      },
      "source": [
        "Now we can train a Doc2Vec model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dh5t1sCnSGYJ"
      },
      "outputs": [],
      "source": [
        "apsD2V = gensim.models.doc2vec.Doc2Vec(apsDF['TaggedAbstracts'], size = 100) #Limiting to 100 dimensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RCeNOgnSGYK"
      },
      "source": [
        "We can get vectors for the tags/documents, just as we did with words. Documents are actually the centroids (high dimensional average points) of their words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyP0SQY9SGYK",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "apsD2V.docvecs[1952]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWLGQShGSGYK"
      },
      "source": [
        "The words can still be accessed in the same way:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShLhcrMySGYK"
      },
      "outputs": [],
      "source": [
        "apsD2V['atom']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtHvW16ZSGYL"
      },
      "source": [
        "We can still use the ``most_similar`` command to perform simple semantic equations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "yf-YBYv3SGYL",
        "outputId": "3ef473d2-5f55-4cb1-cd21-1b8e3d653bf6"
      },
      "outputs": [],
      "source": [
        "apsD2V.most_similar(positive = ['atom','electrons'], negative = ['electron'], topn = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9u1Iw-4SGYR"
      },
      "source": [
        "This is interesting. **Electron** is to **electrons** as **atom** is to **atoms**. Another way to understand this, developed below is: **electrons - electron** induces a singular to plural dimension, so when we subtract **electron** from **atom** and add **electrons**, we get **atoms**!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4RYMOfVSGYR"
      },
      "outputs": [],
      "source": [
        "apsD2V.most_similar(positive = ['einstein','law'], negative = ['equation'], topn = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDBvC1ZWSGYR"
      },
      "source": [
        "In other words **Einstein** minus **equation** plus **law** equals **Meissner**--Walthur Meissner studied mechanical engineering and physics ... and was more likely to produce a \"law\" than a \"equation\", like the Meissner effect, the damping of the magnetic field in superconductors. If we built our word-embedding with a bigger corpus like the entire arXiv, a massive repository of physics preprints, we would see many more such relationships like **gravity - Newton + Einstein = relativity**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpmE5i4QSGYS"
      },
      "source": [
        "We can also compute all of these *by hand*--explicitly wth vector algebra:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6X9VMpeSGYS"
      },
      "outputs": [],
      "source": [
        "sklearn.metrics.pairwise.cosine_similarity(apsD2V['electron'].reshape(1,-1), apsD2V['positron'].reshape(1,-1))\n",
        "#We reorient the vectors with .reshape(1, -1) so that they can be computed without a warning in sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlmY-XhYSGYS"
      },
      "source": [
        "In the doc2vec model, the documents have vectors just as the words do, so that we can compare documents with each other and also with words (similar to how a search engine locates a webpage with a query). First, we will calculate the distance between a word and documents in the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfxiRNQySGYS"
      },
      "outputs": [],
      "source": [
        "apsD2V.docvecs.most_similar([ apsD2V['electron'] ], topn=5 )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0nR13SKSGYT"
      },
      "source": [
        "If we search for the first of these on the web (these are doi codes), we find the following...a pretty good match:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EGpYPnzSGYT"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "Image(\"../data/PhysRev.98.875.jpg\", width=1000, height=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFOD8brISGYT"
      },
      "source": [
        "Now let's go the other way around and find words most similar to this document:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doDnA7OgSGYT"
      },
      "outputs": [],
      "source": [
        "apsD2V.most_similar( [ apsD2V.docvecs['10.1103/PhysRev.98.875'] ], topn=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3COJRK-eSGYU"
      },
      "source": [
        "We can even look for documents most like a query composed of multiple words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sww-3kVDSGYU"
      },
      "outputs": [],
      "source": [
        "apsD2V.docvecs.most_similar([ apsD2V['electron']+apsD2V['positron']+apsD2V['neutron']], topn=5 )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4wFb50nSGYV"
      },
      "source": [
        "Now let's plot some words and documents against one another with a heatmap:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "o3o40mYxSGYV"
      },
      "outputs": [],
      "source": [
        "heatmapMatrix = []\n",
        "for tagOuter in keywords:\n",
        "    column = []\n",
        "    tagVec = apsD2V.docvecs[tagOuter].reshape(1, -1)\n",
        "    for tagInner in keywords:\n",
        "        column.append(sklearn.metrics.pairwise.cosine_similarity(tagVec, apsD2V.docvecs[tagInner].reshape(1, -1))[0][0])\n",
        "    heatmapMatrix.append(column)\n",
        "heatmapMatrix = np.array(heatmapMatrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WAG7B8LSGYV"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "hmap = ax.pcolor(heatmapMatrix, cmap='terrain')\n",
        "cbar = plt.colorbar(hmap)\n",
        "\n",
        "cbar.set_label('cosine similarity', rotation=270)\n",
        "a = ax.set_xticks(np.arange(heatmapMatrix.shape[1]) + 0.5, minor=False)\n",
        "a = ax.set_yticks(np.arange(heatmapMatrix.shape[0]) + 0.5, minor=False)\n",
        "\n",
        "a = ax.set_xticklabels(keywords, minor=False, rotation=270)\n",
        "a = ax.set_yticklabels(keywords, minor=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAqhazpXSGYW"
      },
      "source": [
        "Now let's look at a heatmap of similarities between the first ten documents in the corpus:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "uDOBhZVFSGYW"
      },
      "outputs": [],
      "source": [
        "targetDocs = apsDF['doi'][:10]\n",
        "\n",
        "heatmapMatrixD = []\n",
        "\n",
        "for tagOuter in targetDocs:\n",
        "    column = []\n",
        "    tagVec = apsD2V.docvecs[tagOuter].reshape(1, -1)\n",
        "    for tagInner in targetDocs:\n",
        "        column.append(sklearn.metrics.pairwise.cosine_similarity(tagVec, apsD2V.docvecs[tagInner].reshape(1, -1))[0][0])\n",
        "    heatmapMatrixD.append(column)\n",
        "heatmapMatrixD = np.array(heatmapMatrixD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIFtPuFJSGYW"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "hmap = ax.pcolor(heatmapMatrixD, cmap='terrain')\n",
        "cbar = plt.colorbar(hmap)\n",
        "\n",
        "cbar.set_label('cosine similarity', rotation=270)\n",
        "a = ax.set_xticks(np.arange(heatmapMatrixD.shape[1]) + 0.5, minor=False)\n",
        "a = ax.set_yticks(np.arange(heatmapMatrixD.shape[0]) + 0.5, minor=False)\n",
        "\n",
        "a = ax.set_xticklabels(targetDocs, minor=False, rotation=270)\n",
        "a = ax.set_yticklabels(targetDocs, minor=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7qjpO-0SGYW"
      },
      "source": [
        "Now let's look at a heatmap of similarities between the first ten documents and our keywords:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "TTSXalYVSGYX"
      },
      "outputs": [],
      "source": [
        "heatmapMatrixC = []\n",
        "\n",
        "for tagOuter in targetDocs:\n",
        "    column = []\n",
        "    tagVec = apsD2V.docvecs[tagOuter].reshape(1, -1)\n",
        "    for tagInner in keywords:\n",
        "        column.append(sklearn.metrics.pairwise.cosine_similarity(tagVec, apsD2V.docvecs[tagInner].reshape(1, -1))[0][0])\n",
        "    heatmapMatrixC.append(column)\n",
        "heatmapMatrixC = np.array(heatmapMatrixC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltfkcET_SGYX"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "hmap = ax.pcolor(heatmapMatrixC, cmap='terrain')\n",
        "cbar = plt.colorbar(hmap)\n",
        "\n",
        "cbar.set_label('cosine similarity', rotation=270)\n",
        "a = ax.set_xticks(np.arange(heatmapMatrixC.shape[1]) + 0.5, minor=False)\n",
        "a = ax.set_yticks(np.arange(heatmapMatrixC.shape[0]) + 0.5, minor=False)\n",
        "\n",
        "a = ax.set_xticklabels(keywords, minor=False, rotation=270)\n",
        "a = ax.set_yticklabels(targetDocs, minor=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CzrV_PsSGYX"
      },
      "source": [
        "We will save the model in case we would like to use it again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YnK0IW9VSGYX"
      },
      "outputs": [],
      "source": [
        "apsD2V.save('apsW2V')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwgvG3SDSGYX"
      },
      "source": [
        "We can later load it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "DP4jzYDhSGYY"
      },
      "outputs": [],
      "source": [
        "#apsD2V = gensim.models.word2vec.Word2Vec.load('data/apsW2V')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVvH0SfauQ2m"
      },
      "source": [
        "### Skip Thought\n",
        "\n",
        "There exist many methods inspired by word2vec (and other embedding algorithms) which create embeddings for sentences and paragraphs. A notable method is skip-thought (Kiros et al, 2015) and the related FastSent which attempt to predict the entire sentence context based on the sentence in question. By doing this, sentences that share semantic and syntactic properties are mapped to similar vector representations. We do not explore this method in detail, but one can download the required model via the official GitHub repository, and use the model with just a few lines of code.\n",
        "\n",
        "https://github.com/ryankiros/skip-thoughts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQjKscQnuXCp"
      },
      "outputs": [],
      "source": [
        "# code to run model is in the GitHub link"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNrxcLbGuaGh"
      },
      "source": [
        "### Word Mover's Distance and Embeddings\n",
        "\n",
        "The last method we will explore in this section uses word embeddings to create distance measures between documents in a high dimensional space. Inspired by the Earth Mover’s (or discrete Wasserstein) Distance metric used to measure between points in space, Word Mover’s Distance (Kusner et al, 2015) measures the dissimilarity between two text documents as the minimum amount of distance that the embedded words of one document need to “travel” in the embedding space to reach the embedded words of another document. The best part of WMD is that we don’t need either of the sentences we compare to have any common words. Gensim’s implementation of Word Mover’s Distance works well and can be useful in building information retrieval systems built on word embeddings.\n",
        "\n",
        "(code for WMD - https://radimrehurek.com/gensim/auto_examples/tutorials/run_wmd.html)\n",
        "\n",
        "Building on this method, IBM built a sentence encoder inspired by WMD that goes beyond just distances in space towards encoding sentences using word embeddings and a distance measure. This embedding method is called Word Movers Embeddings, and there exists a C++ based python wrapper of the code, which we link to but do not demonstrate in this section.  Word mover embedding -  https://github.com/IBM/WordMoversEmbeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnIrfbYXudHe"
      },
      "outputs": [],
      "source": [
        "# code to run models are in the GitHub link"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RceGetroulIh"
      },
      "source": [
        "## Classification and topic extraction with word embeddings\n",
        "\n",
        "In this section we will explore the use of word and document embeddings for classification and topic extraction, two popular natural language processing tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTU2xaabuoR0"
      },
      "source": [
        "### Classification using Doc2vec\n",
        "\n",
        "The last section in this tutorial will demonstrate some of the other powerful uses of word embeddings, such as document classification and topic extraction. A standard way to perform classification or clustering tasks for text documents is to use a machine learning (or deep learning) based method on the vectorised text documents, such as Naive Bayes or a Convolutional Neural Network. In the last section we saw a number of ways we can vectorise documents, and most of these would be pretty useful for classifying documents.\n",
        "\n",
        "In this section, we encourage you to visit these Gensim doc2vec tutorials which feature classification as one of the primary tasks.\n",
        "\n",
        "For more doc2vec examples:\n",
        "\n",
        "\n",
        "[Gensim doc2vec code lee corpus](\n",
        "https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html#sphx-glr-auto-examples-tutorials-run-doc2vec-lee-py)\n",
        "\n",
        "[Gensim doc2vec code imdb corpus](\n",
        "https://radimrehurek.com/gensim/auto_examples/howtos/run_doc2vec_imdb.html#sphx-glr-auto-examples-howtos-run-doc2vec-imdb-py\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "A dataset that may be well suited for classification with doc2vec:\n",
        "\n",
        "[Yelp reviews](https://www.kaggle.com/c/yelp-recruiting/data?select=yelp_training_set.zip)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgOgs5uUu63T"
      },
      "outputs": [],
      "source": [
        "# doc2vec classification code - build vectors using model, and set up a classification pipeline.\n",
        "# multiple links to code implementations provided."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqItv98iu7h0"
      },
      "source": [
        "### Inversion of Distributed Representations\n",
        "\n",
        "But this isn’t the only way we can use word and document embeddings. Word embeddings are essentially distributed language models, and such a model can be inverted using bayesian techniques. What this means is that given a document, we can calculate the probability it was generated by a distributed model - by creating multiple word embedding models for each class, we are able to easily whip up a distribution of probabilities for a document belonging to each of these classes. This method of classification by inversion of distributed representations was first introduced by (Taddy 2015), and performed better than a doc2vec based classification on the dataset of Yelp reviews we just used.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09moGhQPSGYl"
      },
      "source": [
        "#### The Score Function\n",
        "\n",
        "The score function is a simple calculation developed by [Matt Taddy](https://arxiv.org/pdf/1504.07295.pdf) to calculate the likelihood that a given text would have been generated by a word-embedding model by summing the inner product between each pair of the text's word vectors. This relies on a few data files that are not in the git repo due to their size please download and unzip [this](https://github.com/Computational-Content-Analysis-2018/Upcoming/raw/master/data/supplement.zip) (472MB) file in the data directory.\n",
        "\n",
        "Here, we explore this using a model trained with millions of resumes from the CareerBuilder website (we can't share the private resumes...but we can share a model built with them :-):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-icQiO70SGYm"
      },
      "outputs": [],
      "source": [
        "resume_model  = gensim.models.word2vec.Word2Vec.load('../data/resumeAll.model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qg9xeNv3SGYm"
      },
      "source": [
        "We can examine the vacabularies of this model by building a word-index map:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xGwKVj_ZSGYm"
      },
      "outputs": [],
      "source": [
        "vocab = resume_model.wv.index2word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHRyacxmSGYm"
      },
      "source": [
        "Let's just load the sample and take a look at it. The sentences in each job description are already tokenized and normalized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2T3WZf_SGYn"
      },
      "outputs": [],
      "source": [
        "sampleDF = pd.read_csv('../data/SampleJobAds.csv', index_col = False)\n",
        "#We need to convert the last couple columns from strings to lists\n",
        "sampleDF['tokenized_sents'] = sampleDF['tokenized_sents'].apply(lambda x: eval(x))\n",
        "sampleDF['normalized_sents'] = sampleDF['normalized_sents'].apply(lambda x: eval(x))\n",
        "sampleDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ggd2DziJSGYn"
      },
      "source": [
        "Let's define a function to calculate the likelihood of each job description. The idea is borrowed from [Matt Taddy](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/deepir.ipynb), who shows how a document can be characterized as the inner product of the distance between its words. In other words, this analysis will show which job ads are most likely to find an appropriate pool of workers in the resume bank that generated our word embedding.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9pz9_7gKSGYn"
      },
      "outputs": [],
      "source": [
        "def adprob(ad, model):\n",
        "    sen_scores = model.score(ad, len(ad))\n",
        "    ad_score = sen_scores.mean()\n",
        "    return ad_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bpv16DqBSGYn"
      },
      "source": [
        "Let's apply this function to every job description."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "uV-ckcBISGYo"
      },
      "outputs": [],
      "source": [
        "sampleDF['likelihood'] = sampleDF['normalized_sents'].apply(lambda x: adprob(x, resume_model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLcm0jMhSGYo"
      },
      "source": [
        "Let's take a look at the top 5 job descriptions that have the highest likelihood."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7zq8GmJSGYo"
      },
      "outputs": [],
      "source": [
        "for ad in sampleDF.sort_values(by = 'likelihood', ascending = False)['jobDescription'][:5]:\n",
        "    print (ad + '\\n\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxXzv0e1SGYo"
      },
      "source": [
        "Let's take a look at the bottom 5 job descriptions that have the lowest likelihood to be matched by the resumes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBqp_8_RSGYo"
      },
      "outputs": [],
      "source": [
        "for ad in sampleDF.sort_values(by = 'likelihood')['jobDescription'][:5]:\n",
        "    print (ad + '\\n\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXfjrkDwSGYp"
      },
      "source": [
        "We can do the same for phrases corresponding to job skills."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUgym9NySGYp"
      },
      "outputs": [],
      "source": [
        "adprob([[\"python\", \"programming\"]], resume_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQnOSDlhSGYp"
      },
      "outputs": [],
      "source": [
        "adprob([[\"julia\", \"programming\"]], resume_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfLzZX7RSGYp"
      },
      "source": [
        "Basic programming appears to be more likely in this pool of resumes than python programming.\n",
        "\n",
        "We can also do some simple statistics. Unfortunately, we don't have a large sample here. Nevertheless, let's first look at the mean likelihood score of each hiring organization. Some organizations will do well to hire on CareerBuilder...while others will not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PY_o8NJ6SGYq"
      },
      "outputs": [],
      "source": [
        "sampleDF.groupby(\"hiringOrganization_organizationName\")[['likelihood']].mean().sort_values('likelihood', ascending = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4HVVcMXSGYq"
      },
      "source": [
        "We can also look at the mean likelihood of each state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6um9dOQSGYq"
      },
      "outputs": [],
      "source": [
        "sampleDF.groupby(\"jobLocation_address_region\")[['likelihood']].mean().sort_values('likelihood', ascending = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYHYuaIrvBYk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c92VC8DYvPY1"
      },
      "source": [
        "### FastText Classification\n",
        "\n",
        "Earlier in the homework we explored the word embedding method FastText, which is built using character n-grams or subwords. FastText can be very powerful for text classification, especially when we are dealing with many classes, partly due to its using Hierarchical Softmax for determining the probabilities a document belongs to a class, after averaging FastText embeddings of the words in a sentence or document. This is because a hierarchical algorithm follows a tree-like approach to determining the probabilities of the class, allowing for much faster computational times (hence, the fast part of the text). This allows for efficient text classification (performs as well as pure deep learning approaches) while being much faster. The code below, taken from the official documentation for FastText classification, demonstrates how to use the pre-trained vectors for the task.\n",
        "\n",
        "FastText classification: https://fasttext.cc/docs/en/supervised-tutorial.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEKt2jPwvTiO"
      },
      "outputs": [],
      "source": [
        "# code for FastText classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeCZlXXfvUPs"
      },
      "source": [
        "### Discourse Atoms\n",
        "\n",
        "Another method also focuses on treating word embedding models as a generative process are Discourse Atoms, which are very similar to topic models. First described by Princeton NLP researchers ((Arora et al. 2016), (Arora et al. 2018)), the approach uses an idea of gist, or discourse as the process which generates text. These discourse atoms, similar to topic models, are defined as probabilities over words, and make it possible to measure similarities between the discourse vector and individual word vectors. While we won’t delve into the mathematical details of the generative process (a random walk over semantic space), what we do need to know is the steps to generate these discourse atoms for us. This is described well in one of the first social scientific papers to use these atoms for descriptive research (Arseniev-Koehler et al. 2020), and we adapt code used by them to create the atoms. The first major step is training word embeddings on the corpus in question.\n",
        "\n",
        "#### imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBHMqKnk_JkC"
      },
      "outputs": [],
      "source": [
        "# from gensim.test.utils import datapath\n",
        "# import re\n",
        "# import string, re\n",
        "# import cython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rhM3yfuALJQ"
      },
      "outputs": [],
      "source": [
        "!pip install ksvd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5y1spof_MCs"
      },
      "outputs": [],
      "source": [
        "from gensim import corpora, models, similarities #calc all similarities at once, from http://radimrehurek.com/gensim/tut3.html\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from gensim.models import KeyedVectors\n",
        "from random import seed, sample\n",
        "from ksvd import ApproximateKSVD #pip or conda install ksvd #this is key!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zietHqDPF5uT"
      },
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFYA73V4GH2Y"
      },
      "outputs": [],
      "source": [
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgH-NiNgvZMz"
      },
      "outputs": [],
      "source": [
        "nytimes_w2v_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjAyx16xvccJ"
      },
      "source": [
        "#### k-SVD\n",
        "\n",
        "We then perform a K-SVD on the word embedding matrix to learn topics in such a way where each word-vector is represented as a spare linear combination of topics. To generate a good representation of the original word vector space, we want to minimize the difference between our word vectors and the vectors generated as a linear combination of topics.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11xyYrXOvgJw"
      },
      "outputs": [],
      "source": [
        "#### TRAIN MODEL:\n",
        "\n",
        "#n_comp: Number of topics (i.e., atoms, or dictionary elements)\n",
        "#n_nonzeros: Number of nonzero coefficients to target (how many atoms each word can load onto)\n",
        "\n",
        "def do_aksvd(w2vmodel, n_comp, n_nonzeros, save=False, savelocation='/content/aksvd_models/'):\n",
        "    #https://github.com/nel215/ksvd #takes about 2 min on Alina's laptop for 30 atoms\n",
        "    aksvd_t = ApproximateKSVD(n_components=n_comp, transform_n_nonzero_coefs=n_nonzeros) #also may adjuste n iter which is default at 10, and tolerance for error which is default at  tol=1e-6 #n_components is number of discourse atoms, since vocab size is smallish, keep this fewer. transform_n is the number of atoms (components) that a word can be a linear combo of\n",
        "    dictionary_t = aksvd_t.fit(w2vmodel.wv.vectors).components_ # Dictionary is the matrix of discourse atoms.\n",
        "    alpha_t = aksvd_t.transform(w2vmodel.wv.vectors) #get the alphas, which are the \"weights\" of each word on a discourse atoms\n",
        "\n",
        "    if save==True:\n",
        "        outfile = open(str(savelocation) + '200d_' + str(n_comp) + 'comp' + str(n_nonzeros) + 'nonzeros_aksvd_nvdrsdf20','wb')\n",
        "        pickle.dump(aksvd_t,outfile)\n",
        "        outfile.close()\n",
        "\n",
        "        outfile = open(str(savelocation) + '200d_' +str(n_comp) + 'comp' + str(n_nonzeros) + 'nonzeros_dictionary_nvdrsdf20','wb')\n",
        "        pickle.dump(dictionary_t,outfile)\n",
        "        outfile.close()\n",
        "\n",
        "        outfile = open(str(savelocation) + '200d_' + str(n_comp) + 'comp' + str(n_nonzeros) + 'nonzeros_alpha_nvdrsdf20','wb')\n",
        "        pickle.dump(alpha_t,outfile)\n",
        "        outfile.close()\n",
        "    return(dictionary_t, alpha_t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEX-yfOQEDKH"
      },
      "source": [
        "Two quick quality checks. These are useful to choose the number of atoms in the dictionary (i.e., number of topics): $R^2$ and Topic Diversity\n",
        "\n",
        "Useful to look at product of the two since $R^2$ tends to increase with higher # topics, as Topic Diversity decreases. Intuition: more topics can better explain the original semantic space, but also then these topics are less distinct from one another. As a result, we typically want a balance between the two."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObzUO-JZEEJS"
      },
      "outputs": [],
      "source": [
        "def reconst_qual(w2vmodel, dictionary_mat, alpha_mat):\n",
        "    #reconstruct the word vectors\n",
        "    reconstructed = alpha_mat.dot(dictionary_mat) #reconstruct word vectors and add back in mean(?). but note that reconstructed norm is still around 0-1, not 1, is that an issue?\n",
        "    #e1 = norm(w2vmodel.wv.vectors - reconstructed) #total reconstruction error, larger means MORE error. norm as specified here takes frobenius norm of error matrix.\n",
        "\n",
        "\n",
        "    #total VARIANCE in the data: sum of squares\n",
        "    squares3= w2vmodel.wv.vectors-np.mean(w2vmodel.wv.vectors, axis=1).reshape(-1,1) #https://dziganto.github.io/data%20science/linear%20regression/machine%20learning/python/Linear-Regression-101-Metrics/\n",
        "    #sst3= np.sum([i.dot(i) for i in squares3] ) #same as below\n",
        "\n",
        "    sst3= np.sum(np.square(squares3))\n",
        "\n",
        "\n",
        "    #total sum of squared ERRORS/residuals\n",
        "    e3= [reconstructed[i]-w2vmodel.wv.vectors[i] for i in range(0,len(w2vmodel.wv.vectors))]  #https://dziganto.github.io/data%20science/linear%20regression/machine%20learning/python/Linear-Regression-101-Metrics/\n",
        "    #sse3= np.sum([i.dot(i) for i in e3] ) #same as below\n",
        "    sse3= np.sum(np.square(e3))\n",
        "\n",
        "    #R^2: 1- (SSE / SST )\n",
        "    r2= 1- (sse3 /  sst3) #https://stats.stackexchange.com/questions/184603/in-pca-what-is-the-connection-between-explained-variance-and-squared-error\n",
        "\n",
        "\n",
        "    #compute root mean square error\n",
        "    rmse=  math.sqrt(np.mean(np.square(e3)))\n",
        "\n",
        "\n",
        "\n",
        "    return(sse3, rmse, r2) #https://stats.stackexchange.com/questions/184603/in-pca-what-is-the-connection-between-explained-variance-and-squared-error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipIs45YhvhBM"
      },
      "source": [
        "#### Inferring topics from document\n",
        "\n",
        "We now use a similar approach to what we saw a little earlier, where we inverted our generative model to see which documents belong to which class - we do the same now, but with discourse atoms instead of the whole model. This process tells us the topic most likely to have generated a specific context (document).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGNQXH96vmiD"
      },
      "outputs": [],
      "source": [
        "#topic diversity (% unique words among total closest 25 words to each atom)\n",
        "def topic_diversity(w2vmodel, dictionary_mat, top_n=25):\n",
        "\n",
        "    topwords=[] #list of list, each innter list includes top N words in that topic\n",
        "\n",
        "    for i in range(0, len(dictionary_mat)): #set to number of total topics\n",
        "        topwords.extend([i[0] for i in w2vmodel.wv.similar_by_vector(dictionary_mat[i],topn=top_n)]) #set for top N words\n",
        "        #print(w2vmodel.wv.similar_by_vector(dictionary[i],topn=N))\n",
        "\n",
        "    uniquewords= set(topwords)\n",
        "    diversity = len(uniquewords)/len(topwords)\n",
        "    return(diversity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMuRYYmVFMCF"
      },
      "outputs": [],
      "source": [
        "dictionary, alpha = do_aksvd(nytimes_w2v_model, 150, 5, save=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0njiXVfFzpH"
      },
      "outputs": [],
      "source": [
        "topic_diversity(nytimes_w2v_model, dictionary, top_n=25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQGv2X_NFL60"
      },
      "outputs": [],
      "source": [
        "reconst_qual(nytimes_w2v_model, dictionary, alpha)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiCYSBTwFLvK"
      },
      "outputs": [],
      "source": [
        "#loading back in the model pieces if not already in\n",
        "\n",
        "# infile = open('../data/aksvd_models/200d_150comp5nonzeros_dictionary_nvdrsdf20','rb')\n",
        "# dictionary=pickle.load(infile)\n",
        "# infile.close()\n",
        "\n",
        "# infile = open('../data/aksvd_models/200d_150comp5nonzeros_aksvd_nvdrsdf20','rb')\n",
        "# aksvd=pickle.load(infile)\n",
        "# infile.close()\n",
        "\n",
        "# infile = open('../data/aksvd_models/200d_150comp5nonzeros_alpha_nvdrsdf20','rb')\n",
        "# alpha=pickle.load(infile)\n",
        "# infile.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VgluAGRGCHi"
      },
      "outputs": [],
      "source": [
        "for i in range(0, len(dictionary)):\n",
        "    print(\"Discourse_Atom \" + str(i))\n",
        "    print([i[0] for i in nytimes_w2v_model.wv.similar_by_vector(dictionary[i],topn=25)]) #what are the most similar words to the Nth\n",
        "    #print([i[0] for i in w2vmodel.wv.similar_by_vector(-dictionary[i],topn=25)]) #what are the most similar words to the Nth dicourse atom?\n",
        "    print('\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ysp5BMTxGEnm"
      },
      "outputs": [],
      "source": [
        "# for a specific atom, e.g., 112th atom look at 25 most similar words:\n",
        "nytimes_w2v_model.wv.similar_by_vector(dictionary[112],topn=25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_C7I9ZFGVi9"
      },
      "outputs": [],
      "source": [
        "print(nytimes_w2v_model.wv.vocab.get('the').index, '\\n', alpha[w2vmodel.wv.vocab.get('the').index])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4ZtXsVrGo-1"
      },
      "outputs": [],
      "source": [
        "#useful relevant code:\n",
        "#w2vmodel.wv.index2word[3452]\n",
        "#w2vmodel.wv.most_similar('the', topn=15)\n",
        "#np.where(alpha[w2vmodel.wv.vocab.get('the').index] != 0) #get index where the loading of a word onto discourse atoms is not 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUTjlEqrsXaH"
      },
      "source": [
        "## Homework Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeDNTLNY1D-P"
      },
      "source": [
        "**1)** Train a word2vec model on your dataset. Explore some of the relationships between words which are learned on your dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "just jotting down ideas ...\n",
        "- Surpise only exists when expectation existss \n",
        "- If cultural meaning emerges from positions and relations within a shared space, how does expectations fit into this? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "po7gV0zI1TAs"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/data/macss_abstracts_full.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/data/macss_abstracts_full.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# inspect columns\u001b[39;00m\n\u001b[1;32m      7\u001b[0m df\u001b[38;5;241m.\u001b[39mcolumns\n",
            "File \u001b[0;32m/opt/anaconda3/envs/agents/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
            "File \u001b[0;32m/opt/anaconda3/envs/agents/lib/python3.13/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "File \u001b[0;32m/opt/anaconda3/envs/agents/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
            "File \u001b[0;32m/opt/anaconda3/envs/agents/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
            "File \u001b[0;32m/opt/anaconda3/envs/agents/lib/python3.13/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data/macss_abstracts_full.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "df = pd.read_csv(\"/data/macss_abstracts_full.csv\")\n",
        "\n",
        "# inspect columns\n",
        "df.columns\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKkVk9IC1Ti4"
      },
      "source": [
        "**2)** Perform an advanced word embedding analysis, such as visualising embeddings, creating dynamic embeddings, projecting embeddings on distinct dimensions, debiasing, retrofitting, or aligning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29RrOs801hN1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ir8dDoiR2M45"
      },
      "source": [
        "**3)** How do you think such methods can be useful in your social scientific research?\n",
        "Hint: think of what different parts of speech or entities might entail!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_Nv6EUe2Vib"
      },
      "outputs": [],
      "source": [
        "comp_ling_possibilities = 'something' #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDTkQEaHNDe9"
      },
      "source": [
        "# Module 2: Encoders, Decoders, Seq2Seq and Attention\n",
        "\n",
        "Encoder-Decoder models are a powerful way to deal with sequence to sequence problems - this kind of setup is also called seq2seq.\n",
        "\n",
        "When we use this kind of sequence to sequence alignment and also map certain parts of the sequences with different values (a process called Attention), the sequence to sequence prediction task performance increases. In this section and the next we will focus on Machine Translation as the domain to try Encoder and Decoder tasks, without and then with attention.\n",
        "\n",
        "**We highly recommend using this [visual blog](http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/) which explains seq2seq and attention based models**\n",
        "\n",
        "We will be using the code from a Keras tutorial and PyTorch tutorial to demonstrate these models.\n",
        "\n",
        "NOTE: these sections can be skimmed through - you can go through these in detail if you decide to do the HW question on sequence to sequence pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9UQ-9budmSS"
      },
      "outputs": [],
      "source": [
        "# empty cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIuje1-Ve-67"
      },
      "source": [
        "## Character-level recurrent sequence-to-sequence model\n",
        "\n",
        "This code is adapted from the [Keras official tutorial](https://keras.io/examples/nlp/lstm_seq2seq/) on seq2seq using LSTM, authored by [fchollet](https://twitter.com/fchollet)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dW7DdSj0e-7J"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "This example demonstrates how to implement a basic character-level\n",
        "recurrent sequence-to-sequence model. We apply it to translating\n",
        "short English sentences into short French sentences,\n",
        "character-by-character. Note that it is fairly unusual to\n",
        "do character-level machine translation, as word-level\n",
        "models are more common in this domain.\n",
        "\n",
        "**Summary of the algorithm**\n",
        "\n",
        "- We start with input sequences from a domain (e.g. English sentences)\n",
        "    and corresponding target sequences from another domain\n",
        "    (e.g. French sentences).\n",
        "- An encoder LSTM turns input sequences to 2 state vectors\n",
        "    (we keep the last LSTM state and discard the outputs).\n",
        "- A decoder LSTM is trained to turn the target sequences into\n",
        "    the same sequence but offset by one timestep in the future,\n",
        "    a training process called \"teacher forcing\" in this context.\n",
        "    It uses as initial state the state vectors from the encoder.\n",
        "    Effectively, the decoder learns to generate `targets[t+1...]`\n",
        "    given `targets[...t]`, conditioned on the input sequence.\n",
        "- In inference mode, when we want to decode unknown input sequences, we:\n",
        "    - Encode the input sequence into state vectors\n",
        "    - Start with a target sequence of size 1\n",
        "        (just the start-of-sequence character)\n",
        "    - Feed the state vectors and 1-char target sequence\n",
        "        to the decoder to produce predictions for the next character\n",
        "    - Sample the next character using these predictions\n",
        "        (we simply use argmax).\n",
        "    - Append the sampled character to the target sequence\n",
        "    - Repeat until we generate the end-of-sequence character or we\n",
        "        hit the character limit.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uk8__4jEe-7L"
      },
      "source": [
        "### Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjnD7Gtle-7M"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9axnF-ie-7O"
      },
      "source": [
        "### Download the data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ik7TrAiMe-7P"
      },
      "outputs": [],
      "source": [
        "!!curl -O http://www.manythings.org/anki/fra-eng.zip\n",
        "!!unzip fra-eng.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzpQ-1sre-7T"
      },
      "source": [
        "### Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14m6qbZVe-7T"
      },
      "outputs": [],
      "source": [
        "batch_size = 64  # Batch size for training.\n",
        "epochs = 100  # Number of epochs to train for.\n",
        "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
        "num_samples = 10000  # Number of samples to train on.\n",
        "# Path to the data txt file on disk.\n",
        "data_path = \"fra.txt\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJ1NzQs6e-7U"
      },
      "source": [
        "### Prepare the data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VXgK_P4e-7V"
      },
      "outputs": [],
      "source": [
        "# Vectorize the data.\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().split(\"\\n\")\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    input_text, target_text, _ = line.split(\"\\t\")\n",
        "    # We use \"tab\" as the \"start sequence\" character\n",
        "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    target_text = \"\\t\" + target_text + \"\\n\"\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in input_characters:\n",
        "            input_characters.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)\n",
        "\n",
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "print(\"Number of samples:\", len(input_texts))\n",
        "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
        "\n",
        "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "encoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
        "    encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
        "    for t, char in enumerate(target_text):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
        "    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmT-s5TGe-7X"
      },
      "source": [
        "### Build the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1emHhx2e-7Y"
      },
      "outputs": [],
      "source": [
        "# Define an input sequence and process it.\n",
        "encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n",
        "encoder = keras.layers.LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n",
        "\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the\n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KVvUL8re-7b"
      },
      "source": [
        "### Train the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIXh70Mte-7c"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    [encoder_input_data, decoder_input_data],\n",
        "    decoder_target_data,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_split=0.2,\n",
        ")\n",
        "\n",
        "# Save model\n",
        "model.save(\"s2s.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HgC-DTCe-7d"
      },
      "source": [
        "### Run inference (sampling)\n",
        "\n",
        "1. encode input and retrieve initial decoder state\n",
        "2. run one step of decoder with this initial state\n",
        "and a \"start of sequence\" token as target.\n",
        "Output will be the next target token.\n",
        "3. Repeat with the current target token and current states\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JSbU7pse-7f"
      },
      "outputs": [],
      "source": [
        "# Define sampling models\n",
        "# Restore the model and construct the encoder and decoder.\n",
        "model = keras.models.load_model(\"s2s.h5\")\n",
        "\n",
        "encoder_inputs = model.input[0]  # input_1\n",
        "encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\n",
        "encoder_states = [state_h_enc, state_c_enc]\n",
        "encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_inputs = model.input[1]  # input_2\n",
        "decoder_state_input_h = keras.Input(shape=(latent_dim,), name=\"input_3\")\n",
        "decoder_state_input_c = keras.Input(shape=(latent_dim,), name=\"input_4\")\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_lstm = model.layers[3]\n",
        "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs\n",
        ")\n",
        "decoder_states = [state_h_dec, state_c_dec]\n",
        "decoder_dense = model.layers[4]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = keras.Model(\n",
        "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
        ")\n",
        "\n",
        "# Reverse-lookup token index to decode sequences back to\n",
        "# something readable.\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.0\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "    return decoded_sentence\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8K7BU-fe-7i"
      },
      "source": [
        "You can now generate decoded sentences as such:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHNiUUmPe-7j"
      },
      "outputs": [],
      "source": [
        "for seq_index in range(20):\n",
        "    # Take one sequence (part of the training set)\n",
        "    # for trying out decoding.\n",
        "    input_seq = encoder_input_data[seq_index : seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print(\"-\")\n",
        "    print(\"Input sentence:\", input_texts[seq_index])\n",
        "    print(\"Decoded sentence:\", decoded_sentence)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTFsrjOgoTjB"
      },
      "source": [
        "\n",
        "##  Translation with a Sequence to Sequence Network and Attention\n",
        "\n",
        "This code is adapted from the [PyTorch official tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html), authored by [Sean Robertson](https://github.com/spro/practical-pytorch).\n",
        "\n",
        "We recommend you to <font color=\"red\">refresh</font> and <font color=\"red\">restart</font> your running session before doing exercises in this module.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03fSZ2pmv02A"
      },
      "source": [
        "### Introduction\n",
        "In this project we will be teaching a neural network to translate from\n",
        "French to English.\n",
        "\n",
        "\n",
        "    [KEY: > input, = target, < output]\n",
        "\n",
        "    > il est en train de peindre un tableau .\n",
        "    = he is painting a picture .\n",
        "    < he is painting a picture .\n",
        "\n",
        "    > pourquoi ne pas essayer ce vin delicieux ?\n",
        "    = why not try that delicious wine ?\n",
        "    < why not try that delicious wine ?\n",
        "\n",
        "    > elle n est pas poete mais romanciere .\n",
        "    = she is not a poet but a novelist .\n",
        "    < she not not a poet but a novelist .\n",
        "\n",
        "    > vous etes trop maigre .\n",
        "    = you re too skinny .\n",
        "    < you re all alone .\n",
        "\n",
        "... to varying degrees of success.\n",
        "\n",
        "This is made possible by the simple but powerful idea of the [sequence\n",
        "to sequence network](https://arxiv.org/abs/1409.3215), in which two\n",
        "recurrent neural networks work together to transform one sequence to\n",
        "another. An encoder network condenses an input sequence into a vector,\n",
        "and a decoder network unfolds that vector into a new sequence.\n",
        "\n",
        "\n",
        "To improve upon this model we'll use an [attention\n",
        "mechanism](https://arxiv.org/abs/1409.0473), which lets the decoder\n",
        "learn to focus over a specific range of the input sequence.\n",
        "\n",
        "### imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIHXqUQvoTjD"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xk9k0xgoTjE"
      },
      "source": [
        "### Loading data files\n",
        "\n",
        "\n",
        "The data for this project is a set of many thousands of English to\n",
        "French translation pairs.\n",
        "\n",
        "`This question on Open Data Stack\n",
        "Exchange <https://opendata.stackexchange.com/questions/3888/dataset-of-sentences-translated-into-many-languages>`__\n",
        "pointed me to the open translation site https://tatoeba.org/ which has\n",
        "downloads available at https://tatoeba.org/eng/downloads - and better\n",
        "yet, someone did the extra work of splitting language pairs into\n",
        "individual text files here: https://www.manythings.org/anki/\n",
        "\n",
        "The English to French pairs are too big to include in the repo, so\n",
        "download to ``data/eng-fra.txt`` before continuing. The file is a tab\n",
        "separated list of translation pairs:\n",
        "\n",
        "::\n",
        "\n",
        "    I am cold.    J'ai froid.\n",
        "\n",
        ".. Note::\n",
        "   Download the data from\n",
        "   `here <https://download.pytorch.org/tutorial/data.zip>`_\n",
        "   and extract it to the current directory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nSBX2TTdkcj"
      },
      "outputs": [],
      "source": [
        "!unzip data.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8VYnzaHoTjE"
      },
      "source": [
        "Similar to the character encoding used in the character-level RNN\n",
        "tutorials, we will be representing each word in a language as a one-hot\n",
        "vector, or giant vector of zeros except for a single one (at the index\n",
        "of the word). Compared to the dozens of characters that might exist in a\n",
        "language, there are many many more words, so the encoding vector is much\n",
        "larger. We will however cheat a bit and trim the data to only use a few\n",
        "thousand words per language.\n",
        "\n",
        ".. figure:: /_static/img/seq-seq-images/word-encoding.png\n",
        "   :alt:\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUB-q_f8oTjG"
      },
      "source": [
        "We'll need a unique index per word to use as the inputs and targets of\n",
        "the networks later. To keep track of all this we will use a helper class\n",
        "called ``Lang`` which has word → index (``word2index``) and index → word\n",
        "(``index2word``) dictionaries, as well as a count of each word\n",
        "``word2count`` to use to later replace rare words.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yulUXQbioTjG"
      },
      "outputs": [],
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBwz8kK8oTjH"
      },
      "source": [
        "The files are all in Unicode, to simplify we will turn Unicode\n",
        "characters to ASCII, make everything lowercase, and trim most\n",
        "punctuation.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RiyZRpvFoTjH"
      },
      "outputs": [],
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
        "    return s.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2iz97nYoTjI"
      },
      "source": [
        "To read the data file we will split the file into lines, and then split\n",
        "lines into pairs. The files are all English → Other Language, so if we\n",
        "want to translate from Other Language → English I added the ``reverse``\n",
        "flag to reverse the pairs.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8gLtSLyoTjI"
      },
      "outputs": [],
      "source": [
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90NyCjp7oTjJ"
      },
      "source": [
        "Since there are a *lot* of example sentences and we want to train\n",
        "something quickly, we'll trim the data set to only relatively short and\n",
        "simple sentences. Here the maximum length is 10 words (that includes\n",
        "ending punctuation) and we're filtering to sentences that translate to\n",
        "the form \"I am\" or \"He is\" etc. (accounting for apostrophes replaced\n",
        "earlier).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hC5odnfoTjJ"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 10\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "        p[1].startswith(eng_prefixes)\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFJFSQrvoTjK"
      },
      "source": [
        "The full process for preparing the data is:\n",
        "\n",
        "-  Read text file and split into lines, split lines into pairs\n",
        "-  Normalize text, filter by length and content\n",
        "-  Make word lists from sentences in pairs\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zbqOWRVoTjK"
      },
      "outputs": [],
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
        "print(random.choice(pairs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KElEyvZOoTjL"
      },
      "source": [
        "### The Seq2Seq Model\n",
        "\n",
        "A Recurrent Neural Network, or RNN, is a network that operates on a\n",
        "sequence and uses its own output as input for subsequent steps.\n",
        "\n",
        "A `Sequence to Sequence network <https://arxiv.org/abs/1409.3215>`__, or\n",
        "seq2seq network, or `Encoder Decoder\n",
        "network <https://arxiv.org/pdf/1406.1078v3.pdf>`__, is a model\n",
        "consisting of two RNNs called the encoder and decoder. The encoder reads\n",
        "an input sequence and outputs a single vector, and the decoder reads\n",
        "that vector to produce an output sequence.\n",
        "\n",
        ".. figure:: /_static/img/seq-seq-images/seq2seq.png\n",
        "   :alt:\n",
        "\n",
        "Unlike sequence prediction with a single RNN, where every input\n",
        "corresponds to an output, the seq2seq model frees us from sequence\n",
        "length and order, which makes it ideal for translation between two\n",
        "languages.\n",
        "\n",
        "Consider the sentence \"Je ne suis pas le chat noir\" → \"I am not the\n",
        "black cat\". Most of the words in the input sentence have a direct\n",
        "translation in the output sentence, but are in slightly different\n",
        "orders, e.g. \"chat noir\" and \"black cat\". Because of the \"ne/pas\"\n",
        "construction there is also one more word in the input sentence. It would\n",
        "be difficult to produce a correct translation directly from the sequence\n",
        "of input words.\n",
        "\n",
        "With a seq2seq model the encoder creates a single vector which, in the\n",
        "ideal case, encodes the \"meaning\" of the input sequence into a single\n",
        "vector — a single point in some N dimensional space of sentences.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRgxRkttoTjM"
      },
      "source": [
        "### The Encoder\n",
        "\n",
        "The encoder of a seq2seq network is a RNN that outputs some value for\n",
        "every word from the input sentence. For every input word the encoder\n",
        "outputs a vector and a hidden state, and uses the hidden state for the\n",
        "next input word.\n",
        "\n",
        ".. figure:: /_static/img/seq-seq-images/encoder-network.png\n",
        "   :alt:\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcpRP9DKoTjM"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, input):\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, hidden = self.gru(embedded)\n",
        "        return output, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koRIOArfoTjM"
      },
      "source": [
        "### The Decoder\n",
        "\n",
        "The decoder is another RNN that takes the encoder output vector(s) and\n",
        "outputs a sequence of words to create the translation.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUT9JS-poTjN"
      },
      "source": [
        "\n",
        "\n",
        "In the simplest seq2seq decoder we use only last output of the encoder.\n",
        "This last output is sometimes called the *context vector* as it encodes\n",
        "context from the entire sequence. This context vector is used as the\n",
        "initial hidden state of the decoder.\n",
        "\n",
        "At every step of decoding, the decoder is given an input token and\n",
        "hidden state. The initial input token is the start-of-string ``<SOS>``\n",
        "token, and the first hidden state is the context vector (the encoder's\n",
        "last hidden state).\n",
        "\n",
        ".. figure:: /_static/img/seq-seq-images/decoder-network.png\n",
        "   :alt:\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJoM8Mn4oTjN"
      },
      "outputs": [],
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_outputs = []\n",
        "\n",
        "        for i in range(MAX_LENGTH):\n",
        "            decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden)\n",
        "            decoder_outputs.append(decoder_output)\n",
        "\n",
        "            if target_tensor is not None:\n",
        "                # Teacher forcing: Feed the target as the next input\n",
        "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
        "            else:\n",
        "                # Without teacher forcing: use its own predictions as the next input\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        return decoder_outputs, decoder_hidden, None # We return `None` for consistency in the training loop\n",
        "\n",
        "    def forward_step(self, input, hidden):\n",
        "        output = self.embedding(input)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.out(output)\n",
        "        return output, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLkleT6hoTjO"
      },
      "source": [
        "I encourage you to train and observe the results of this model, but to\n",
        "save space we'll be going straight for the gold and introducing the\n",
        "Attention Mechanism.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCR7DceXoTjO"
      },
      "source": [
        "Attention Decoder\n",
        "^^^^^^^^^^^^^^^^^\n",
        "\n",
        "If only the context vector is passed betweeen the encoder and decoder,\n",
        "that single vector carries the burden of encoding the entire sentence.\n",
        "\n",
        "Attention allows the decoder network to \"focus\" on a different part of\n",
        "the encoder's outputs for every step of the decoder's own outputs. First\n",
        "we calculate a set of *attention weights*. These will be multiplied by\n",
        "the encoder output vectors to create a weighted combination. The result\n",
        "(called ``attn_applied`` in the code) should contain information about\n",
        "that specific part of the input sequence, and thus help the decoder\n",
        "choose the right output words.\n",
        "\n",
        ".. figure:: https://i.imgur.com/1152PYf.png\n",
        "   :alt:\n",
        "\n",
        "Calculating the attention weights is done with another feed-forward\n",
        "layer ``attn``, using the decoder's input and hidden state as inputs.\n",
        "Because there are sentences of all sizes in the training data, to\n",
        "actually create and train this layer we have to choose a maximum\n",
        "sentence length (input length, for encoder outputs) that it can apply\n",
        "to. Sentences of the maximum length will use all the attention weights,\n",
        "while shorter sentences will only use the first few.\n",
        "\n",
        ".. figure:: /_static/img/seq-seq-images/attention-decoder-network.png\n",
        "   :alt:\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-tZt8K8oTjO"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Va = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, query, keys):\n",
        "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
        "        scores = scores.squeeze(2).unsqueeze(1)\n",
        "\n",
        "        weights = F.softmax(scores, dim=-1)\n",
        "        context = torch.bmm(weights, keys)\n",
        "\n",
        "        return context, weights\n",
        "\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.attention = BahdanauAttention(hidden_size)\n",
        "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_outputs = []\n",
        "        attentions = []\n",
        "\n",
        "        for i in range(MAX_LENGTH):\n",
        "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            decoder_outputs.append(decoder_output)\n",
        "            attentions.append(attn_weights)\n",
        "\n",
        "            if target_tensor is not None:\n",
        "                # Teacher forcing: Feed the target as the next input\n",
        "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
        "            else:\n",
        "                # Without teacher forcing: use its own predictions as the next input\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        attentions = torch.cat(attentions, dim=1)\n",
        "\n",
        "        return decoder_outputs, decoder_hidden, attentions\n",
        "\n",
        "\n",
        "    def forward_step(self, input, hidden, encoder_outputs):\n",
        "        embedded =  self.dropout(self.embedding(input))\n",
        "\n",
        "        query = hidden.permute(1, 0, 2)\n",
        "        context, attn_weights = self.attention(query, encoder_outputs)\n",
        "        input_gru = torch.cat((embedded, context), dim=2)\n",
        "\n",
        "        output, hidden = self.gru(input_gru, hidden)\n",
        "        output = self.out(output)\n",
        "\n",
        "        return output, hidden, attn_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrQR6uRVoTjP"
      },
      "source": [
        "\n",
        "\n",
        "### Training\n",
        "\n",
        "Preparing Training Data\n",
        "\n",
        "To train, for each pair we will need an input tensor (indexes of the\n",
        "words in the input sentence) and target tensor (indexes of the words in\n",
        "the target sentence). While creating these vectors we will append the\n",
        "EOS token to both sequences.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPDyE3u6oTjP"
      },
      "outputs": [],
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)\n",
        "\n",
        "def get_dataloader(batch_size):\n",
        "    input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
        "\n",
        "    n = len(pairs)\n",
        "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
        "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
        "\n",
        "    for idx, (inp, tgt) in enumerate(pairs):\n",
        "        inp_ids = indexesFromSentence(input_lang, inp)\n",
        "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
        "        inp_ids.append(EOS_token)\n",
        "        tgt_ids.append(EOS_token)\n",
        "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
        "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
        "\n",
        "    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
        "                               torch.LongTensor(target_ids).to(device))\n",
        "\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "    return input_lang, output_lang, train_dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qq6IR_LgoTjP"
      },
      "source": [
        "### Training the Model\n",
        "\n",
        "To train we run the input sentence through the encoder, and keep track\n",
        "of every output and the latest hidden state. Then the decoder is given\n",
        "the ``<SOS>`` token as its first input, and the last hidden state of the\n",
        "encoder as its first hidden state.\n",
        "\n",
        "\"Teacher forcing\" is the concept of using the real target outputs as\n",
        "each next input, instead of using the decoder's guess as the next input.\n",
        "Using teacher forcing causes it to converge faster but `when the trained\n",
        "network is exploited, it may exhibit\n",
        "instability <http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.378.4095&rep=rep1&type=pdf>`__.\n",
        "\n",
        "You can observe outputs of teacher-forced networks that read with\n",
        "coherent grammar but wander far from the correct translation -\n",
        "intuitively it has learned to represent the output grammar and can \"pick\n",
        "up\" the meaning once the teacher tells it the first few words, but it\n",
        "has not properly learned how to create the sentence from the translation\n",
        "in the first place.\n",
        "\n",
        "Because of the freedom PyTorch's autograd gives us, we can randomly\n",
        "choose to use teacher forcing or not with a simple if statement. Turn\n",
        "``teacher_forcing_ratio`` up to use more of it.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8X76FoDFoTjP"
      },
      "outputs": [],
      "source": [
        "def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n",
        "          decoder_optimizer, criterion):\n",
        "\n",
        "    total_loss = 0\n",
        "    for data in dataloader:\n",
        "        input_tensor, target_tensor = data\n",
        "\n",
        "        encoder_optimizer.zero_grad()\n",
        "        decoder_optimizer.zero_grad()\n",
        "\n",
        "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
        "\n",
        "        loss = criterion(\n",
        "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
        "            target_tensor.view(-1)\n",
        "        )\n",
        "        loss.backward()\n",
        "\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhBofIiGoTjQ"
      },
      "source": [
        "This is a helper function to print time elapsed and estimated time\n",
        "remaining given the current time and progress %.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cbdMCvNoTjQ"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mklP0f8VoTjQ"
      },
      "source": [
        "The whole training process looks like this:\n",
        "\n",
        "-  Start a timer\n",
        "-  Initialize optimizers and criterion\n",
        "-  Create set of training pairs\n",
        "-  Start empty losses array for plotting\n",
        "\n",
        "Then we call ``train`` many times and occasionally print the progress (%\n",
        "of examples, time so far, estimated time) and average loss.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFA8uyDBoTjQ"
      },
      "outputs": [],
      "source": [
        "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n",
        "               print_every=100, plot_every=100):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if epoch % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n",
        "                                        epoch, epoch / n_epochs * 100, print_loss_avg))\n",
        "\n",
        "        if epoch % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slMtEO_FoTjR"
      },
      "source": [
        "### Plotting results\n",
        "\n",
        "Plotting is done with matplotlib, using the array of loss values\n",
        "``plot_losses`` saved while training.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-hQtgY9oTjR"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8wE-4jioTjR"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "Evaluation is mostly the same as training, but there are no targets so\n",
        "we simply feed the decoder's predictions back to itself for each step.\n",
        "Every time it predicts a word we add it to the output string, and if it\n",
        "predicts the EOS token we stop there. We also store the decoder's\n",
        "attention outputs for display later.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_khje6GoTjR"
      },
      "outputs": [],
      "source": [
        "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "\n",
        "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
        "\n",
        "        _, topi = decoder_outputs.topk(1)\n",
        "        decoded_ids = topi.squeeze()\n",
        "\n",
        "        decoded_words = []\n",
        "        for idx in decoded_ids:\n",
        "            if idx.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            decoded_words.append(output_lang.index2word[idx.item()])\n",
        "    return decoded_words, decoder_attn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV5vpawIoTjR"
      },
      "source": [
        "We can evaluate random sentences from the training set and print out the\n",
        "input, target, and output to make some subjective quality judgements:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLDbHlxmoTjS"
      },
      "outputs": [],
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-u_pJpTKoTjS"
      },
      "source": [
        "### Training and Evaluating\n",
        "\n",
        "With all these helper functions in place (it looks like extra work, but\n",
        "it makes it easier to run multiple experiments) we can actually\n",
        "initialize a network and start training.\n",
        "\n",
        "Remember that the input sentences were heavily filtered. For this small\n",
        "dataset we can use relatively small networks of 256 hidden nodes and a\n",
        "single GRU layer. After about 40 minutes on a MacBook CPU we'll get some\n",
        "reasonable results.\n",
        "\n",
        ".. Note::\n",
        "   If you run this notebook you can train, interrupt the kernel,\n",
        "   evaluate, and continue training later. Comment out the lines where the\n",
        "   encoder and decoder are initialized and run ``trainIters`` again.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9HTlKHcoTjS"
      },
      "outputs": [],
      "source": [
        "hidden_size = 128\n",
        "batch_size = 32\n",
        "\n",
        "input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n",
        "\n",
        "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "train(train_dataloader, encoder, decoder, 80, print_every=5, plot_every=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MWe6J7B9H7c"
      },
      "source": [
        "Set dropout layers to `eval` mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUMOAFIHoTjS",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "encoder.eval()\n",
        "decoder.eval()\n",
        "evaluateRandomly(encoder, decoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TLHkNqzoTjS",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Visualizing Attention\n",
        "\n",
        "\n",
        "A useful property of the attention mechanism is its highly interpretable\n",
        "outputs. Because it is used to weight specific encoder outputs of the\n",
        "input sequence, we can imagine looking where the network is focused most\n",
        "at each time step.\n",
        "\n",
        "You could simply run ``plt.matshow(attentions)`` to see attention output\n",
        "displayed as a matrix, with the columns being input steps and rows being\n",
        "output steps:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGhWjlvXoTjT"
      },
      "outputs": [],
      "source": [
        "def showAttention(input_sentence, output_words, attentions):\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(attentions.cpu().numpy(), cmap='bone')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
        "                       ['<EOS>'], rotation=90)\n",
        "    ax.set_yticklabels([''] + output_words)\n",
        "\n",
        "    # Show label at every tick\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def evaluateAndShowAttention(input_sentence):\n",
        "    output_words, attentions = evaluate(encoder, decoder, input_sentence, input_lang, output_lang)\n",
        "    print('input =', input_sentence)\n",
        "    print('output =', ' '.join(output_words))\n",
        "    showAttention(input_sentence, output_words, attentions[0, :len(output_words), :])\n",
        "\n",
        "\n",
        "evaluateAndShowAttention('il n est pas aussi grand que son pere')\n",
        "\n",
        "evaluateAndShowAttention('je suis trop fatigue pour conduire')\n",
        "\n",
        "evaluateAndShowAttention('je suis desole si c est une question idiote')\n",
        "\n",
        "evaluateAndShowAttention('je suis reellement fiere de vous')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nG6GR90oDwH2"
      },
      "source": [
        "## Homework Exercise\n",
        "\n",
        "**1)** Use a large pre-trained language model for sequence classification, question answering, language modelling, or any other NLP task. Fine-tune the model on your dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xe2P9tgMD0q0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0w-5vDO3EJN8"
      },
      "source": [
        "**2)** What model did you use? How did your model perform?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6J6Ea8VELvn"
      },
      "outputs": [],
      "source": [
        "model_performance = 'something' #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99L6C39OE1HV"
      },
      "source": [
        "**3)** Build a SHAP-based explainer of your model for 3 inputs from your data. Describe the feature importance relationships you see for each of your inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WW_2z32E1HW"
      },
      "outputs": [],
      "source": [
        "explanations = 'something' #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFv9Y00rE1HX"
      },
      "source": [
        "**4)** Build a SHAP-based explainer of your model over a substantial subsample of your data. Describe the aggregate feature importance values you obtain for each of your labels.\n",
        "\n",
        "NOTE: This task may not be appropriate for models other than for sequence classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWs65ujvE1HX"
      },
      "outputs": [],
      "source": [
        "explanations = 'something' #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOXhlS5ANBDx"
      },
      "source": [
        "# Module 3: Transformers\n",
        "\n",
        "Transformers are a neural architecture comprised of encoder component and a decoder component, which use self-attention to reach high levels of performance for sequence to sequence tasks (and drops all RNN components!)\n",
        "\n",
        "**We highly recommend reading this [visual blog](http://jalammar.github.io/illustrated-transformer/) on the Transformer before proceeding.**\n",
        "\n",
        "\n",
        "## Models\n",
        "\n",
        "We usually see two families of models - the Generative Pre-Trained Transformer models (GPT-n) developed by Open-AI, and the BERT family of models developed by Google, which featured Bi-directional Transformers as language models.\n",
        "\n",
        "### GPT and similar models\n",
        "\n",
        "These are auto-regressive language models created by [OpenAI](https://openai.com/). GPT3 has 175 billion parameters, and we don't even know how large GPT-4 is. GPT-N is currently the largest, most powerful generative language model, which drives ChatGPT.\n",
        "\n",
        "**We highly recommend reading this [visual blog](http://jalammar.github.io/illustrated-gpt2/) on the GPT-2 before proceeding.**\n",
        "\n",
        "**Also, here is the [GPT-3 paper](https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html)**\n",
        "\n",
        "\n",
        "### BERT and similar models\n",
        "\n",
        "The BERT model, first presented by Google Research, uses bi-directional transformers and two tasks - Masked Language Modelling task, and Next Sentence Prediction task, to train the model. These large transformers based models are trained on lots of data (Wikipedia + books). BERT has since spun off many models, include language specific ones (SpanBERT), domain specific models (SciBERT), and different level of sequences (CharacterBERT).\n",
        "\n",
        "**We highly recommend reading this [visual blog](http://jalammar.github.io/illustrated-bert/) on BERT and similar models before proceeding**.\n",
        "\n",
        "We also highly recommend these papers for a more critical view on such language models:\n",
        "\n",
        "[On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜](https://dl.acm.org/doi/10.1145/3442188.3445922) - Bender et al, 2021\n",
        "\n",
        "[Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data](https://www.aclweb.org/anthology/2020.acl-main.463/) - Bender and Koller, 2020\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cJ-SkJpAdhp"
      },
      "source": [
        "## Popular Tasks using Transformers & Model Explanations\n",
        "\n",
        "We will now use the 'pipelines' feature from the popular Transformers package to explore common NLP tasks. For several of these tasks, we perform model interpretability exercises that provide a window into model's decision-making process by computing the SHAP values of different parts of our input. Before we jump into our code, let's take a brief overview of the concepts behind our model explanations.\n",
        "\n",
        "SHAP ([SHapley Additive exPlanations](https://arxiv.org/abs/1705.07874)) - which we introduced in Week 1 - is a game theoretic approach to explain model outputs based on [Shapley values](https://christophm.github.io/interpretable-ml-book/shapley.html). The intuition behind Shapley values is simple: It treats model inputs as a 'coalition' of features ('players') that collectively contribute to the output ('reward'). It then estimates the importance of any particular feature as the average marginal contribution of the feature across all combinatorial possibilities of 'coalitions' of features.\n",
        "\n",
        "[SHAP](https://christophm.github.io/interpretable-ml-book/shap.html#shap) is a computational approach to estimating Shapley values by building a separate 'explanation' model to explain the output of any particular input. It is a model agnostic measure (it is not specific to any particular model architecture), making it a very handy analytical tool to better understand and compare the deep-learning-based models we build.\n",
        "\n",
        "[Transformers Documentation](https://huggingface.co/transformers/)\n",
        "\n",
        "[Transformers GitHub](https://github.com/huggingface/transformers)\n",
        "\n",
        "[A great resource for interpretable ML by Christoph Molnar](https://christophm.github.io/interpretable-ml-book/)\n",
        "\n",
        "The following sections of code are based on the [Summary of Tasks](https://huggingface.co/transformers/task_summary.html) page in the Transformers documentation and [SHAP documentation.](https://shap.readthedocs.io/en/latest/index.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRcgtG9MqHi2"
      },
      "source": [
        "### Sequence Classification\n",
        "\n",
        "Here, we look at an example of emotion identification: Classifying text into joy, sadness, love, anger, fear, or surprise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuLXZ8VsqSCP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import transformers\n",
        "from datasets import load_dataset\n",
        "import shap\n",
        "\n",
        "# load the emotion dataset\n",
        "dataset  = load_dataset(\"emotion\", split = \"train\")\n",
        "data = pd.DataFrame({'text':dataset['text'],'emotion':dataset['label']})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxZ3SO3Uqp6g"
      },
      "outputs": [],
      "source": [
        "# load the model and tokenizer\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\"nateraw/bert-base-uncased-emotion\", use_fast=True)\n",
        "model = transformers.AutoModelForSequenceClassification.from_pretrained(\"nateraw/bert-base-uncased-emotion\").cuda()\n",
        "\n",
        "# build a pipeline object to do predictions\n",
        "pred = transformers.pipeline(\"text-classification\", model=model, tokenizer=tokenizer, device=0, return_all_scores=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTPjxQqiCAZ4"
      },
      "outputs": [],
      "source": [
        "data['text'][:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nupRdlAVE00w"
      },
      "source": [
        "Now let's run a single example through our pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8xaobgJE9uI"
      },
      "outputs": [],
      "source": [
        "eg = \"Felt so intimidated by all that code!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9M68nMazEnLR"
      },
      "outputs": [],
      "source": [
        "pred(eg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKjXK2jyFY4Z"
      },
      "source": [
        "We get softmax values over our labels.\n",
        "That output makes sense!\n",
        "\n",
        "Let's now *explain* the reasoning of ou model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNfQzzSOqp9P"
      },
      "outputs": [],
      "source": [
        "#Creaing an explainer that wraps around our pipeline\n",
        "explainer = shap.Explainer(pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1t6gtFU0CWOV"
      },
      "source": [
        "In the code block below, we visualize the explainer we built on our three text inputs.\n",
        "\n",
        "For each example at the top, we see can our five emotion labels, with fear highlighted in red for our first example to indicate that it is the prediction for the input. If you click on 'fear' the graphic changes to indicate which words in our input has the most positive SHAP contribution (red) and negative contribution (blue) to our final decision. To be clear, both red and blue features matter to the model: red positively influences label probability; blue negatively influences label probability. The width of the bar (right coordinate - left coordinate) for each token indicates the magnitude of our estimate. You might see that many featurees have very litte corresponding width - this indicates that these features have negligible influence on label probabilites (for the input in question).\n",
        "\n",
        "In the case below, we see that 'intimidated' is a strong red - which makes a lot of sense!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCvGyktvF5nv"
      },
      "outputs": [],
      "source": [
        "#Estimating SHAP values for our example\n",
        "shap_values = explainer([eg])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H81pzPAIGH56"
      },
      "outputs": [],
      "source": [
        "shap.plots.text(shap_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpwQe-jUGuuX"
      },
      "source": [
        "A cool feature of the explainer is that you can click on the other labels as well - not just the final prediction - to understand what the model 'thinks' of each label and input!\n",
        "\n",
        "Play around with some additional examples below - you should see that our model is building a lot of dependencies behind the scenes (thanks to attention):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mv_oi29jqqAJ"
      },
      "outputs": [],
      "source": [
        "#Estimating SHAP values for first three text inputs\n",
        "shap_values = explainer(data['text'][:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zor92vW4JCW2"
      },
      "outputs": [],
      "source": [
        "shap.plots.text(shap_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtL8jsd4IzXS"
      },
      "source": [
        "You can also limit your visualization to focus on explanations for a particular label:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIduYlugqw23"
      },
      "outputs": [],
      "source": [
        "shap.plots.text(shap_values[:, :, \"anger\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_F7Cyh_JJFX"
      },
      "source": [
        "We can also use other plots to help us compare feature contributions more readily. Below, we calculate feature importance values across *all* of our three prior inputs. This is a powerful tool to directly investigate the associations our model builds between labels and data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4QVORp4qw50"
      },
      "outputs": [],
      "source": [
        "shap.plots.bar(shap_values[:,:,\"joy\"].mean(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6qEkAQPK0yp"
      },
      "source": [
        "rinting features with most negative relationship with label:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1S8KDTOqw8l"
      },
      "outputs": [],
      "source": [
        "shap.plots.bar(shap_values[:,:,\"joy\"].mean(0), order=shap.Explanation.argsort)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "girmIHWXK-2g"
      },
      "source": [
        "And the most positive relationships with label:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8kjiBAoqw_N"
      },
      "outputs": [],
      "source": [
        "# ...or acending order\n",
        "shap.plots.bar(shap_values[:,:,\"joy\"].mean(0), order=shap.Explanation.argsort.flip)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2oChnp1-ZMm"
      },
      "source": [
        "### Extractive Question Answering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12uRPuEq-ZMm"
      },
      "source": [
        "Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\n",
        "question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune a\n",
        "model on a SQuAD task, you may leverage the [run_squad.py](https://github.com/huggingface/transformers/tree/master/examples/question-answering/run_squad.py) and\n",
        "[run_tf_squad.py](https://github.com/huggingface/transformers/tree/master/examples/question-answering/run_tf_squad.py) scripts.\n",
        "\n",
        "\n",
        "Here is an example of using pipelines to do question answering: extracting an answer from a text given a question. It\n",
        "leverages a fine-tuned model on SQuAD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ast1vLPT-ZMm"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "nlp = pipeline(\"question-answering\")\n",
        "context = r\"\"\"\n",
        "Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\n",
        "question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n",
        "a model on a SQuAD task, you may leverage the examples/question-answering/run_squad.py script.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5fq8iB0-ZMn"
      },
      "source": [
        "This returns an answer extracted from the text, a confidence score, alongside \"start\" and \"end\" values, which are the\n",
        "positions of the extracted answer in the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMSCEtkU-ZMn"
      },
      "outputs": [],
      "source": [
        "result = nlp(question=\"What is extractive question answering?\", context=context)\n",
        "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")\n",
        "result = nlp(question=\"What is a good example of a question answering dataset?\", context=context)\n",
        "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xGv8frNMLZW"
      },
      "source": [
        "Let's now take a look at model explanations for extractive question answering:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvL0tl8zmcz7"
      },
      "outputs": [],
      "source": [
        "import shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeARlTObmsrJ"
      },
      "outputs": [],
      "source": [
        "def f(questions, start, device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    Generate logits (start or end) for a list of questions and contexts.\n",
        "\n",
        "    Args:\n",
        "    - questions: List of strings with format \"question[SEP]context\".\n",
        "    - start: Boolean, True for start logits, False for end logits.\n",
        "    - device: The device to run the model on ('cuda' or 'cpu').\n",
        "\n",
        "    Returns:\n",
        "    - List of logits for each input question.\n",
        "    \"\"\"\n",
        "    outs = []\n",
        "    for q in questions:\n",
        "        if \"[SEP]\" not in q:\n",
        "            raise ValueError(\"Each question must contain '[SEP]' separating question and context.\")\n",
        "\n",
        "        # Split question and context\n",
        "        question, context = q.split(\"[SEP]\")\n",
        "\n",
        "        # Tokenize and move tensors to the specified device\n",
        "        d = nlp.tokenizer(question, context, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "        d = {k: v.to(device) for k, v in d.items()}\n",
        "\n",
        "        # Forward pass through the model\n",
        "        out = nlp.model(**d)\n",
        "        logits = out.start_logits if start else out.end_logits\n",
        "\n",
        "        # Append logits as a numpy array\n",
        "        outs.append(logits.squeeze(0).detach().cpu().numpy())  # Squeeze batch dimension and move to CPU\n",
        "    return outs\n",
        "\n",
        "def f_start(questions, device=\"cuda\"):\n",
        "    \"\"\"Wrapper for start logits.\"\"\"\n",
        "    return f(questions, True, device=device)\n",
        "\n",
        "def f_end(questions, device=\"cuda\"):\n",
        "    \"\"\"Wrapper for end logits.\"\"\"\n",
        "    return f(questions, False, device=device)\n",
        "\n",
        "# Attach a dynamic output_names property to the models for token visualization\n",
        "def out_names(inputs):\n",
        "    \"\"\"\n",
        "    Decode input tokens to words.\n",
        "\n",
        "    Args:\n",
        "    - inputs: String with format \"question[SEP]context\".\n",
        "\n",
        "    Returns:\n",
        "    - List of token strings.\n",
        "    \"\"\"\n",
        "    if \"[SEP]\" not in inputs:\n",
        "        raise ValueError(\"Input must contain '[SEP]' separating question and context.\")\n",
        "\n",
        "    question, context = inputs.split(\"[SEP]\")\n",
        "    d = nlp.tokenizer(question, context, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
        "    return [nlp.tokenizer.decode([token_id.item()]) for token_id in d[\"input_ids\"].squeeze()]\n",
        "\n",
        "# Attach output names to f_start and f_end\n",
        "f_start.output_names = out_names\n",
        "f_end.output_names = out_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8tXRxy4PaoQ"
      },
      "source": [
        "Let's first take a look at model explanations for identifying the start of our answer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iB9_gRNoGx0k"
      },
      "outputs": [],
      "source": [
        "data = [\"\"\"What is extractive question answering?[SEP]Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\n",
        "question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n",
        "a model on a SQuAD task, you may leverage the examples/question-answering/run_squad.py script.\"\"\"]\n",
        "\n",
        "inputs = tokenizer(data, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "for key in inputs.keys():\n",
        "    inputs[key] = inputs[key].to(\"cuda:0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJ-fwigzmPqA",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "explainer_start = shap.Explainer(f_start, nlp.tokenizer)\n",
        "shap_values_start = explainer_start(data)\n",
        "\n",
        "shap.plots.text(shap_values_start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdFC3kvEPk67",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Let's now take a look at model explanations for identifying the end of our answer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CL_umzIfoBeW",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "explainer_end = shap.Explainer(f_end, nlp.tokenizer)\n",
        "shap_values_end = explainer_end(data)\n",
        "\n",
        "shap.plots.text(shap_values_end)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XHTFBFIPpbt"
      },
      "source": [
        "And finally, the whole extract. Click on the alternative answers and notice how the model is paying 'attention' to different parts of our context:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6WWR3kHoFPe",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def make_answer_scorer(answers):\n",
        "    def f(questions):\n",
        "        out = []\n",
        "        for q in questions:\n",
        "            question, context = q.split(\"[SEP]\")\n",
        "            results = nlp(question, context, top_k=20)\n",
        "            values = []\n",
        "            for answer in answers:\n",
        "                value = 0\n",
        "                for result in results:\n",
        "                    if result[\"answer\"] == answer:\n",
        "                        value = result[\"score\"]\n",
        "                        break\n",
        "                values.append(value)\n",
        "            out.append(values)\n",
        "        return out\n",
        "    f.output_names = answers\n",
        "    return f\n",
        "\n",
        "f_answers = make_answer_scorer([\"the task of extracting an answer from a text given a question\", \"SQuAD dataset\", \"run_squad.py script\"])\n",
        "explainer_answers = shap.Explainer(f_answers, nlp.tokenizer)\n",
        "shap_values_answers = explainer_answers(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7wRGmms-sdw",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "shap.plots.text(shap_values_answers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4t3RxO8D-ZMn"
      },
      "source": [
        "We instantiated a pipeline automatically using the Transformers library in the previous example. Here is another example of question answering using a model and a tokenizer. The process is the following:\n",
        "\n",
        "1. Instantiate a tokenizer and a model from the checkpoint name. The model is identified as a BERT model and loads it\n",
        "   with the weights stored in the checkpoint.\n",
        "2. Define a text and a few questions.\n",
        "3. Iterate over the questions and build a sequence from the text and the current question, with the correct\n",
        "   model-specific separators token type ids and attention masks.\n",
        "4. Pass this sequence through the model. This outputs a range of scores across the entire sequence of tokens (question and\n",
        "   text), for both the start and end positions.\n",
        "5. Compute the softmax of the result to get probabilities over the tokens.\n",
        "6. Fetch the tokens from the identified start and stop values, convert those tokens to a string.\n",
        "7. Print the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbvf9egg-ZMo"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "import torch\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
        "text = r\"\"\"\n",
        "🤗 Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
        "architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet…) for Natural Language Understanding (NLU) and Natural\n",
        "Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
        "TensorFlow 2.0 and PyTorch.\n",
        "\"\"\"\n",
        "questions = [\n",
        "    \"How many pretrained models are available in 🤗 Transformers?\",\n",
        "    \"What does 🤗 Transformers provide?\",\n",
        "    \"🤗 Transformers provides interoperability between which frameworks?\",\n",
        "]\n",
        "for question in questions:\n",
        "    inputs = tokenizer(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
        "\n",
        "    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "    outputs = model(**inputs)\n",
        "    answer_start_scores = outputs.start_logits\n",
        "    answer_end_scores = outputs.end_logits\n",
        "\n",
        "    answer_start = torch.argmax(\n",
        "        answer_start_scores\n",
        "    )  # Get the most likely beginning of answer with the argmax of the score\n",
        "    answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n",
        "\n",
        "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
        "\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Answer: {answer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOrLpn_G-ZMp"
      },
      "source": [
        "### Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AEzL3nu-ZMp"
      },
      "source": [
        "Language modeling is the task of fitting a model to a corpus, which can be domain specific. All popular\n",
        "transformer-based models are trained using a variant of language modeling, e.g., BERT with masked language modeling,\n",
        "GPT-2/3 with causal language modeling.\n",
        "\n",
        "Language modeling can be useful beyond pretraining as well, for example to shift the model distribution to be\n",
        "domain-specific: using a language model trained over a very large corpus, and then fine-tuning it to a news dataset or\n",
        "on scientific papers e.g. [LysandreJik/arxiv-nlp](https://huggingface.co/lysandre/arxiv-nlp)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zwgOcdZ-ZMp"
      },
      "source": [
        "#### Masked Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4r5bR8hM-ZMp"
      },
      "source": [
        "Masked language modeling is the task of masking tokens in a sequence with a masking token, and prompting the model to\n",
        "fill that mask with an appropriate token. This allows the model to attend to both the right context (tokens on the\n",
        "right of the mask) and the left context (tokens on the left of the mask). Such a training creates a strong basis for\n",
        "downstream tasks requiring bi-directional context, such as SQuAD (question answering, see [Lewis, Lui, Goyal et al.](https://arxiv.org/abs/1910.13461), part 4.2).\n",
        "\n",
        "Here is an example of using pipelines to replace a mask from a sequence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBTkRCoE-ZMq"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "nlp = pipeline(\"fill-mask\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDiURWsT-ZMq"
      },
      "source": [
        "This outputs the sequences with the mask filled, the confidence score, and the token id in the tokenizer vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayEq2LE2-ZMq"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "pprint(nlp(f\"HuggingFace is creating a {nlp.tokenizer.mask_token} that the community uses to solve NLP tasks.\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUN9TACu-ZMq"
      },
      "source": [
        "Here is an example of doing masked language modeling using a model and a tokenizer. The process as follows:\n",
        "\n",
        "1. Instantiate a tokenizer and a model from the checkpoint name. The model is identified as a DistilBERT model and\n",
        "   loads it with the weights stored in the checkpoint.\n",
        "2. Define a sequence with a masked token, placing the `tokenizer.mask_token` instead of a word.\n",
        "3. Encode that sequence into a list of IDs and find the position of the masked token in that list.\n",
        "4. Retrieve the predictions at the index of the mask token: this tensor has the same size as the vocabulary, and the\n",
        "   values are the scores attributed to each token. The model gives higher scores to tokens it deems probable in that\n",
        "   context.\n",
        "5. Retrieve the top 5 tokens using the PyTorch `topk` or TensorFlow `top_k` methods.\n",
        "6. Replace the mask token by the tokens and print the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihYScqSt-ZMq"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
        "import torch\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
        "model = AutoModelWithLMHead.from_pretrained(\"distilbert-base-cased\")\n",
        "sequence = f\"Distilled models are smaller than the models they mimic. Using them instead of the large versions would help {tokenizer.mask_token} our carbon footprint.\"\n",
        "input = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
        "mask_token_index = torch.where(input == tokenizer.mask_token_id)[1]\n",
        "token_logits = model(input).logits\n",
        "mask_token_logits = token_logits[0, mask_token_index, :]\n",
        "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVBAG5Al-ZMr"
      },
      "source": [
        "This prints five sequences, with the top 5 tokens predicted by the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyBSdXGp-ZMr"
      },
      "outputs": [],
      "source": [
        "for token in top_5_tokens:\n",
        "    print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxGiKa1h-ZMr"
      },
      "source": [
        "#### Causal Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qt7FighC-ZMr"
      },
      "source": [
        "Causal language modeling is the task of predicting the token following a sequence of tokens. In this situation, the\n",
        "model only attends to the left context (tokens on the left of the mask). Such a training is particularly interesting\n",
        "for generation tasks.\n",
        "\n",
        "Usually, the next token is predicted by sampling from the logits of the last hidden state the model produces from the\n",
        "input sequence.\n",
        "\n",
        "Here is an example of using the tokenizer and model and leveraging the\n",
        "`PreTrainedModel.top_k_top_p_filtering` method to sample the next token following an input sequence\n",
        "of tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28b1aMwXJw2F"
      },
      "outputs": [],
      "source": [
        "from torch import Tensor\n",
        "\n",
        "def top_k_top_p_filtering(\n",
        "    logits: Tensor,\n",
        "    top_k: int = 0,\n",
        "    top_p: float = 1.0,\n",
        "    filter_value: float = -float(\"Inf\"),\n",
        "    min_tokens_to_keep: int = 1,\n",
        ") -> Tensor:\n",
        "    \"\"\"Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
        "    Args:\n",
        "        logits: logits distribution shape (batch size, vocabulary size)\n",
        "        if top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
        "        if top_p < 1.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
        "            Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
        "        Make sure we keep at least min_tokens_to_keep per batch example in the output\n",
        "    From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
        "    \"\"\"\n",
        "    if top_k > 0:\n",
        "        top_k = min(max(top_k, min_tokens_to_keep), logits.size(-1))  # Safety check\n",
        "        # Remove all tokens with a probability less than the last token of the top-k\n",
        "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "\n",
        "    if top_p < 1.0:\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "        # Remove tokens with cumulative probability above the threshold (token with 0 are kept)\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "        if min_tokens_to_keep > 1:\n",
        "            # Keep at least min_tokens_to_keep (set to min_tokens_to_keep-1 because we add the first one below)\n",
        "            sorted_indices_to_remove[..., :min_tokens_to_keep] = 0\n",
        "        # Shift the indices to the right to keep also the first token above the threshold\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        # scatter sorted tensors to original indexing\n",
        "        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
        "        logits[indices_to_remove] = filter_value\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8mfXD4L-ZMs"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "model = AutoModelWithLMHead.from_pretrained(\"gpt2\")\n",
        "sequence = f\"Hugging Face is based in DUMBO, New York City, and \"\n",
        "input_ids = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
        "# get logits of last hidden state\n",
        "next_token_logits = model(input_ids).logits[:, -1, :]\n",
        "# filter\n",
        "filtered_next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=50, top_p=1.0)\n",
        "# sample\n",
        "probs = F.softmax(filtered_next_token_logits, dim=-1)\n",
        "next_token = torch.multinomial(probs, num_samples=1)\n",
        "generated = torch.cat([input_ids, next_token], dim=-1)\n",
        "resulting_string = tokenizer.decode(generated.tolist()[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpULOLx0-ZMs"
      },
      "source": [
        "This outputs a (hopefully) coherent next token following the original sequence, which in our case is the word **has**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHzm0TFJ-ZMs"
      },
      "outputs": [],
      "source": [
        "print(resulting_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQFoV5oM-ZMu"
      },
      "source": [
        "In the next section, we show how this functionality is leveraged in `PreTrainedModel.generate` to\n",
        "generate multiple tokens up to a user-defined length."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZRvqHBm-ZMu"
      },
      "source": [
        "### Text Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJ3yetUs-ZMu"
      },
      "source": [
        "In text generation (**a.k.a** **open-ended text generation**) the goal is to create a coherent portion of text that is a\n",
        "continuation from the given context. The following example shows how **GPT-2** can be used in pipelines to generate text.\n",
        "As a default all models apply **Top-K** sampling when used in pipelines, as configured in their respective configurations\n",
        "(see [gpt-2 config](https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json) for example)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emadcB78-ZMv"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "text_generator = pipeline(\"text-generation\")\n",
        "print(text_generator(\"As far as I am concerned, I will\", max_length=50, do_sample=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxjVQiU3-ZMw"
      },
      "source": [
        "Here, the model generates a random text with a total maximal length of **50** tokens from context **\"As far as I am\n",
        "concerned, I will\"**. The default arguments of `PreTrainedModel.generate()` can be directly overridden in the\n",
        "pipeline, as is shown above for the argument `max_length`.\n",
        "\n",
        "Here is an example of text generation using `XLNet` and its tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHqrRI7R-ZMw"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\").cuda()\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbjjPEsIs_Ks"
      },
      "outputs": [],
      "source": [
        "# set model decoder to true\n",
        "model.config.is_decoder=True\n",
        "# set text-generation params under task_specific_params\n",
        "model.config.task_specific_params[\"text-generation\"] = {\n",
        "    \"do_sample\": True,\n",
        "    \"max_length\": 70,\n",
        "    \"temperature\": 0.7,\n",
        "    \"top_k\": 70,\n",
        "    \"no_repeat_ngram_size\": 2\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIiyMq5uSCFB"
      },
      "source": [
        "Let's now take a look at how a pretrained GPT-2 model completes certain sentences and *why* it completed them as it does:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdUNADbstDS4"
      },
      "outputs": [],
      "source": [
        "sent = [\"Computational social science is the academic sub-discipline concerned with computational approaches to\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tltpYaLmtDnz"
      },
      "outputs": [],
      "source": [
        "explainer = shap.Explainer(model, tokenizer)\n",
        "shap_values = explainer(sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPql-x3ytDuo",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "shap.plots.text(shap_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dtrBpSESW3s"
      },
      "source": [
        "Bingo! We see that 'social science.' is indeed an obvious logical end the sentence. The model explanation is also clearly intuitive. But this was a too easy - how about a more difficult query?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OKFz3F6tDxT"
      },
      "outputs": [],
      "source": [
        "sent = ['Computational social science work increasingly relies on the greater availability of large']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dh1f_1Tsys0J"
      },
      "outputs": [],
      "source": [
        "explainer = shap.Explainer(model, tokenizer)\n",
        "shap_values = explainer(sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRdd2vBtys4v",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "shap.plots.text(shap_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjMfdhkfS9E7"
      },
      "source": [
        "More impressive! Perhaps most interesting is the reasoning our model employs: 'scale' follows due to 'large', but 'data sets' are partly inferred from the context of 'science' and 'relies'. The model picks up on the notion that 'Science relies on data'!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ms4-QUv9-ZMx"
      },
      "source": [
        "Text generation is currently possible with **GPT-2**, **OpenAi-GPT**, **CTRL**, **XLNet**, **Transfo-XL** and **Reformer** in\n",
        "PyTorch and for most models in Tensorflow as well. **XLNet** and **Transfo-XL** often\n",
        "need to be padded to work well. GPT-2 is usually a good choice for **open-ended text generation** because it was trained\n",
        "on millions of webpages with a causal language modeling objective.\n",
        "\n",
        "For more information on how to apply different decoding strategies for text generation, please also refer to this text\n",
        "generation blog post [here](https://huggingface.co/blog/how-to-generate)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xyZikOT-ZMx"
      },
      "source": [
        "### Named Entity Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3naHHJ6E-ZMx"
      },
      "source": [
        "Named Entity Recognition (NER) is the task of classifying tokens according to a class, for example, identifying a token\n",
        "as a person, an organisation or a location. An example of a named entity recognition dataset is the CoNLL-2003 dataset,\n",
        "which is entirely based on that task. If you would like to fine-tune a model on an NER task, you may leverage the\n",
        "[run_ner.py](https://github.com/huggingface/transformers/tree/master/examples/token-classification/run_ner.py)\n",
        "(PyTorch), [run_pl_ner.py](https://github.com/huggingface/transformers/tree/master/examples/token-classification/run_pl_ner.py) (leveraging\n",
        "pytorch-lightning) or the [run_tf_ner.py](https://github.com/huggingface/transformers/tree/master/examples/token-classification/run_tf_ner.py) (TensorFlow)\n",
        "scripts.\n",
        "\n",
        "Here is an example of using pipelines to do named entity recognition, specifically, trying to identify tokens as\n",
        "belonging to one of 9 classes:\n",
        "\n",
        "- O, Outside of a named entity\n",
        "- B-MIS, Beginning of a miscellaneous entity right after another miscellaneous entity\n",
        "- I-MIS, Miscellaneous entity\n",
        "- B-PER, Beginning of a person's name right after another person's name\n",
        "- I-PER, Person's name\n",
        "- B-ORG, Beginning of an organisation right after another organisation\n",
        "- I-ORG, Organisation\n",
        "- B-LOC, Beginning of a location right after another location\n",
        "- I-LOC, Location\n",
        "\n",
        "It leverages a fine-tuned model on CoNLL-2003, fine-tuned by [@stefan-it](https://github.com/stefan-it) from [dbmdz](https://github.com/dbmdz)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJciNeGO-ZMy"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "nlp = pipeline(\"ner\")\n",
        "sequence = \"Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very close to the Manhattan Bridge which is visible from the window.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qU0UdGTt-ZMy"
      },
      "source": [
        "This outputs a list of all words that have been identified as one of the entities from the 9 classes defined above.\n",
        "Here are the expected results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZnjywhw-ZMy"
      },
      "outputs": [],
      "source": [
        "print(nlp(sequence))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AHQ2Sqn-ZMy"
      },
      "source": [
        "Note how the tokens of the sequence \"Hugging Face\" have been identified as an organisation, and \"New York City\",\n",
        "\"DUMBO\" and \"Manhattan Bridge\" have been identified as locations.\n",
        "\n",
        "Here is an example of doing named entity recognition, using a model and a tokenizer. The process is the following:\n",
        "\n",
        "1. Instantiate a tokenizer and a model from the checkpoint name. The model is identified as a BERT model and loads it\n",
        "   with the weights stored in the checkpoint.\n",
        "2. Define the label list with which the model was trained on.\n",
        "3. Define a sequence with known entities, such as \"Hugging Face\" as an organisation and \"New York City\" as a location.\n",
        "4. Split words into tokens so that they can be mapped to predictions. We use a small hack by, first, completely\n",
        "   encoding and decoding the sequence, so that we're left with a string that contains the special tokens.\n",
        "5. Encode that sequence into IDs (special tokens are added automatically).\n",
        "6. Retrieve the predictions by passing the input to the model and getting the first output. This results in a\n",
        "   distribution over the 9 possible classes for each token. We take the argmax to retrieve the most likely class for\n",
        "   each token.\n",
        "7. Zip together each token with its prediction and print it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7le9YPL9-ZMz"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
        "import torch\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "label_list = [\n",
        "    \"O\",       # Outside of a named entity\n",
        "    \"B-MISC\",  # Beginning of a miscellaneous entity right after another miscellaneous entity\n",
        "    \"I-MISC\",  # Miscellaneous entity\n",
        "    \"B-PER\",   # Beginning of a person's name right after another person's name\n",
        "    \"I-PER\",   # Person's name\n",
        "    \"B-ORG\",   # Beginning of an organisation right after another organisation\n",
        "    \"I-ORG\",   # Organisation\n",
        "    \"B-LOC\",   # Beginning of a location right after another location\n",
        "    \"I-LOC\"    # Location\n",
        "]\n",
        "sequence = \"Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very\" \\\n",
        "           \"close to the Manhattan Bridge.\"\n",
        "# Bit of a hack to get the tokens with the special tokens\n",
        "tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(sequence)))\n",
        "inputs = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
        "outputs = model(inputs).logits\n",
        "predictions = torch.argmax(outputs, dim=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRmyFjWv-ZMz"
      },
      "source": [
        "This outputs a list of each token mapped to its corresponding prediction. Differently from the pipeline, here every\n",
        "token has a prediction as we didn't remove the \"0\"th class, which means that no particular entity was found on that\n",
        "token. The following array should be the output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVGGzja3-ZMz"
      },
      "outputs": [],
      "source": [
        "print([(token, label_list[prediction]) for token, prediction in zip(tokens, predictions[0].numpy())])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-H5fFtGt-ZM0"
      },
      "source": [
        "### Summarization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LE9OrGAJ-ZM0"
      },
      "source": [
        "Summarization is the task of summarizing a document or an article into a shorter text.\n",
        "\n",
        "An example of a summarization dataset is the CNN / Daily Mail dataset, which consists of long news articles and was\n",
        "created for the task of summarization. If you would like to fine-tune a model on a summarization task, various\n",
        "approaches are described in this [document](https://github.com/huggingface/transformers/blob/master/examples/seq2seq/README.md).\n",
        "\n",
        "Here is an example of using the pipelines to do summarization. It leverages a Bart model that was fine-tuned on the CNN\n",
        "/ Daily Mail data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBmfYCc6-ZM0"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "summarizer = pipeline(\"summarization\")\n",
        "ARTICLE = \"\"\" New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\n",
        "A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\n",
        "Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other.\n",
        "In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\n",
        "Barrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the\n",
        "2010 marriage license application, according to court documents.\n",
        "Prosecutors said the marriages were part of an immigration scam.\n",
        "On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\n",
        "After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective\n",
        "Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.\n",
        "All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.\n",
        "Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.\n",
        "Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.\n",
        "The case was referred to the Bronx District Attorney\\'s Office by Immigration and Customs Enforcement and the Department of Homeland Security\\'s\n",
        "Investigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.\n",
        "Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.\n",
        "If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNppiVo2-ZM0"
      },
      "source": [
        "Because the summarization pipeline depends on the `PreTrainedModel.generate()` method, we can override the default\n",
        "arguments of `PreTrainedModel.generate()` directly in the pipeline for `max_length` and `min_length` as shown\n",
        "below. This outputs the following summary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6GAHEEl-ZM0"
      },
      "outputs": [],
      "source": [
        "print(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rn9S6Rmn-ZM1"
      },
      "source": [
        "Here is an example of doing summarization using a model and a tokenizer. The process is the following:\n",
        "\n",
        "1. Instantiate a tokenizer and a model from the checkpoint name. Summarization is usually done using an encoder-decoder\n",
        "   model, such as `Bart` or `T5`.\n",
        "2. Define the article that should be summarized.\n",
        "3. Add the T5 specific prefix \"summarize: \".\n",
        "4. Use the `PreTrainedModel.generate()` method to generate the summary.\n",
        "\n",
        "In this example we use Google`s T5 model. Even though it was pre-trained only on a multi-task mixed dataset (including\n",
        "CNN / Daily Mail), it yields very good results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rfuam6GI-ZM1"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
        "model = AutoModelWithLMHead.from_pretrained(\"t5-base\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
        "# T5 uses a max_length of 512 so we cut the article to 512 tokens.\n",
        "inputs = tokenizer.encode(\"summarize: \" + ARTICLE, return_tensors=\"pt\", max_length=512)\n",
        "outputs = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8gF8TN1-ZM1"
      },
      "source": [
        "### Translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBB0eYEv-ZM1"
      },
      "source": [
        "We have already seen Translation examples early in this notebook. We will now use a different case with BERT.\n",
        "\n",
        "An example of a translation dataset is the WMT English to German dataset, which has sentences in English as the input\n",
        "data and the corresponding sentences in German as the target data. If you would like to fine-tune a model on a\n",
        "translation task, various approaches are described in this [document](https://github.com/huggingface/transformers/blob/master/examples/seq2seq/README.md).\n",
        "\n",
        "Here is an example of using the pipelines to do translation. It leverages a T5 model that was only pre-trained on a\n",
        "multi-task mixture dataset (including WMT), yet, yielding impressive translation results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnKOrsbD-ZM1"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "translator = pipeline(\"translation_en_to_de\")\n",
        "print(translator(\"Hugging Face is a technology company based in New York and Paris\", max_length=40))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esSGSUHh-ZM2"
      },
      "source": [
        "Because the translation pipeline depends on the `PreTrainedModel.generate()` method, we can override the default\n",
        "arguments of `PreTrainedModel.generate()` directly in the pipeline as is shown for `max_length` above.\n",
        "\n",
        "Here is an example of doing translation using a model and a tokenizer. The process is the following:\n",
        "\n",
        "1. Instantiate a tokenizer and a model from the checkpoint name. Summarization is usually done using an encoder-decoder\n",
        "   model, such as `Bart` or `T5`.\n",
        "2. Define the article that should be summarized.\n",
        "3. Add the T5 specific prefix \"translate English to German: \"\n",
        "4. Use the `PreTrainedModel.generate()` method to perform the translation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKIy-6ba-ZM2"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
        "model = AutoModelWithLMHead.from_pretrained(\"t5-base\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
        "inputs = tokenizer.encode(\"translate English to German: Hugging Face is a technology company based in New York and Paris\", return_tensors=\"pt\")\n",
        "outputs = model.generate(inputs, max_length=40, num_beams=4, early_stopping=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PMky27P-ZM2"
      },
      "source": [
        "As with the pipeline example, we get the same translation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfsUBnVb-ZM2"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.decode(outputs[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47yVwspLAvZx"
      },
      "source": [
        "## Fine tuning models  \n",
        "\n",
        "One of the most exciting things about BERT and GPT is being able to retune them the way we want to. This is what allows us to unleash the full power of BERT, whether it is for classification, generation, or language modelling.\n",
        "\n",
        "The first section contains the CoLA (Linguistic Acceptability) classification task, the second contains training a model on Trump tweets, and the last bit has us training a model on US and UK blog posts. In each of these cases we are adapting our model to a specific domain.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nFkME77f1Zh"
      },
      "source": [
        "### Fine tuning sequence classification\n",
        "\n",
        "To demonstrate this, we will use the [Corpus of Linguistic Acceptability](https://nyu-mll.github.io/CoLA/). This corpus has sentences which are labelled as \"acceptable\" and \"unnacceptable\", and is a test of natural language understanding.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pwp7W3BWWERu"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertConfig\n",
        "from transformers import AdamW, BertForSequenceClassification\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import io"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPbf8Yvyf-Hn"
      },
      "source": [
        "We'll now load the CoLA dataset up here to Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZH0HWkNgbho"
      },
      "outputs": [],
      "source": [
        "!pip install wget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRqSO7DUWXmR"
      },
      "outputs": [],
      "source": [
        "import wget\n",
        "import os\n",
        "\n",
        "print('Downloading dataset...')\n",
        "\n",
        "# The URL for the dataset zip file.\n",
        "url = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'\n",
        "\n",
        "# Download the file (if we haven't already)\n",
        "if not os.path.exists('./cola_public_1.1.zip'):\n",
        "    wget.download(url, './cola_public_1.1.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dewGhdzgEkH"
      },
      "outputs": [],
      "source": [
        "# Unzip the dataset (if we haven't already)\n",
        "if not os.path.exists('./cola_public/'):\n",
        "    !unzip cola_public_1.1.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzyIo3XfgVxJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"./cola_public/raw/in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onaOGdjxgiyG"
      },
      "outputs": [],
      "source": [
        "df.loc[df.label == 0].sample(5)[['sentence', 'label']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "saNk183jgjVP"
      },
      "outputs": [],
      "source": [
        "# Create sentence and label lists\n",
        "sentences = df.sentence.values\n",
        "\n",
        "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
        "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
        "labels = df.label.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOLRljp3gr9T"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "print (\"Tokenize the first sentence:\")\n",
        "print (tokenized_texts[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wywdbBISguFj"
      },
      "outputs": [],
      "source": [
        "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway.\n",
        "# In the original paper, the authors used a length of 512.\n",
        "MAX_LEN = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfXhgejEgxRY"
      },
      "outputs": [],
      "source": [
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ko6fHIlAiHm7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSaU7DbkgzWh"
      },
      "outputs": [],
      "source": [
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEUBxUm8g0ov"
      },
      "outputs": [],
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    attention_masks.append(seq_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4QPyBRDiIx1"
      },
      "outputs": [],
      "source": [
        "# Use train_test_split to split our data into train and validation sets for training\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels,\n",
        "                                                            random_state=2020, test_size=0.1)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
        "                                             random_state=2020, test_size=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLwGeP3u6s0C"
      },
      "source": [
        "#### How do shallow networks do?\n",
        "\n",
        "Just to get a reference of how easy/difficult this task is, let's train a keras LSTM model to perform this classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWLfSUPjiLJA"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQ0A0HbAiMpc"
      },
      "outputs": [],
      "source": [
        "vocab_in_size = tokenizer.vocab_size\n",
        "embedding_dim = 32\n",
        "unit = 100\n",
        "no_labels = len(np.unique(train_labels))\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POlslC2AiPAI"
      },
      "outputs": [],
      "source": [
        "model_lstm = Sequential()\n",
        "model_lstm.add(Embedding(vocab_in_size, embedding_dim, input_length=MAX_LEN))\n",
        "model_lstm.add(LSTM(unit))\n",
        "model_lstm.add(Dense(no_labels, activation='softmax'))\n",
        "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_lstm.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gu2c6CWSiP3D"
      },
      "outputs": [],
      "source": [
        "history_lstm = model_lstm.fit(train_inputs, train_labels,\n",
        "                              epochs=10,batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6muAlDeiac7"
      },
      "source": [
        "#### On with BERT!\n",
        "\n",
        "So while Neural Networks can do a good job with some kind of classification tasks, they don't perform too well on intent classification. Let us see how BERT might do."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJDljt39iddQ"
      },
      "outputs": [],
      "source": [
        "# Convert all of our data into torch tensors, the required datatype for our model\n",
        "\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgmJ-E_VigTt"
      },
      "outputs": [],
      "source": [
        "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
        "batch_size = 32\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop,\n",
        "# with an iterator the entire dataset does not need to be loaded into memory\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUPMINIHQmni"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eX6t1SIUi22s"
      },
      "source": [
        "#### Loading our pre-trained model\n",
        "\n",
        "#### Train Model\n",
        "Now that our input data is properly formatted, it's time to fine tune the BERT model.\n",
        "For this task, we first want to modify the pre-trained BERT model to give outputs for classification, and then we want to continue training the model on our dataset until that the entire model, end-to-end, is well-suited for our task. (Note that sometimes fine-tuning is also called pre-training the model.) Thankfully, the huggingface pytorch implementation includes a set of interfaces designed for a variety of NLP tasks. Though these interfaces are all built atop a trained BERT model, each has different top layers and output types designed to accomodate their specific NLP task.\n",
        "We'll load BertForSequenceClassification. This is the normal BERT model with an added single linear layer on top for classification that we will use as a sentence classifier. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task.\n",
        "\n",
        "#### Structure of Fine-Tuning Model\n",
        "\n",
        "As we've showed before, the first token of every sequence is the special classification token ([CLS]). Unlike the hidden state vector corresponding to a normal word token, the hidden state corresponding to this special token is designated by the authors of BERT as an aggregate representation of the whole sentence used for classification tasks. As such, when we feed in an input sentence to our model during training, the output is the length 768 hidden state vector corresponding to this token. The additional layer that we've added on top consists of untrained linear neurons of size [hidden_state, number_of_labels], so [768,2], meaning that the output of BERT plus our classification layer is a vector of two numbers representing the \"score\" for \"grammatical/non-grammatical\" that are then fed into a cross-entropy loss function.\n",
        "\n",
        "#### The Fine-Tuning (or, confusingly, Pre-Training) Process\n",
        "\n",
        "Because the pre-trained BERT layers already encode a lot of information about the language, training the classifier is relatively inexpensive. Rather than training every layer in a large model from scratch, it's as if we have already trained the bottom layers 95% of where they need to be, and only really need to train the top layer, with a bit of tweaking going on in the lower levels to accomodate our task.\n",
        "Sometimes practicioners will opt to \"freeze\" certain layers when fine-tuning, or to apply different learning rates, apply diminishing learning rates, etc. all in an effort to preserve the good quality weights in the network and speed up training (often considerably). In fact, recent research on BERT specifically has demonstrated that freezing the majority of the weights results in only minimal accuracy declines, but there are exceptions and broader rules of transfer learning that should also be considered. For example, if your task and fine-tuning dataset is very different from the dataset used to train the transfer learning model, freezing the weights may not be a good idea. We'll cover the broader scope of transfer learning in NLP in a future post.\n",
        "OK, let's load BERT! There are a few different pre-trained BERT models available. \"bert-base-uncased\" means the version that has only lowercase letters (\"uncased\") and is the smaller version of the two (\"base\" vs \"large\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYkR2tJxiiJk"
      },
      "outputs": [],
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfwPyoh9ii0x"
      },
      "outputs": [],
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "likqi1-FimRZ"
      },
      "outputs": [],
      "source": [
        "# This variable contains all of the hyperparemeter information our training loop needs\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJcXKErgioEE"
      },
      "outputs": [],
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-fvXJW_ip1k"
      },
      "outputs": [],
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtCJt0A0irEL"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "\n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnwi-bj1itbL"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "\n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "\n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to\n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "\n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader.\n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the\n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids\n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because\n",
        "        # accumulating the gradients is \"convenient while training RNNs\".\n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here:\n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids,\n",
        "                    token_type_ids=None,\n",
        "                    attention_mask=b_input_mask,\n",
        "                    labels=b_labels)\n",
        "\n",
        "        # The call to `model` always returns a tuple, so we need to pull the\n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value\n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "\n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which\n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here:\n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids,\n",
        "                            token_type_ids=None,\n",
        "                            attention_mask=b_input_mask)\n",
        "\n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "\n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZPeyGKYtoCn"
      },
      "outputs": [],
      "source": [
        "loss_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFogz7jeiycc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"./cola_public/raw/out_of_domain_dev.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                   )\n",
        "\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN,\n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    attention_masks.append(seq_mask)\n",
        "\n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3S6GaadjK48"
      },
      "outputs": [],
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables\n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict\n",
        "for batch in prediction_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "    # Telling the model not to compute or store gradients, saving memory and\n",
        "    # speeding up prediction\n",
        "    with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None,\n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "    logits = outputs[0]\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    # Store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_fqVB6CtzTo"
      },
      "outputs": [],
      "source": [
        "print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIoseqqzt1pc"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "\n",
        "    # The predictions for this batch are a 2-column ndarray (one column for \"0\"\n",
        "    # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "    # in to a list of 0s and 1s.\n",
        "    pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "\n",
        "    # Calculate and store the coef for this batch.\n",
        "    matthews = matthews_corrcoef(true_labels[i], pred_labels_i)\n",
        "    matthews_set.append(matthews)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BreKUJ9t25l"
      },
      "outputs": [],
      "source": [
        "matthews_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20s0Poipt6XT"
      },
      "outputs": [],
      "source": [
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('MCC: %.3f' % mcc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tqQ5tX2C6K_"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLFKqDerDw3n"
      },
      "outputs": [],
      "source": [
        "print(classification_report(flat_true_labels, flat_predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpLhjm3D9-vA"
      },
      "source": [
        "We see that BERT performs a lot better  on the Linguistic Acceptability task!\n",
        "\n",
        "We now want to save this model to disk. Once you save it to disk, right click on the file and save it to your local computer to use it on your notebook. You would have trained your model on your own dataset, in which case you would also need to upload your data, or load it using wget as we did before.\n",
        "\n",
        "NOTE: the files are accessible, both uploading and downloading, on the left hand side of the page, under the \"folder\" section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJ1Y-b7-90S9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = './model_save/'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vq7G7JbAt9sn"
      },
      "source": [
        "BERT is a powerful tool for sequence classification and we encourage you to try it out for your dataset of choice!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCyfnZ7i-NzR"
      },
      "source": [
        "### Fine-tuning text generation\n",
        "\n",
        "You have to upload your files to the colab file - on the left of the screen, on the files section, click the upload section, and upload test_text_trump, train_text_trump, run_generation.py, and run_language_modelling.py. These files would be on this [GitHub repository page](https://github.com/UChicago-Computational-Content-Analysis/Homework-Notebooks/tree/main/week-8).\n",
        "\n",
        "We start with training a model on Trump tweets. The following two lines of code does the language training, which we will then use for text generation. The important part here is training your model. You will notice it being saved in the files section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XdVsM_cL_3O"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqiVuxFuLLvu"
      },
      "outputs": [],
      "source": [
        "!python /content/run_language_modelling.py --output_dir=output_gpt_trump --model_type=gpt2 --model_name_or_path=gpt2 --do_train --train_data_file=/content/train_text_trump --do_eval --eval_data_file=/content/test_text_trump --per_gpu_train_batch_size=1 --per_gpu_eval_batch_size=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1LRPvhkP8Mm"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelWithLMHead, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fu-a109sOcS5"
      },
      "outputs": [],
      "source": [
        "tokenizer_trump = AutoTokenizer.from_pretrained(\"/content/output_gpt_trump\")\n",
        "model_trump = AutoModelWithLMHead.from_pretrained(\"/content/output_gpt_trump\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXi4jQIdwkHN"
      },
      "source": [
        "Now that we've loaded our fine-tuned Trump GPT model, let's see what the model thinks of Obama."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ov5YOjtOh0C"
      },
      "outputs": [],
      "source": [
        "sequence = \"Obama is going to\"\n",
        "\n",
        "input = tokenizer_trump.encode(sequence, return_tensors=\"pt\")\n",
        "generated = model_trump.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
        "\n",
        "resulting_string = tokenizer_trump.decode(generated.tolist()[0])\n",
        "print(resulting_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEFIJaHnxcE2"
      },
      "source": [
        "Yikes. Sounds like what we'd expect from a Trump bot. Let's now try with the standard, untuned GPT2 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ye9REQXyd3R"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
        "\n",
        "tokenizer_gpt = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "model_gpt = AutoModelWithLMHead.from_pretrained(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gf7GRWQ8Okmu"
      },
      "outputs": [],
      "source": [
        "sequence = \"Obama is going to\"\n",
        "\n",
        "input = tokenizer_gpt.encode(sequence, return_tensors=\"pt\")\n",
        "generated = model_gpt.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
        "\n",
        "resulting_string = tokenizer_gpt.decode(generated.tolist()[0])\n",
        "print(resulting_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-CDbhF8SQoN"
      },
      "source": [
        "We see dramatically different results for the two models!\n",
        "\n",
        "### Fine tuning language models\n",
        "\n",
        "We will also look at an example of training BERT-like models, such as RoBERTa, on US and UK blog posts. Our hope is to introduce an \"accent\" in a model, which learns distrinctive aspects of the kinds of speech - but considering how similar they can be and that we don't have access to a _lot_ of data, so it may not be perfect."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwsrsQ2qSVnN"
      },
      "outputs": [],
      "source": [
        "!python run_language_modelling.py --output_dir=output_roberta_US --model_type=roberta --model_name_or_path=roberta-base --do_train --train_data_file=us_blog_train --do_eval --eval_data_file=us_blog_test --mlm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R80-_g-xpiwx"
      },
      "outputs": [],
      "source": [
        "!python run_language_modelling.py --output_dir=output_roberta_GB --model_type=roberta --model_name_or_path=roberta-base --do_train --train_data_file=gb_blog_train --do_eval --eval_data_file=gb_blog_test --mlm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eC_wG9PGPIAx"
      },
      "outputs": [],
      "source": [
        "from transformers import RobertaConfig, RobertaModel, RobertaTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-N1_R-9EPIoY"
      },
      "outputs": [],
      "source": [
        "roberta_us_model_embedding = RobertaModel.from_pretrained('/content/roberta_us')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmWY040oPKw-"
      },
      "outputs": [],
      "source": [
        "roberta_us_tokenizer = RobertaTokenizer.from_pretrained('/content/roberta_us')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrJ0SUEgPPYR"
      },
      "source": [
        "Let us try to visualise how words in a sentence or different or similar to each other. We will try to construct sentences where words might mean different things in different countries - in the US, people might eat chips with salsa, but in the UK, chips are what Americans call french fries, and might eat it fried fish instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YAHxlZyPKqI"
      },
      "outputs": [],
      "source": [
        "text = \"Do you have your chips with fish or with salsa?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lAOBNvYPSjh"
      },
      "outputs": [],
      "source": [
        "text1 = \"He went out in just his undershirt and pants.\" #pants are underwear in Britain; maybe closer to an undershirt\n",
        "text2 = \"His braces completed the outfit.\" #braces are suspenders (in Britain); maybe closer to an outfit\n",
        "text3 = \"Does your pencil have a rubber on it?\" #rubber is an eraser in Britain); maybe closer to a pencil\n",
        "text4 = \"Was the bog closer to the forest or the house?\" #bog is a toilen in Britain); maybe closer to a house\n",
        "text5 = \"Are you taking the trolley or the train to the grocery market\" #trolley is a food carriage; possibly closer to a market"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4GJGzrsPSXm"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlPmWCSPPX9A"
      },
      "outputs": [],
      "source": [
        "def visualise_diffs(text, model, tokenizer):\n",
        "    word_vecs = []\n",
        "    for i in range(0, len(text.split())):\n",
        "        word_vecs.append(word_vector(text, i, model, tokenizer))\n",
        "    L = []\n",
        "    for p in word_vecs:\n",
        "        l = []\n",
        "        for q in word_vecs:\n",
        "            l.append(1 - cosine(p, q))\n",
        "        L.append(l)\n",
        "    M = np.array(L)\n",
        "    fig = plt.figure()\n",
        "    div = pd.DataFrame(M, columns = list(text.split()), index = list(text.split()))\n",
        "    ax = sns.heatmap(div)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMe1SrUOPbZs"
      },
      "outputs": [],
      "source": [
        "visualise_diffs(text, roberta_us_model_embedding, roberta_us_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pksnpt6hPdU4"
      },
      "outputs": [],
      "source": [
        "roberta_gb_model_embedding = RobertaModel.from_pretrained('/content/roberta_gb')\n",
        "roberta_gb_tokenizer = RobertaTokenizer.from_pretrained('/content/roberta_gb')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kus6hDHSPfCG"
      },
      "outputs": [],
      "source": [
        "visualise_diffs(text, roberta_gb_model_embedding, roberta_gb_tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOcuLpiMPjGv"
      },
      "source": [
        "What do you see regarding the relations with chips and sala/fish? What about the other sentences? How about comparing sentence embeddings?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46JPmKEzBcJ1"
      },
      "source": [
        "### Post-training (Pre-training) and Domain Specific Models\n",
        "\n",
        "After you finish training your models, you can download them by right clicking on each of the 8 files inside the name of your output folder. The largest file, pytorch_model.bin, is the file which does most of the heavy lifting but you need the other files to be able to use it later.\n",
        "\n",
        "We also recommend checking out other domain specific models, such as [sciBERT](https://github.com/allenai/scibert) and [bioBERT](https://github.com/dmis-lab/biobert). This link should be useful for this [domain BERT tutorial](https://mccormickml.com/2020/06/22/domain-specific-bert-tutorial/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSDWW3sOF80A"
      },
      "source": [
        "## Homework Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3v7IkE4E1HX"
      },
      "source": [
        "**1)** Use a large pre-trained language model different from the one used in the prior module's assignment for sequence classification, question answering, language modeling, or any other NLP task. Fine-tune the model on your dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-eoZB_ARE1HX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqWuMCCSE1HX"
      },
      "source": [
        "**2)** What model did you use? How did your model perform?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huP7zHA5E1HX"
      },
      "outputs": [],
      "source": [
        "model_performance = 'something' #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RX2nO2EaAdHf"
      },
      "outputs": [],
      "source": [
        "# empty cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QG7FcZJ4GUI-"
      },
      "source": [
        "**3)** BONUS: use a PyTorch or Keras sequence to sequence model to perform a task other than machine translation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfqyZ5S5JB8d"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlhFQGoYGVsx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0GYy34n1zmB"
      },
      "source": [
        "# Module 4: BERT Word and Sentence Embeddings\n",
        "\n",
        "In this section we will explore the many ways we can use BERT to generate word and sentence embeddings.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIKFzr2s7ZtK"
      },
      "source": [
        "### Embeddings, Context Words\n",
        "\n",
        "We saw how a bootstrapped BERT model performed so much better than a model trained from scatch. Because BERT's method of capturing context is bidirectional, meaning that words can now have different word embedding values based on their location within a sentence. Let us use the same BERT model to capture sentence and word embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sFlXkaFf7ZtK"
      },
      "outputs": [],
      "source": [
        "from transformers import BertModel\n",
        "from transformers import BertTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crmHvLsp7ZtK"
      },
      "source": [
        "Let's go through the sentence format for the BERT model, as well as how our vocabulary looks like. Note that you have to use the BERT tokenizer to use the BERT model because of the similar vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "o4goTAyK7ZtK"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnCAUot67ZtK"
      },
      "outputs": [],
      "source": [
        "text = \"Here is the sentence I want embeddings for.\"\n",
        "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "\n",
        "# Tokenize our sentence with the BERT tokenizer.\n",
        "tokenized_text = tokenizer.tokenize(marked_text)\n",
        "\n",
        "# Print out the tokens.\n",
        "print (tokenized_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzcdpcIK7ZtL"
      },
      "source": [
        "The BERT model uses a WordPiece technique to do its tokenizing, as described in the initial published paper. That's why the word embedding is split up the way it is.\n",
        "A quick peek at what the voabulary looks like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hVi4xx57ZtL"
      },
      "outputs": [],
      "source": [
        "list(tokenizer.vocab.keys())[6000:6030]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHBVq2rk7ZtL"
      },
      "outputs": [],
      "source": [
        "# Define a new example sentence with multiple meanings of the word \"bank\"\n",
        "text = \"After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank.\"\n",
        "\n",
        "# Add the special tokens.\n",
        "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "\n",
        "# Split the sentence into tokens.\n",
        "tokenized_text = tokenizer.tokenize(marked_text)\n",
        "\n",
        "# Map the token strings to their vocabulary indeces.\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "# Display the words with their indices.\n",
        "for tup in zip(tokenized_text, indexed_tokens):\n",
        "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uURNyYKD7ZtL"
      },
      "source": [
        "#### Segment ID\n",
        "\n",
        "BERT is trained on and expects sentence pairs, using 1s and 0s to distinguish between the two sentences. That is, for each token in “tokenized_text,” we must specify which sentence it belongs to: sentence 0 (a series of 0s) or sentence 1 (a series of 1s). For our purposes, single-sentence inputs only require a series of 1s, so we will create a vector of 1s for each token in our input sentence.\n",
        "\n",
        "If you want to process two sentences, assign each word in the first sentence plus the ‘[SEP]’ token at 0, and all tokens of the second sentence a 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWF4CpzR7ZtM"
      },
      "outputs": [],
      "source": [
        "segments_ids = [1] * len(tokenized_text)\n",
        "\n",
        "print (segments_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oY0Goj27ZtN"
      },
      "source": [
        "As we did for classification, we now convert these segments to tensors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8Bt1vwX7ZtN"
      },
      "source": [
        "The embedding layer is the hidden state layer, and this is what we pick up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "HkgVruQM7ZtN"
      },
      "outputs": [],
      "source": [
        "# Convert inputs to PyTorch tensors\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "segments_tensors = torch.tensor([segments_ids])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkYDOGeg7ZtO"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained model (weights)\n",
        "model_embedding = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
        "model_embedding.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "1uEUCKHD7ZtO"
      },
      "outputs": [],
      "source": [
        "output = model_embedding(tokens_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "S5rDTvJR7ZtO"
      },
      "outputs": [],
      "source": [
        "output[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uwa_CRL47ZtO"
      },
      "outputs": [],
      "source": [
        "output[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_XYjM4p7ZtP"
      },
      "source": [
        "#### Understanding the Output\n",
        "\n",
        "This kind of forward pass returns us the last layer of the net, which we will use to make our vectors. The first object returned contains the batch number, followed by each of the tokens and their vector values. The second object contains a vector value, which I suspect is the sentence vector of the tokens.\n",
        "\n",
        "The first index is the batch size, and our batch size is 1, so we just choose the 0th index and work with that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tXlaU_EO7ZtP"
      },
      "outputs": [],
      "source": [
        "word_embeddings, sentence_embedding = output[0], output[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tr1Fe4IZ7ZtP"
      },
      "outputs": [],
      "source": [
        "len(word_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSRojl6QP9LE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m91OJMw17ZtP"
      },
      "outputs": [],
      "source": [
        "word_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J25Lkeb7ZtQ"
      },
      "source": [
        "Let’s take a quick look at the range of values for a given layer and token.\n",
        "\n",
        "You’ll find that the range is fairly similar for all layers and tokens, with the majority of values falling between [-2, 2], and a small smattering of values around -10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROd5AAVzP951"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6D6zo2IJ7ZtQ"
      },
      "outputs": [],
      "source": [
        "vec = word_embeddings[0][0]\n",
        "vec = vec.detach().numpy()\n",
        "# Plot the values as a histogram to show their distribution.\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.hist(vec, bins=200)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t16xoZYq7ZtQ"
      },
      "source": [
        "These values are grouped by layer - we can use the permute function to make it grouped by each individual token instead. Let us look at what the latter looks like:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xd8NOQB7ZtQ"
      },
      "source": [
        "#### Word Vectors\n",
        "\n",
        "So each of those tokens have embedding values - let us try and compare them with each other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "aJO51oGQ7ZtR"
      },
      "outputs": [],
      "source": [
        "token_vecs = []\n",
        "# For each token in the sentence...\n",
        "for embedding in word_embeddings[0]:\n",
        "    cat_vec = embedding.detach().numpy()\n",
        "    # Use `cat_vec` to represent `token`.\n",
        "    token_vecs.append(cat_vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oZR3FKN7ZtR"
      },
      "outputs": [],
      "source": [
        "len(token_vecs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5KWAimA7ZtR"
      },
      "source": [
        "Another method to create the vectors is to sum the last four layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhCeAYWN7ZtR"
      },
      "source": [
        "#### Sentence Vector\n",
        "\n",
        "To get a single vector for our entire sentence we have multiple application-dependent strategies - we could just average all the tokens in our sentence. We can also use this oppurtunity to see if the second vector returned is a sentence vector too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "UlahGmRm7ZtS"
      },
      "outputs": [],
      "source": [
        "sentence_embedding_0 = sentence_embedding.detach().numpy()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Nyv_QXsk7ZtS"
      },
      "outputs": [],
      "source": [
        "sentence_embedding_1 = np.mean(token_vecs, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikFTzpVp7ZtS"
      },
      "outputs": [],
      "source": [
        "len(sentence_embedding_0), len(sentence_embedding_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAB1xXXf7ZtT"
      },
      "source": [
        "Remember that the power of these vectors is how they are context dependant - our sentence had multiple uses of the word bank. Let us see the index and the word of the sentence and check the context accordingly. We'll then print the simlarity values for the similar and different meanings and see how it turns out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sr0_tT2g7ZtT"
      },
      "outputs": [],
      "source": [
        "for i, token_str in enumerate(tokenized_text):\n",
        "    print(i, token_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SLTbo087ZtT"
      },
      "outputs": [],
      "source": [
        "print('First 5 vector values for each instance of \"bank\".')\n",
        "print('')\n",
        "print(\"bank vault   \", str(token_vecs[6][:5]))\n",
        "print(\"bank robber  \", str(token_vecs[10][:5]))\n",
        "print(\"river bank   \", str(token_vecs[19][:5]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3DMy8cah0B-"
      },
      "source": [
        "This means that within the space inscribed by BERT vectors, each word is represented not by a point (or vector) in the space, but rather by a cloud of points (or vectors)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "um9VIP-D7ZtT"
      },
      "outputs": [],
      "source": [
        "from scipy.spatial.distance import cosine\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYJAau6T7ZtT"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Calculate the cosine similarity between the word bank\n",
        "# in \"bank robber\" vs \"river bank\" (different meanings).\n",
        "diff_bank = 1 - cosine(token_vecs[10], token_vecs[19])\n",
        "\n",
        "# Calculate the cosine similarity between the word bank\n",
        "# in \"bank robber\" vs \"bank vault\" (same meaning).\n",
        "same_bank = 1 - cosine(token_vecs[10], token_vecs[6])\n",
        "\n",
        "print('Vector similarity for  *similar*  meanings:  %.2f' % same_bank)\n",
        "print('Vector similarity for *different* meanings:  %.2f' % diff_bank)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BesrJbg7ZtU"
      },
      "source": [
        "This makes sense! Let us see if the mean value of all the tokens and what we think is the sentence vector is the same thing, by checking their cosine distance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPRi2ErU7ZtU"
      },
      "outputs": [],
      "source": [
        "1 - cosine(sentence_embedding_0, sentence_embedding_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7xbAiqb7ZtU"
      },
      "source": [
        "That is good - it seems it is indeed the sentence vector, so we can now write two functions which calculate the word and sentence vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ZioE7gS77ZtU"
      },
      "outputs": [],
      "source": [
        "def word_vector(text, word_id, model, tokenizer):\n",
        "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "    tokenized_text = tokenizer.tokenize(marked_text)\n",
        "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "    tokens_tensor = torch.tensor([indexed_tokens])\n",
        "    output = model(tokens_tensor)\n",
        "    word_embeddings, sentence_embeddings = output[0], output[1]\n",
        "    vector = word_embeddings[0][word_id].detach().numpy()\n",
        "    return vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "wsl46pjQ7ZtU"
      },
      "outputs": [],
      "source": [
        "word_10 = word_vector(text, 6, model_embedding, tokenizer)\n",
        "word_6 = word_vector(text, 10, model_embedding, tokenizer)\n",
        "word_19 = word_vector(text, 19, model_embedding, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YYIlUpJt7ZtV"
      },
      "outputs": [],
      "source": [
        "def sentence_vector(text, model, tokenizer, method=\"average\"):\n",
        "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "    tokenized_text = tokenizer.tokenize(marked_text)\n",
        "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "    tokens_tensor = torch.tensor([indexed_tokens])\n",
        "    output = model(tokens_tensor)\n",
        "    word_embeddings, sentence_embeddings = output[0], output[1]\n",
        "    token_vecs = []\n",
        "\n",
        "    for embedding in word_embeddings[0]:\n",
        "        cat_vec = embedding.detach().numpy()\n",
        "        token_vecs.append(cat_vec)\n",
        "\n",
        "    if method == \"average\":\n",
        "        sentence_embedding = np.mean(token_vecs, axis=0)\n",
        "    if method == \"model\":\n",
        "        sentence_embedding = sentence_embeddings\n",
        "    # do something\n",
        "    return sentence_embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "lqAidK2r7ZtV"
      },
      "outputs": [],
      "source": [
        "sen_vec_0 = sentence_vector(text, model_embedding, tokenizer)\n",
        "sen_vec_1 = sentence_vector(text, model_embedding, tokenizer, method=\"model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GsjHA-o7ZtV"
      },
      "source": [
        "#### Similarity metrics\n",
        "It is worth noting that word-level similarity comparisons are not appropriate with BERT embeddings because these embeddings are contextually dependent, meaning that the word vector changes depending on the sentence in which it appears. This enables direct sensitivity to polysemy so that, e.g., your representation encodes river “bank” and not a financial institution “bank”. Nevertheless, it makes direct word-to-word similarity comparisons less valuable. For sentence embeddings, however, similarity comparison is still valid such that one can query, for example, a single sentence against a dataset of other sentences in order to find the most similar. Depending on the similarity metric used, the resulting similarity values will be less informative than the relative ranking of similarity outputs as some similarity metrics make assumptions about the vector space (equally-weighted dimensions, for example) that do not hold for our 768-dimensional vector space.\n",
        "\n",
        "### Using the Vectors\n",
        "\n",
        "Without fine-tuning, BERT features may be less useful than plain GloVe or word2vec.\n",
        "They start to be interesting when you fine-tune a classifier on top of BERT.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcBeb-E0GApS"
      },
      "source": [
        "### BERT Sentence Perplexity\n",
        "\n",
        "BERT can be used as a language model to see how \"likely\" a sentence is in that language model's configurations.\n",
        "\n",
        "Here is some additional reading on the topic:\n",
        "\n",
        "- [Can We Use BERT as a Language Model to Assign a Score to a Sentence?](https://www.scribendi.ai/can-we-use-bert-as-a-language-model-to-assign-score-of-a-sentence/)\n",
        "\n",
        "- [Comparing BERT and GPT-2 as Language Models to Score the Grammatical Correctness of a Sentence](https://www.scribendi.ai/comparing-bert-and-gpt-2-as-language-models-to-score-the-grammatical-correctness-of-a-sentence/)\n",
        "\n",
        "- [Transformers - Perplexity of fixed-length models](https://huggingface.co/transformers/perplexity.html)\n",
        "\n",
        "- [Understanding evaluation metrics for language models](https://thegradient.pub/understanding-evaluation-metrics-for-language-models/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYOx7a1VKE6r"
      },
      "outputs": [],
      "source": [
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkbud6iEJJma"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer,BertForMaskedLM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKxudeIGJNML"
      },
      "outputs": [],
      "source": [
        "bertMaskedLM = BertForMaskedLM.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xR4c5pp3JSve"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgGODxHsHDLE"
      },
      "outputs": [],
      "source": [
        "def get_score(sentence, model, tokenizer):\n",
        "    tokenize_input = tokenizer.tokenize(sentence)\n",
        "    tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\n",
        "    predictions = model(tensor_input)\n",
        "    # print(predictions[0])\n",
        "    # sentence_embedding, word_embedding = predictions[0], predictions[1]\n",
        "    loss_fct = torch.nn.CrossEntropyLoss()\n",
        "    loss = loss_fct(predictions[0].squeeze(),tensor_input.squeeze()).data\n",
        "    return math.exp(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaCzvXPhJbH2"
      },
      "outputs": [],
      "source": [
        "get_score(\"Can you get some bread?\", bertMaskedLM, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvYzCBbFK7X7"
      },
      "outputs": [],
      "source": [
        "get_score(\"This is a correct sentence.\", bertMaskedLM, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82QXPQBDK-2u"
      },
      "outputs": [],
      "source": [
        "get_score(\"Fungus skatebutter\", bertMaskedLM, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmbMW-eHKHTn"
      },
      "outputs": [],
      "source": [
        "get_score(\"Xuusgs ddd\", bertMaskedLM, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2DwGvS9LEKc"
      },
      "source": [
        "A lower perplexity score suggests a sentence with a higher probability of being correct."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zpo1ZO-12KB"
      },
      "source": [
        "### Contextual Sentence Embeddings\n",
        "\n",
        "Standard BERT sentence embeddings don't perform as well as special architectures built for sentence embeddings, such as BERT siamese networks.\n",
        "\n",
        "We will use the [UKPLab package \"sentence transformers\"](https://github.com/UKPLab/sentence-transformers\n",
        "), which implement such methods.\n",
        "\n",
        "- Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks (EMNLP 2019)\n",
        "- Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation (EMNLP 2020)\n",
        "- Augmented SBERT: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks (NAACL 2021)\n",
        "- The Curse of Dense Low-Dimensional Information Retrieval for Large Index Sizes (arXiv 2020)\n",
        "\n",
        "Documentation for [SBERT](https://www.sbert.net/examples/applications/computing-embeddings/README.html).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTm6r2TRHw1C"
      },
      "outputs": [],
      "source": [
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K432TawSH0mV"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtnxsyhoH1nk"
      },
      "outputs": [],
      "source": [
        "# Our sentences we like to encode\n",
        "sentences = ['This framework generates embeddings for each input sentence',\n",
        "    'Sentences are passed as a list of string.',\n",
        "    'The quick brown fox jumps over the lazy dog.']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jspJw3pH24w"
      },
      "outputs": [],
      "source": [
        "#Sentences are encoded by calling model.encode()\n",
        "embeddings = model.encode(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDkn9wcaZ1mD"
      },
      "outputs": [],
      "source": [
        "#Print the embeddings\n",
        "for sentence, embedding in zip(sentences, embeddings):\n",
        "    print(\"Sentence:\", sentence)\n",
        "    print(\"Embedding:\", embedding)\n",
        "    print(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-ha_CxEIB45"
      },
      "source": [
        "We recommend visiting the documentation to view other uses of these sentence embeddings, such as [clustering](https://www.sbert.net/examples/applications/clustering/README.html), [Paraphrase Mining](https://www.sbert.net/examples/applications/paraphrase-mining/README.html), [Semantic Search](https://www.sbert.net/examples/applications/semantic-search/README.html), [Retreive and Rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html), and even multi-modal [Image Search](https://www.sbert.net/examples/applications/image-search/README.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzgLNbSwezjB"
      },
      "source": [
        "## GPT embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVtbRBUIe4ze"
      },
      "source": [
        "You can also utilize GPT embeddings, OpenAI's text embeddings that measure the relatedness of text strings. To obtain text embeddings, you need to install the OpenAI library, which enables you to use their API and extract embeddings from your texts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6O0UFm1eflcC"
      },
      "source": [
        "Install openai library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ah3SZRYfj_3"
      },
      "outputs": [],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGNI2ZzSf043"
      },
      "source": [
        "You need to get your own \"API key\" from the OpenAI website (https://platform.openai.com/api-keys). Also, set the name of the model that you want to use. text-embedding-3-small and large are the most recent embedding models from OpenAI (See https://openai.com/blog/new-embedding-models-and-api-updates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBIVQC1Ygo6G"
      },
      "outputs": [],
      "source": [
        "api_key = '' #put your API key here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3o5_jjICiH0-"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=api_key)\n",
        "model=\"text-embedding-3-small\"\n",
        "\n",
        "#Our sentences we like to encode\n",
        "sentences = ['This framework generates embeddings for each input sentence',\n",
        "    'Sentences are passed as a list of string.',\n",
        "    'The quick brown fox jumps over the lazy dog.']\n",
        "\n",
        "embeddings = []\n",
        "for text in sentences:\n",
        "   embeddings.append(client.embeddings.create(input = [text], model=model).data[0].embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sV7S2-jihND7"
      },
      "source": [
        "Print the embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XN7xW8WzhNyk"
      },
      "outputs": [],
      "source": [
        "#Print the embeddings\n",
        "for sentence, embedding in zip(sentences, embeddings):\n",
        "    print(\"Sentence:\", sentence)\n",
        "    print(\"Embedding:\", embedding)\n",
        "    print(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsnU6W6p4e0J"
      },
      "source": [
        "## Homework Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qSjegARMEms"
      },
      "source": [
        "**1)** Use BERT or a related model to create word and sentence embeddings from your own corpus, and perform different similarity metrics on your embeddings, as well as perplexity measures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRwBdNBXMjPd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPN6WV1eMjkB"
      },
      "source": [
        "**2)** How does fine-tuning your model change the similarity and perplexity metrics?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1os9EzltMn9Y"
      },
      "outputs": [],
      "source": [
        "fine_tuning = 'something' #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZoyb9z3NADe"
      },
      "source": [
        "**3)** BONUS: Fine tune a Transformers based model and use bertviz to visualise its attention layers using two sentences. Compare this to two sentences on a non fine tuned model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RlbGm4W4NIqS",
        "outputId": "246005d9-7571-4fa1-c52c-cd02603fe80c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaG_tw0SLATx"
      },
      "source": [
        "# Module 5: Diffussion Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "aPgqzY_WT5n7",
        "outputId": "043cb300-20d6-4beb-c6c9-8ed49037f425"
      },
      "outputs": [],
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/madaan/minimal-text-diffusion.git\n",
        "%cd minimal-text-diffusion\n",
        "\n",
        "# Install system dependencies for mpi4py\n",
        "!apt-get install libopenmpi-dev\n",
        "\n",
        "# Install Python requirements\n",
        "!pip install -r requirements.txt\n",
        "!pip install mpi4py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGA1METyfmzv",
        "outputId": "3dd9dae5-856b-4dab-e971-a4f1d90ef885"
      },
      "outputs": [],
      "source": [
        "%%writefile /content/minimal-text-diffusion/setup.py\n",
        "from setuptools import setup, find_packages\n",
        "\n",
        "setup(\n",
        "    name=\"minimal_text_diffusion\",\n",
        "    version=\"0.1\",\n",
        "    packages=find_packages(),\n",
        "    install_requires=[\n",
        "        \"torch\",\n",
        "        \"numpy\",\n",
        "        \"tqdm\",\n",
        "        \"transformers\",\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "teSuzD5xfviy",
        "outputId": "35f806f7-5f84-4724-d328-1ad2c4f17e48"
      },
      "outputs": [],
      "source": [
        "# Install the current directory as a package\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gdse99mnT6hX",
        "outputId": "e6215bb8-686c-4648-c35c-a00faa712ab8"
      },
      "outputs": [],
      "source": [
        "# Create a word-level tokenizer for the example dataset\n",
        "!python src/utils/custom_tokenizer.py train-word-level data/simple/simple.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDm2822XmllM",
        "outputId": "c821c45e-7575-4b41-953d-ea2e18415d6f"
      },
      "outputs": [],
      "source": [
        "from src.utils.custom_tokenizer import create_tokenizer\n",
        "\n",
        "tokenizer = create_tokenizer(\n",
        "    return_pretokenized=False,\n",
        "    path=\"data/simple\"\n",
        ")\n",
        "\n",
        "print(\"Vocab size:\", tokenizer.vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEDyfR6tcxya"
      },
      "source": [
        "## Experiment Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teiBYdI2dOfX"
      },
      "source": [
        "### 1. Text-to-Text Diffusion: What Are We Training?\n",
        "\n",
        "In this notebook, we train a **text diffusion model** that generates text by\n",
        "**iteratively denoising continuous token embeddings** rather than predicting tokens\n",
        "autoregressively.\n",
        "\n",
        "High-level idea:\n",
        "\n",
        "1. Text → token embeddings (continuous vectors)\n",
        "2. Add Gaussian noise over many diffusion steps\n",
        "3. Train a Transformer to denoise embeddings conditioned on time\n",
        "4. Decode the final embeddings back into discrete tokens\n",
        "\n",
        "This approach follows *Diffusion-LM–style* modeling:\n",
        "- diffusion happens in **embedding space**\n",
        "- language structure comes from a **Transformer encoder**\n",
        "- discreteness is enforced via a **token-level decoding loss**\n",
        "\n",
        "In the next sections, we will:\n",
        "- define all training hyperparameters explicitly (instead of using bash scripts)\n",
        "- construct the model and diffusion process\n",
        "- run a minimal training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgDb3bcDU4OI",
        "outputId": "6512b2c1-d7ec-484f-b453-9806cbad39e0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# For reproducibility\n",
        "SEED = 60637\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vefTEe3pVCgN"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class DiffusionConfig:\n",
        "    # ===== Data =====\n",
        "    dataset: str = \"simple\"\n",
        "    train_txt_path: str = \"data/simple-train.txt\"\n",
        "    val_txt_path: str = \"data/simple-test.txt\"\n",
        "    sequence_len: int = 32\n",
        "    batch_size: int = 16\n",
        "\n",
        "    # ===== Diffusion process =====\n",
        "    diffusion_steps: int = 200\n",
        "    noise_schedule: str = \"sqrt\"   # linear | cosine | sqrt\n",
        "    predict_xstart: bool = True    # model predicts x_0 (clean embedding)\n",
        "\n",
        "    # ===== Model =====\n",
        "    config_name: str = \"bert-base-uncased\"\n",
        "    in_channel: int = 64           # embedding dimension\n",
        "    out_channel: int = 64\n",
        "    num_channels: int = 64         # Transformer hidden channels\n",
        "    num_heads: int = 2\n",
        "    dropout: float = 0.1\n",
        "\n",
        "    # ===== Embeddings =====\n",
        "    init_pretrained: bool = False\n",
        "    use_pretrained_embeddings: bool = False\n",
        "    freeze_embeddings: bool = False\n",
        "\n",
        "    # ===== Optimization =====\n",
        "    lr: float = 1e-4\n",
        "    weight_decay: float = 0.0\n",
        "    lr_anneal_steps: int = 25000\n",
        "\n",
        "    # ===== Logging / checkpoints =====\n",
        "    checkpoint_path: str = \"ckpts/simple\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyvJPnBAVKrN",
        "outputId": "61ad2e1d-9318-4117-ac76-cb35d14430e6"
      },
      "outputs": [],
      "source": [
        "cfg = DiffusionConfig()\n",
        "os.makedirs(cfg.checkpoint_path, exist_ok=True)\n",
        "\n",
        "cfg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Mb2lOVmdJrb"
      },
      "source": [
        "### Why these defaults?\n",
        "\n",
        "**Embedding diffusion**\n",
        "- `predict_xstart = True` means the model predicts the clean embedding directly.\n",
        "- This is more stable for text than predicting noise.\n",
        "\n",
        "**Pretrained structure**\n",
        "- We reuse a BERT encoder as a *denoiser*, not a language model.\n",
        "- This gives strong inductive bias for syntax and semantics.\n",
        "\n",
        "**Fixed sequence length**\n",
        "- Diffusion models require fixed-size inputs.\n",
        "- Text is padded or truncated to `sequence_len`.\n",
        "\n",
        "**Token-level grounding**\n",
        "- Even though diffusion is continuous, training includes a token-level loss\n",
        "  so generated embeddings remain decodable into real text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pykd9lgZdVKL"
      },
      "source": [
        "## Creating the Model and Diffusion Process\n",
        "\n",
        "We now instantiate the **Transformer denoiser** and the **Gaussian diffusion process**.\n",
        "\n",
        "Recall the separation of concerns:\n",
        "- The **model** learns *how* to denoise embeddings.\n",
        "- The **diffusion object** defines *what noise is added* and *what loss is optimized*.\n",
        "\n",
        "Both are created together using a factory method from the repository."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQIqYeDBiDZJ"
      },
      "source": [
        "### Import the factory method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHoPVdhPVQ7G"
      },
      "outputs": [],
      "source": [
        "# Core factory method\n",
        "from src.train_infer.factory_methods import create_model_and_diffusion\n",
        "\n",
        "# Utilities for argument handling\n",
        "from src.utils.args_utils import model_and_diffusion_defaults"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w88cRQsHiOCO"
      },
      "source": [
        "### Build an argument dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6apE5sv2dcXC"
      },
      "outputs": [],
      "source": [
        "# Start from the repo defaults\n",
        "args = model_and_diffusion_defaults()\n",
        "\n",
        "# ---- Diffusion settings ----\n",
        "args[\"diffusion_steps\"] = cfg.diffusion_steps\n",
        "args[\"noise_schedule\"] = cfg.noise_schedule\n",
        "args[\"predict_xstart\"] = cfg.predict_xstart\n",
        "\n",
        "# ---- Model architecture ----\n",
        "args[\"in_channel\"] = cfg.in_channel\n",
        "args[\"out_channel\"] = cfg.out_channel\n",
        "args[\"num_channels\"] = cfg.num_channels\n",
        "args[\"num_heads\"] = cfg.num_heads\n",
        "args[\"dropout\"] = cfg.dropout\n",
        "args[\"config_name\"] = cfg.config_name\n",
        "\n",
        "# ---- Embeddings ----\n",
        "args[\"init_pretrained\"] = cfg.init_pretrained\n",
        "args[\"use_pretrained_embeddings\"] = cfg.use_pretrained_embeddings\n",
        "args[\"freeze_embeddings\"] = cfg.freeze_embeddings\n",
        "\n",
        "# ---- Text specifics ----\n",
        "# Vocabulary size is determined by the tokenizer.\n",
        "# For the \"simple\" dataset, this is read from disk.\n",
        "from src.utils.custom_tokenizer import create_tokenizer\n",
        "tokenizer = create_tokenizer(\n",
        "    return_pretokenized=cfg.use_pretrained_embeddings,\n",
        "    path=\"data/simple\"\n",
        ")\n",
        "args[\"vocab_size\"] = tokenizer.vocab_size\n",
        "\n",
        "# ---- Misc (safe defaults) ----\n",
        "args[\"class_cond\"] = False\n",
        "args[\"learn_sigma\"] = False\n",
        "args[\"sigma_small\"] = False\n",
        "args[\"use_kl\"] = False\n",
        "args[\"rescale_timesteps\"] = False\n",
        "args[\"rescale_learned_sigmas\"] = False\n",
        "args[\"timestep_respacing\"] = \"\"\n",
        "args[\"model_arch\"] = \"transformer\"\n",
        "args[\"training_mode\"] = \"diffusion-lm\"\n",
        "args[\"logits_mode\"] = 1\n",
        "args[\"use_checkpoint\"] = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nyi5r3n0ibDX"
      },
      "source": [
        "### Instantiate model and diffusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9FdacniMAUa"
      },
      "source": [
        "Run this cell before you create the model/diffusion:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yk6-pqWMEiv",
        "outputId": "061bc152-5ea5-4191-d230-67bc628115db"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoConfig\n",
        "\n",
        "# Save original function\n",
        "_original_from_pretrained = AutoConfig.from_pretrained\n",
        "\n",
        "def _patched_from_pretrained(*args, **kwargs):\n",
        "    cfg = _original_from_pretrained(*args, **kwargs)\n",
        "    # Transformers (newer versions) expect this to be set for BERT attention\n",
        "    if getattr(cfg, \"_attn_implementation\", None) is None:\n",
        "        cfg._attn_implementation = \"eager\"\n",
        "    return cfg\n",
        "\n",
        "AutoConfig.from_pretrained = _patched_from_pretrained\n",
        "print(\"Patched AutoConfig.from_pretrained to set config._attn_implementation='eager' when missing.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XmtdB77FiYfO",
        "outputId": "c1094ba2-4521-4d06-b22d-4d2bfe800e6d"
      },
      "outputs": [],
      "source": [
        "model, diffusion = create_model_and_diffusion(**args)\n",
        "\n",
        "model = model.to(DEVICE)\n",
        "model.eval()  # we will switch to train() later\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UURpIqrRiuXp"
      },
      "source": [
        "### What did we just create?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dh3r6NGmiyIw"
      },
      "source": [
        "**Model**\n",
        "- A time-conditioned Transformer (BERT encoder)\n",
        "- Input: noisy token embeddings + diffusion timestep\n",
        "- Output: predicted clean embeddings (x₀)\n",
        "\n",
        "**Diffusion**\n",
        "- A Gaussian forward process over embeddings\n",
        "- A training loss combining:\n",
        "  - diffusion MSE\n",
        "  - token-level reconstruction loss\n",
        "  - regularization at extreme timesteps\n",
        "\n",
        "Together, they define the learning problem:\n",
        "> “Given a noisy embedding sequence at step *t*, predict the clean embedding.”\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPzR00_Mi3i1"
      },
      "source": [
        "### Sanity check: shapes and forward pass\n",
        "\n",
        "Before training, we do a minimal forward sanity check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iV9LMLSniqSA",
        "outputId": "b90ec4f8-48e9-4148-e0ce-10ea47f259f9"
      },
      "outputs": [],
      "source": [
        "# Create a dummy batch\n",
        "batch_size = 2\n",
        "seq_len = cfg.sequence_len\n",
        "embed_dim = cfg.in_channel\n",
        "\n",
        "# Fake token IDs\n",
        "dummy_input_ids = torch.randint(\n",
        "    low=0,\n",
        "    high=tokenizer.vocab_size,\n",
        "    size=(batch_size, seq_len),\n",
        "    device=DEVICE,\n",
        ")\n",
        "\n",
        "# Get clean embeddings\n",
        "with torch.no_grad():\n",
        "    x0 = model.get_embeds(dummy_input_ids)\n",
        "\n",
        "print(\"Clean embedding shape:\", x0.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-r2-wLsqjOsa"
      },
      "source": [
        "Now simulate one diffusion step:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRUpPedpi79b",
        "outputId": "5a3df583-12e4-4eb0-d5b7-8ea2c1965bea"
      },
      "outputs": [],
      "source": [
        "# Sample random timesteps\n",
        "t = torch.randint(\n",
        "    low=0,\n",
        "    high=cfg.diffusion_steps,\n",
        "    size=(batch_size,),\n",
        "    device=DEVICE,\n",
        ")\n",
        "\n",
        "# Add noise using the diffusion process\n",
        "noise = torch.randn_like(x0)\n",
        "x_t = diffusion.q_sample(x0, t, noise=noise)\n",
        "\n",
        "print(\"Noisy embedding shape:\", x_t.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cf-kdf9gjTT6"
      },
      "source": [
        "And run the denoiser:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jr7YGNYPjRgR",
        "outputId": "d4334c38-14b6-4e0f-ef93-f4e36b1a401b"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    x0_pred = model(x_t, t)\n",
        "\n",
        "print(\"Predicted x0 shape:\", x0_pred.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-oafM6qjYdi"
      },
      "source": [
        "You should see:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KONTauKdjVHX",
        "outputId": "3b9cf930-8806-48ea-83d0-91cc0033c822"
      },
      "outputs": [],
      "source": [
        "(batch_size, seq_len, embed_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hij4Effqjmbm"
      },
      "source": [
        "for all three tensors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKzRaL4SjoLY"
      },
      "source": [
        "### Key takeaway"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MI-s-bJ-jt--"
      },
      "source": [
        "At this point, we have verified that:\n",
        "\n",
        "- Text is represented as **continuous embeddings**\n",
        "- Gaussian noise is added via the diffusion process\n",
        "- A Transformer conditioned on time can predict clean embeddings\n",
        "- All tensor shapes line up correctly\n",
        "\n",
        "We are now ready to define the **training loop**, where the model learns\n",
        "to denoise embeddings across many timesteps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7G-0i0jj9cO"
      },
      "source": [
        "## Training the Text Diffusion Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypnnA7eFkB0P"
      },
      "source": [
        "### What happens during training\n",
        "\n",
        "Training a diffusion model follows a simple repeated procedure:\n",
        "\n",
        "For each batch:\n",
        "1. Encode text into token embeddings (x₀)\n",
        "2. Sample a diffusion timestep t\n",
        "3. Add Gaussian noise → xₜ\n",
        "4. Ask the Transformer to predict x₀ from (xₜ, t)\n",
        "5. Compute diffusion + token reconstruction loss\n",
        "6. Update model parameters\n",
        "\n",
        "All diffusion math is handled by the `diffusion` object.\n",
        "We only need to call it correctly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZU-u0R-kPIC"
      },
      "source": [
        "### Build the dataloader\n",
        "\n",
        "We reuse the repo's sentencepiece utilities to stay faithful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ualj9Ba3ja1e"
      },
      "outputs": [],
      "source": [
        "from src.utils import data_utils_sentencepiece\n",
        "\n",
        "train_dataloader = data_utils_sentencepiece.get_dataloader(\n",
        "    tokenizer=tokenizer,\n",
        "    data_path=cfg.train_txt_path,\n",
        "    batch_size=cfg.batch_size,\n",
        "    max_seq_len=cfg.sequence_len,\n",
        ")\n",
        "\n",
        "val_dataloader = data_utils_sentencepiece.get_dataloader(\n",
        "    tokenizer=tokenizer,\n",
        "    data_path=cfg.val_txt_path,\n",
        "    batch_size=cfg.batch_size,\n",
        "    max_seq_len=cfg.sequence_len,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvNfGHDjkXS7"
      },
      "source": [
        "### Optimizer setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuWKNxz5NJ5I",
        "outputId": "7c684fd2-587e-4bc2-b650-0e6a038214c9"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class _ModelHolder:\n",
        "    \"\"\"Simple holder so code can do model.model.module like in DDP wrappers.\"\"\"\n",
        "    def __init__(self, module: nn.Module):\n",
        "        self.module = module\n",
        "\n",
        "class DiffusionCompatWrapper(nn.Module):\n",
        "    \"\"\"\n",
        "    Compatibility wrapper for this repo's diffusion code.\n",
        "    Supports BOTH:\n",
        "      - model.module.* (DDP style)\n",
        "      - model.model.module.* (their internal wrapper style)\n",
        "    \"\"\"\n",
        "    def __init__(self, net: nn.Module):\n",
        "        super().__init__()\n",
        "        self.net = net\n",
        "        self.model = _ModelHolder(net)  # so .model.module exists\n",
        "\n",
        "    @property\n",
        "    def module(self):\n",
        "        # so .module exists\n",
        "        return self.net\n",
        "\n",
        "    def forward(self, *args, **kwargs):\n",
        "        return self.net(*args, **kwargs)\n",
        "\n",
        "    # Convenience passthroughs (sometimes used in sampling/logging)\n",
        "    def get_embeds(self, *args, **kwargs):\n",
        "        return self.net.get_embeds(*args, **kwargs)\n",
        "\n",
        "    def get_logits(self, *args, **kwargs):\n",
        "        return self.net.get_logits(*args, **kwargs)\n",
        "\n",
        "# Re-wrap the base model\n",
        "wrapped_model = DiffusionCompatWrapper(model).to(DEVICE)\n",
        "wrapped_model.train()\n",
        "\n",
        "print(\"Compat wrapper ready:\",\n",
        "      \"has .module ->\", hasattr(wrapped_model, \"module\"),\n",
        "      \"has .model.module ->\", hasattr(wrapped_model, \"model\") and hasattr(wrapped_model.model, \"module\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3OfdjJNkVQ-"
      },
      "outputs": [],
      "source": [
        "model.train()\n",
        "\n",
        "optimizer = torch.optim.AdamW(wrapped_model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xMXTkv2Pben"
      },
      "outputs": [],
      "source": [
        "def unpack_batch(batch, device):\n",
        "    \"\"\"\n",
        "    Supports:\n",
        "      - dict: {\"input_ids\": ..., \"attention_mask\": ...}\n",
        "      - tuple: (None, {...})  <-- what this repo returns\n",
        "      - tuple: (input_ids, attention_mask)\n",
        "      - tuple: (input_ids,)\n",
        "    \"\"\"\n",
        "    if batch is None:\n",
        "        raise ValueError(\"Got batch=None. Something is wrong with the DataLoader.\")\n",
        "\n",
        "    # Case 1: already a dict\n",
        "    if isinstance(batch, dict):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch.get(\"attention_mask\", None)\n",
        "        if attention_mask is not None:\n",
        "            attention_mask = attention_mask.to(device)\n",
        "        return input_ids, attention_mask\n",
        "\n",
        "    # Case 2: tuple/list\n",
        "    if isinstance(batch, (tuple, list)):\n",
        "        # Special case: (None, dict)\n",
        "        if len(batch) == 2 and batch[0] is None and isinstance(batch[1], dict):\n",
        "            return unpack_batch(batch[1], device)\n",
        "\n",
        "        # Common cases: (input_ids,) or (input_ids, attention_mask)\n",
        "        if len(batch) >= 1 and hasattr(batch[0], \"to\"):\n",
        "            input_ids = batch[0].to(device)\n",
        "            attention_mask = batch[1].to(device) if len(batch) > 1 and hasattr(batch[1], \"to\") else None\n",
        "            return input_ids, attention_mask\n",
        "\n",
        "        raise ValueError(f\"Unrecognized tuple batch structure: {batch}\")\n",
        "\n",
        "    raise TypeError(f\"Unexpected batch type: {type(batch)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjEel2eUOVHn",
        "outputId": "fedc1de4-793e-454a-d9f2-e3766fa7dbd7"
      },
      "outputs": [],
      "source": [
        "# If this prints a number, your training loop will run.\n",
        "b = next(iter(train_dataloader))\n",
        "input_ids, attention_mask = unpack_batch(b, DEVICE)\n",
        "t = torch.randint(0, cfg.diffusion_steps, (input_ids.size(0),), device=DEVICE)\n",
        "\n",
        "out = diffusion.training_losses(\n",
        "    model=wrapped_model,\n",
        "    x_start_never_used=None,\n",
        "    t=t,\n",
        "    model_kwargs={\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
        ")\n",
        "print(out.keys(), out[\"loss\"].mean().item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_PnkR4GkcXO"
      },
      "source": [
        "### One training step\n",
        "\n",
        "This cell is the conceptual heart of the notebook. We are using Mixed precision (AMP) to speed up the process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPxgBTWwka6G",
        "outputId": "a3decf18-e37f-4fc5-8a5e-5008a2e055af"
      },
      "outputs": [],
      "source": [
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "scaler = GradScaler(enabled=(DEVICE==\"cuda\"))\n",
        "\n",
        "def diffusion_train_step_amp(model, diffusion, batch, optimizer):\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    input_ids, attention_mask = unpack_batch(batch, DEVICE)\n",
        "    batch_size = input_ids.size(0)\n",
        "\n",
        "    t = torch.randint(0, cfg.diffusion_steps, (batch_size,), device=DEVICE)\n",
        "\n",
        "    model_kwargs = {\"input_ids\": input_ids}\n",
        "    if attention_mask is not None:\n",
        "        model_kwargs[\"attention_mask\"] = attention_mask\n",
        "\n",
        "    with autocast(enabled=(DEVICE==\"cuda\")):\n",
        "        losses = diffusion.training_losses(\n",
        "            model=model,\n",
        "            x_start_never_used=None,\n",
        "            t=t,\n",
        "            model_kwargs=model_kwargs,\n",
        "        )\n",
        "        loss = losses[\"loss\"].mean()\n",
        "\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    return float(loss.detach().cpu())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71uZnUNEk3EY"
      },
      "source": [
        "### Training loop\n",
        "\n",
        "We intentionally keep this short and explicit.\n",
        "\n",
        "☕ Coffee break time! This training run is time-consuming. You can interrupt it after about 30 minutes and proceed to the next section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443,
          "referenced_widgets": [
            "3af1596760904d1ca68ca2c60a461110",
            "e6642e69f6c74678a0e2e5f09b7aeb03",
            "372398789f0e48408f9ba23859028b52",
            "c50f057122d44ce7818b4da242b5994f",
            "0dcd0acaaa484cdeb8df1da44784c827",
            "93878759764a48b0b447e6bc169af3af",
            "06de9abe6bb649eca887b7d5ef800bc3",
            "f1c8f35a06bb4eeb85b7457d12f963c4",
            "af7d56d3b263463983d17f2f7e454714",
            "7f8694661cfe42d19b1ff2f8186befa8",
            "20f33c26105c4dc9b852ff2604d8a25d"
          ]
        },
        "collapsed": true,
        "id": "wEjLeeWgkiBj",
        "outputId": "31a66055-35b5-4b4d-f4d2-30980757c8b4"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "\n",
        "# Speed optimizations\n",
        "if torch.cuda.is_available():\n",
        "    # Matmul precision: \"high\" often enables TF32 where appropriate\n",
        "    torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "    # New backend knobs (replacing allow_tf32 flags)\n",
        "    try:\n",
        "        torch.backends.cuda.matmul.fp32_precision = \"tf32\"   # or \"ieee\"\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        torch.backends.cudnn.conv.fp32_precision = \"tf32\"    # or \"ieee\"\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "from torch.amp import autocast, GradScaler\n",
        "\n",
        "amp_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "scaler = GradScaler(amp_device, enabled=(amp_device == \"cuda\"))\n",
        "\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 1\n",
        "loss_history = []\n",
        "\n",
        "wrapped_model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    pbar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\")\n",
        "    for batch in pbar:\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        input_ids, attention_mask = unpack_batch(batch, DEVICE)\n",
        "        batch_size = input_ids.size(0)\n",
        "\n",
        "        t = torch.randint(0, cfg.diffusion_steps, (batch_size,), device=DEVICE)\n",
        "\n",
        "        model_kwargs = {\"input_ids\": input_ids}\n",
        "        if attention_mask is not None:\n",
        "            model_kwargs[\"attention_mask\"] = attention_mask\n",
        "\n",
        "        with autocast(device_type=amp_device, enabled=(amp_device == \"cuda\")):\n",
        "            losses = diffusion.training_losses(\n",
        "                model=wrapped_model,\n",
        "                x_start_never_used=None,\n",
        "                t=t,\n",
        "                model_kwargs=model_kwargs,\n",
        "            )\n",
        "            loss = losses[\"loss\"].mean()\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        loss_history.append(loss.item())\n",
        "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkCEeJfFlP3H"
      },
      "source": [
        "### Plot training loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "7BF897Mtk_8g",
        "outputId": "e5180645-0892-47bb-921f-50fa63b644f7"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(loss_history)\n",
        "plt.xlabel(\"Training step\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Text Diffusion Training Loss\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8egj3BSElaI8"
      },
      "source": [
        "### What is actually being optimized?\n",
        "\n",
        "The loss optimized here is a **sum of three terms**:\n",
        "\n",
        "1. **Diffusion loss (MSE)**  \n",
        "   Encourages the model to predict clean embeddings from noisy ones.\n",
        "\n",
        "2. **Token reconstruction loss (Cross-Entropy)**  \n",
        "   Ensures embeddings remain decodable into real words.\n",
        "\n",
        "3. **Endpoint regularization**  \n",
        "   Stabilizes behavior at the start and end of the diffusion chain.\n",
        "\n",
        "This combination is what allows diffusion to work for text,\n",
        "despite text being fundamentally discrete."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cUpZZlCcZZR"
      },
      "source": [
        "## Generating Text and Extracting Embeddings with a Diffusion Model\n",
        "\n",
        "In this section, we will generate text from a trained text diffusion model.\n",
        "Conceptually, this mirrors image diffusion — except we diffuse and denoise token embeddings instead of pixels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_K_Ob4WDUidb"
      },
      "source": [
        "### High-level intuition\n",
        "Key idea\n",
        "\n",
        "1.\tText is first represented as continuous embeddings\n",
        "\n",
        "2.\tThe diffusion process:\n",
        "\n",
        "\t•\tadds noise in embedding space (forward process)\n",
        "\n",
        "\t•\tlearns to remove noise (reverse process)\n",
        "\n",
        "3.\tOnly at the very end do we convert embeddings → discrete tokens\n",
        "\n",
        "This is why text diffusion is fundamentally harder than image diffusion:\n",
        "\n",
        "> **language is discrete, but diffusion is continuous**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiqJClrsVHM_"
      },
      "source": [
        "### Switch the model to evaluation mode\n",
        "\n",
        "Before sampling, disable training-specific behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vICnLfsnluLH",
        "outputId": "4d079aea-53f0-4305-9ee7-d06ed011c0a5"
      },
      "outputs": [],
      "source": [
        "wrapped_model.eval()\n",
        "torch.set_grad_enabled(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ar_jTErVM4v"
      },
      "source": [
        "### What exactly does the model generate?\n",
        "\n",
        "During sampling, the diffusion model produces a tensor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvM7ZxQ2VLBs",
        "outputId": "fcb66864-8e47-4209-f3e2-1f605fc337e0"
      },
      "outputs": [],
      "source": [
        "[batch_size, seq_len, embed_dim]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vffR4vI8VatM"
      },
      "source": [
        "This represents denoised token embeddings, not tokens themselves.\n",
        "\n",
        "We must decode them using the model’s LM head.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vo6HRiW_VgZG"
      },
      "source": [
        "### Helper: decode embeddings → text\n",
        "\n",
        "<font color=\"orange\">Note:\n",
        "We use greedy decoding for clarity. Sampling strategies (top-k, nucleus) can be added later.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DE-wKi82VSub"
      },
      "outputs": [],
      "source": [
        "def decode_embeddings(x, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Convert continuous embeddings into text via the LM head.\n",
        "\n",
        "    x: Tensor of shape [B, L, D]\n",
        "    \"\"\"\n",
        "    logits = model.get_logits(x)          # [B, L, vocab_size]\n",
        "    token_ids = logits.argmax(dim=-1)     # greedy decoding\n",
        "\n",
        "    texts = []\n",
        "    for ids in token_ids:\n",
        "        text = tokenizer.decode(\n",
        "            ids.tolist(),\n",
        "            skip_special_tokens=True\n",
        "        )\n",
        "        texts.append(text)\n",
        "    return texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOfiWqnwVtQw"
      },
      "source": [
        "### Sampling from pure noise (core diffusion loop)\n",
        "\n",
        "This is the reverse diffusion process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oy0gVTQ0XDih"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def decode_by_nearest_embedding(x, model, tokenizer, topk=1):\n",
        "    \"\"\"\n",
        "    x: [B, L, D] continuous embeddings\n",
        "    Returns decoded strings by nearest neighbor in embedding table.\n",
        "    \"\"\"\n",
        "    net = model.module  # underlying TransformerNetModel\n",
        "    # Token embedding matrix: [V, D]\n",
        "    E = net.word_embedding.weight  # on DEVICE\n",
        "    E = F.normalize(E, dim=-1)\n",
        "    x = F.normalize(x, dim=-1)\n",
        "\n",
        "    # cosine sim: [B, L, V]\n",
        "    sims = torch.matmul(x, E.t())\n",
        "\n",
        "    # either argmax or sample among top-k\n",
        "    if topk == 1:\n",
        "        token_ids = sims.argmax(dim=-1)\n",
        "    else:\n",
        "        top = torch.topk(sims, k=topk, dim=-1).indices  # [B, L, K]\n",
        "        # sample uniformly from top-k (simple)\n",
        "        pick = torch.randint(0, topk, top.shape[:-1], device=x.device)\n",
        "        token_ids = top.gather(-1, pick.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "    texts = []\n",
        "    for ids in token_ids:\n",
        "        texts.append(tokenizer.decode(ids.tolist(), skip_special_tokens=True))\n",
        "    return texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "bab9aa2ca8b147bf9d69bc37a01931b0",
            "64b9301441e24dbe9e6bae3cff455cad",
            "1fb926776b8f46398259732a43a5f5a3",
            "682740dbc386431b93647a93112fe900",
            "c88fe98370b9423099952738eb728acf",
            "2bc30696efbb438fb18161d3ea9edc46",
            "0016e42b597f474bb233d2d2c4ca30dc",
            "a0d7c2d6a212428c91ee6eeba585a839",
            "8020b0818ad845f8b6019e0c356d404b",
            "9e8fee7af7aa4ee082a995c500400fcc",
            "0664b2ea4f0845eba2c9929e2949bb02"
          ]
        },
        "id": "3uu8DfgfVjBW",
        "outputId": "398435c1-8d4f-4011-8e65-652ac04f1944"
      },
      "outputs": [],
      "source": [
        "batch_size = 4\n",
        "\n",
        "sample_shape = (\n",
        "    batch_size,\n",
        "    cfg.sequence_len,\n",
        "    cfg.in_channel,\n",
        ")\n",
        "\n",
        "samples = diffusion.p_sample_loop(\n",
        "    model=wrapped_model,\n",
        "    shape=sample_shape,\n",
        "    device=DEVICE,\n",
        "    progress=True,   # visualize denoising steps\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwFlpVqxV2AA"
      },
      "source": [
        "What happens internally:\n",
        "\n",
        "```\n",
        "Gaussian noise  →  x_T\n",
        "x_T → x_{T-1} → ... → x_0  (learned denoising)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_73gbieBV_WE"
      },
      "source": [
        "### Decode and inspect generated text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoOSp4XGVzG6",
        "outputId": "31336113-7f26-45bc-fed7-4f3cbeef7a05"
      },
      "outputs": [],
      "source": [
        "texts = decode_by_nearest_embedding(samples, wrapped_model, tokenizer, topk=5)\n",
        "for i, t in enumerate(texts, 1):\n",
        "    print(f\"\\n=== Sample {i} ===\\n{t}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3TtJGhvWKkJ"
      },
      "source": [
        "What to expect\n",
        "\n",
        "- Early training: short, repetitive, semi-grammatical phrases\n",
        "\n",
        "- More training: clearer structure, dataset-specific patterns\n",
        "\n",
        "- This is normal for text diffusion models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFipEG6eWSG5"
      },
      "source": [
        "### Optional: control noise with top_p\n",
        "\n",
        "You can limit extreme Gaussian noise during denoising:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "7888a2cca72d4df2ad0af1590684665d",
            "d6b156156f3a4424a56109eed972f816",
            "7bb46faf00c04171ba5cc658645452b0",
            "ca6f06cf0f2748879bd6c23788d7db1f",
            "08a4eb90cd46400c98014ea01309cab2",
            "404111bb1c2448b880d4c44512c22dee",
            "26ddce2f844b4ae9b4569ae8c6e49a74",
            "bdc948d0c1284e0696392a558dcf18cb",
            "9bc749fdf0884f04b2b51b3b1d9c7ffd",
            "7ed1376d21304bf880a77ded5f62804d",
            "c55c7dbf3a704e6ba16b4a8b3a6ac995"
          ]
        },
        "id": "r-YIZPVvV23_",
        "outputId": "d4af529e-2f64-422c-9156-d21d51d86d23"
      },
      "outputs": [],
      "source": [
        "samples = diffusion.p_sample_loop(\n",
        "    model=wrapped_model,\n",
        "    shape=sample_shape,\n",
        "    device=DEVICE,\n",
        "    progress=True,\n",
        "    top_p=0.9,   # lower = more conservative generation\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vbuhkVYWX2h"
      },
      "source": [
        "This often improves stability for small models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26tQVpkvWVxd",
        "outputId": "879b0b8c-d95c-498f-a928-841609406365"
      },
      "outputs": [],
      "source": [
        "texts = decode_by_nearest_embedding(samples, wrapped_model, tokenizer, topk=5)\n",
        "for i, t in enumerate(texts, 1):\n",
        "    print(f\"\\n=== Sample {i} ===\\n{t}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsA8u6QMWi2R"
      },
      "source": [
        "### Baseline comparison: random embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKpXGaQfWZrh",
        "outputId": "a63af347-7343-408c-99c7-6fe0414e3dca"
      },
      "outputs": [],
      "source": [
        "random_embeds = torch.randn(sample_shape, device=DEVICE)\n",
        "random_texts = decode_embeddings(random_embeds, wrapped_model, tokenizer)\n",
        "\n",
        "print(\"\\n--- Random baseline (no diffusion) ---\")\n",
        "for t in random_texts:\n",
        "    print(t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvGrjBonWp4z"
      },
      "source": [
        "### Key takeaways\n",
        "\n",
        "• Diffusion models for text operate in **continuous embedding space**\n",
        "\n",
        "• Discrete tokens only appear at the **final decoding step**\n",
        "\n",
        "• Training teaches the model to denoise embeddings, not tokens\n",
        "\n",
        "• This explains why text diffusion is harder than image diffusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugejph7_FvgZ"
      },
      "source": [
        "### Extract token embeddings and visualize with PCA/UMAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dyAXHJPWl2M",
        "outputId": "94fe6b3c-d04d-4546-df8e-44e973aae573"
      },
      "outputs": [],
      "source": [
        "def token_id_for_word(tokenizer, word: str):\n",
        "    # HF-style tokenizers\n",
        "    if hasattr(tokenizer, \"encode\"):\n",
        "        ids = tokenizer.encode(word)\n",
        "        # for single-word tokenizers, you often get special tokens too\n",
        "        # keep only non-special if possible\n",
        "        if hasattr(tokenizer, \"all_special_ids\"):\n",
        "            ids = [i for i in ids if i not in set(tokenizer.all_special_ids)]\n",
        "        return ids[0] if len(ids) > 0 else None\n",
        "    return None\n",
        "\n",
        "words = [\"salty\", \"grapefruit\", \"cucumber\", \"fried\", \"squash\", \"bitter\"]\n",
        "word_to_id = {w: token_id_for_word(tokenizer, w) for w in words}\n",
        "word_to_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLP_qo_EGFHJ"
      },
      "source": [
        "### Extract “static” token embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Qnj9LDHF2yu",
        "outputId": "23cc9704-1822-4aee-c8da-b8887bd46f9a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def get_embedding_matrix(model):\n",
        "    # works because our wrapper exposes .module\n",
        "    net = model.module\n",
        "    emb = net.word_embedding.weight.detach().cpu().numpy()  # [vocab, dim]\n",
        "    return emb\n",
        "\n",
        "E = get_embedding_matrix(wrapped_model)\n",
        "E.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfUYsszlGKwS",
        "outputId": "073009d7-287d-4386-f893-5818a37f0887"
      },
      "outputs": [],
      "source": [
        "selected = [(w, i) for w, i in word_to_id.items() if i is not None]\n",
        "selected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZniDXXtqGNI2",
        "outputId": "6f5f037e-2ab1-48eb-ca62-27a3047e9a33"
      },
      "outputs": [],
      "source": [
        "vecs = np.stack([E[i] for _, i in selected], axis=0)  # [n, dim]\n",
        "labels = [w for w, _ in selected]\n",
        "vecs.shape, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJcmGTAjGTY5"
      },
      "source": [
        "### 2D visualization with PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "0fY-izigGQGw",
        "outputId": "e629ce02-89e0-45fb-80c7-94f9fed36791"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "xy = pca.fit_transform(vecs)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.scatter(xy[:, 0], xy[:, 1])\n",
        "\n",
        "for (x, y), label in zip(xy, labels):\n",
        "    plt.text(x + 0.02, y + 0.02, label)\n",
        "\n",
        "plt.title(\"Token Embeddings (PCA projection)\")\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YShefx4NGapS"
      },
      "source": [
        "### Diffusion-specific embeddings: denoised contextual embeddings at different timesteps\n",
        "\n",
        "This is the more diffusion-native view.\n",
        "\n",
        "Idea:\n",
        "\n",
        "- take a real sentence\n",
        "\n",
        "- embed it → x₀\n",
        "\n",
        "- add noise at timestep t → xₜ\n",
        "\n",
        "- ask model to predict x₀ from xₜ\n",
        "\n",
        "- extract the predicted embedding for certain positions/words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLLb-GiSGpGr"
      },
      "source": [
        "Encode one sentence and locate word positions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwFJKO14GXro",
        "outputId": "65b9a76c-5c4e-4f56-eae5-73edcae84b6a"
      },
      "outputs": [],
      "source": [
        "sentence = \"i love fried cucumber\"\n",
        "ids = tokenizer.encode(sentence)\n",
        "# strip special tokens if present\n",
        "if hasattr(tokenizer, \"all_special_ids\"):\n",
        "    ids = [i for i in ids if i not in set(tokenizer.all_special_ids)]\n",
        "\n",
        "# pad/truncate\n",
        "pad_id = getattr(tokenizer, \"pad_token_id\", 0)\n",
        "ids = ids[:cfg.sequence_len] + [pad_id] * (cfg.sequence_len - len(ids))\n",
        "\n",
        "input_ids = torch.tensor([ids], device=DEVICE)\n",
        "attention_mask = (input_ids != pad_id).long()\n",
        "input_ids.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0y3Sr2IG4rW"
      },
      "source": [
        "Pick a timestep grid:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnANJHLQG2IE",
        "outputId": "259ea16b-9439-4677-8253-dbcca24b678b"
      },
      "outputs": [],
      "source": [
        "ts = [0, cfg.diffusion_steps//4, cfg.diffusion_steps//2, 3*cfg.diffusion_steps//4, cfg.diffusion_steps-1]\n",
        "ts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1nuSILvG8_w"
      },
      "source": [
        "Get predicted x0 embeddings at each timestep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LalxeSe_G6UX"
      },
      "outputs": [],
      "source": [
        "def predict_x0_embeddings_at_t(model, diffusion, input_ids, attention_mask, t_scalar: int):\n",
        "    model.eval()\n",
        "    B, L = input_ids.shape\n",
        "    t = torch.full((B,), t_scalar, device=input_ids.device, dtype=torch.long)\n",
        "\n",
        "    # x_start_mean from embeddings\n",
        "    x0_mean = model.module.get_embeds(input_ids)  # [B, L, D]\n",
        "\n",
        "    # create x0 sample with std at t=0 used in repo\n",
        "    std0 = torch.tensor(diffusion.sqrt_one_minus_alphas_cumprod[0], device=input_ids.device).float()\n",
        "    x0 = x0_mean + std0 * torch.randn_like(x0_mean)\n",
        "\n",
        "    # forward diffuse to x_t\n",
        "    noise = torch.randn_like(x0)\n",
        "    x_t = diffusion.q_sample(x0, t, noise=noise)\n",
        "\n",
        "    # model predicts x_start (since predict_xstart=True in your args)\n",
        "    x0_pred = model(x_t, diffusion._scale_timesteps(t), attention_mask=attention_mask)\n",
        "\n",
        "    return x0_pred.detach().cpu().numpy(), x0_mean.detach().cpu().numpy()\n",
        "\n",
        "preds = []\n",
        "for t in ts:\n",
        "    x0_pred, x0_mean = predict_x0_embeddings_at_t(wrapped_model, diffusion, input_ids, attention_mask, t)\n",
        "    preds.append((t, x0_pred[0], x0_mean[0]))  # [L, D]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nk5KHhcvHBaI"
      },
      "source": [
        "Visualize how embeddings “move” across timesteps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEzpGrZ0G_O1",
        "outputId": "f8df7932-6b5e-4779-a77f-4389de537072"
      },
      "outputs": [],
      "source": [
        "# Pick a few token positions (e.g., first 4 non-pad tokens):\n",
        "\n",
        "nonpad = (input_ids[0].cpu().numpy() != pad_id).nonzero()[0].tolist()\n",
        "positions = nonpad[:4]  # choose first few tokens\n",
        "positions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "WjR3hNQvHICU",
        "outputId": "784fa6cf-28dc-4735-b489-c0792009ab66"
      },
      "outputs": [],
      "source": [
        "# Now project the predicted embeddings to 2D with PCA (fit on all timesteps + positions):\n",
        "all_vecs = []\n",
        "all_labels = []\n",
        "for t, x0_pred_LD, _ in preds:\n",
        "    for pos in positions:\n",
        "        all_vecs.append(x0_pred_LD[pos])\n",
        "        all_labels.append((t, pos))\n",
        "\n",
        "all_vecs = np.stack(all_vecs, axis=0)\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "xy = pca.fit_transform(all_vecs)\n",
        "\n",
        "plt.figure(figsize=(7, 6))\n",
        "plt.scatter(xy[:, 0], xy[:, 1], s=30)\n",
        "\n",
        "# annotate: (t, pos)\n",
        "for (x, y), (t, pos) in zip(xy, all_labels):\n",
        "    plt.text(x + 0.02, y, f\"t={t},p={pos}\", fontsize=8)\n",
        "\n",
        "plt.title(\"Predicted x0 Embeddings Across Diffusion Timesteps (PCA)\")\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptL_5tjZCyhO"
      },
      "source": [
        "### Homework Exericise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kM2K_3nVC7kF"
      },
      "source": [
        "**1)** Play with hyper-parameters in `DiffusionConfig` class and report how 1 or 2 of them impacts model learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txYx3VVGC6Sr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-Y2C5owC-xz"
      },
      "source": [
        "**2)** Propose one application of diffusion-based language models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ih01-N11DDl3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Te0XnhTCDMHA"
      },
      "source": [
        "**3)** BONUS: What unique properties of language do diffusion-based embeddings encode that transformer-based embeddings fail to capture?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JB7lcHEzHLC8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlm65U4TDFcx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPMG1Olq7vhH"
      },
      "source": [
        "## Memo Pilot Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D26YPu9-8KgC"
      },
      "source": [
        "Create a preliminary implementation (pilot) of the research design you outlined in your memo of this week. Use the techniques and frameworks introduced in this assignment to demonstrate how your proposed approach can be operationalized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycGo6kJ476w7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9bGWJEW1h_m"
      },
      "source": [
        "<font color=\"purple\">**Reminder:** To complete this week’s off-class work, please fill out the survey using the [link](https://forms.gle/N6wyqTxbzRWGTKtL7). Thank you for your feedback!</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fveULmw61vtY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "agents",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0016e42b597f474bb233d2d2c4ca30dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0664b2ea4f0845eba2c9929e2949bb02": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "06de9abe6bb649eca887b7d5ef800bc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "08a4eb90cd46400c98014ea01309cab2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0dcd0acaaa484cdeb8df1da44784c827": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fb926776b8f46398259732a43a5f5a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0d7c2d6a212428c91ee6eeba585a839",
            "max": 200,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8020b0818ad845f8b6019e0c356d404b",
            "value": 180
          }
        },
        "20f33c26105c4dc9b852ff2604d8a25d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "26ddce2f844b4ae9b4569ae8c6e49a74": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2bc30696efbb438fb18161d3ea9edc46": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "372398789f0e48408f9ba23859028b52": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1c8f35a06bb4eeb85b7457d12f963c4",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_af7d56d3b263463983d17f2f7e454714",
            "value": 1
          }
        },
        "3af1596760904d1ca68ca2c60a461110": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e6642e69f6c74678a0e2e5f09b7aeb03",
              "IPY_MODEL_372398789f0e48408f9ba23859028b52",
              "IPY_MODEL_c50f057122d44ce7818b4da242b5994f"
            ],
            "layout": "IPY_MODEL_0dcd0acaaa484cdeb8df1da44784c827"
          }
        },
        "404111bb1c2448b880d4c44512c22dee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64b9301441e24dbe9e6bae3cff455cad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bc30696efbb438fb18161d3ea9edc46",
            "placeholder": "​",
            "style": "IPY_MODEL_0016e42b597f474bb233d2d2c4ca30dc",
            "value": " 90%"
          }
        },
        "682740dbc386431b93647a93112fe900": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e8fee7af7aa4ee082a995c500400fcc",
            "placeholder": "​",
            "style": "IPY_MODEL_0664b2ea4f0845eba2c9929e2949bb02",
            "value": " 180/200 [00:02&lt;00:00, 88.91it/s]"
          }
        },
        "7888a2cca72d4df2ad0af1590684665d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d6b156156f3a4424a56109eed972f816",
              "IPY_MODEL_7bb46faf00c04171ba5cc658645452b0",
              "IPY_MODEL_ca6f06cf0f2748879bd6c23788d7db1f"
            ],
            "layout": "IPY_MODEL_08a4eb90cd46400c98014ea01309cab2"
          }
        },
        "7bb46faf00c04171ba5cc658645452b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bdc948d0c1284e0696392a558dcf18cb",
            "max": 200,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9bc749fdf0884f04b2b51b3b1d9c7ffd",
            "value": 180
          }
        },
        "7ed1376d21304bf880a77ded5f62804d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f8694661cfe42d19b1ff2f8186befa8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8020b0818ad845f8b6019e0c356d404b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "93878759764a48b0b447e6bc169af3af": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9bc749fdf0884f04b2b51b3b1d9c7ffd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9e8fee7af7aa4ee082a995c500400fcc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0d7c2d6a212428c91ee6eeba585a839": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af7d56d3b263463983d17f2f7e454714": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bab9aa2ca8b147bf9d69bc37a01931b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_64b9301441e24dbe9e6bae3cff455cad",
              "IPY_MODEL_1fb926776b8f46398259732a43a5f5a3",
              "IPY_MODEL_682740dbc386431b93647a93112fe900"
            ],
            "layout": "IPY_MODEL_c88fe98370b9423099952738eb728acf"
          }
        },
        "bdc948d0c1284e0696392a558dcf18cb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c50f057122d44ce7818b4da242b5994f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f8694661cfe42d19b1ff2f8186befa8",
            "placeholder": "​",
            "style": "IPY_MODEL_20f33c26105c4dc9b852ff2604d8a25d",
            "value": " 47561/? [1:03:56&lt;00:00, 12.67it/s, loss=0.1124]"
          }
        },
        "c55c7dbf3a704e6ba16b4a8b3a6ac995": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c88fe98370b9423099952738eb728acf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca6f06cf0f2748879bd6c23788d7db1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ed1376d21304bf880a77ded5f62804d",
            "placeholder": "​",
            "style": "IPY_MODEL_c55c7dbf3a704e6ba16b4a8b3a6ac995",
            "value": " 180/200 [00:02&lt;00:00, 76.49it/s]"
          }
        },
        "d6b156156f3a4424a56109eed972f816": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_404111bb1c2448b880d4c44512c22dee",
            "placeholder": "​",
            "style": "IPY_MODEL_26ddce2f844b4ae9b4569ae8c6e49a74",
            "value": " 90%"
          }
        },
        "e6642e69f6c74678a0e2e5f09b7aeb03": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93878759764a48b0b447e6bc169af3af",
            "placeholder": "​",
            "style": "IPY_MODEL_06de9abe6bb649eca887b7d5ef800bc3",
            "value": "Epoch 1: "
          }
        },
        "f1c8f35a06bb4eeb85b7457d12f963c4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
